[
  {
    "objectID": "posts/2022-02-28-glm_coefficients.html",
    "href": "posts/2022-02-28-glm_coefficients.html",
    "title": "Interpreting the coefficients of Generalized Linear Model",
    "section": "",
    "text": "Linear model is the most popular model used in various of fields, due to its simple execution and interpretation. It can be not only used to predict like all other machine learning models. but also widely used for statistical inference due to its simplicity.\nGeneralized Linear Model (GLM), as named indicated, is generalized from linear regression model, and extends linear model default assumptions to include outcome variables following exponential family distribution. It used link function to transform the outcome so that the transformed Y can be represented by linear combination of predictors. Due to this transformation, it makes coefficients interpretation a little confusing. In this blog, I will use four classical examples (Boston, Default, BrainCancer, and Bikeshare from ISLR2 package) to illustrate how to interpret the coefficients of GLM from tidymodels fit tidy outcome in R."
  },
  {
    "objectID": "posts/2022-02-28-glm_coefficients.html#linear-regression",
    "href": "posts/2022-02-28-glm_coefficients.html#linear-regression",
    "title": "Interpreting the coefficients of Generalized Linear Model",
    "section": "Linear regression",
    "text": "Linear regression\nModeling linear regression in R is simple. The following example used dis (weighted mean of distances to five Boston employment centers) as single predictor to predict medv (median value of house in $1000s) in Boston.\n\n\nCode\ndata(Boston)\nlm_m1 <- lm(medv ~ dis, data = Boston)\nsummary(lm_m1)\n\n\n\nCall:\nlm(formula = medv ~ dis, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.016  -5.556  -1.865   2.288  30.377 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  18.3901     0.8174  22.499  < 2e-16 ***\ndis           1.0916     0.1884   5.795 1.21e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.914 on 504 degrees of freedom\nMultiple R-squared:  0.06246,   Adjusted R-squared:  0.0606 \nF-statistic: 33.58 on 1 and 504 DF,  p-value: 1.207e-08\n\n\nBased on coefficients summary, dis is significantly (p-value = 1.21e-08) positively correlated with medv. With 1 unit increase in term of distances to Boston employment centers, the median value of house increase $1091.6 = 1.0916 * 1000.\n\nMultivariate linear regression\nIn multivariate linear regression, when we interpret the coefficients, there are two components taken into account - whether the variables are independent - how to interpret the interaction term\nIn the following example, we model the medv with dis (weighted mean of distances to five Boston employment centers), rm (average number of rooms per dwelling), crim (per capita crime rate by town) and chas (tract bounds river).\nFor practice purpose, I will use tidymodels to build linear model in the multivariate linear regression example.\n\nNo interaction term\nWe starts with no interactions among the predictors.\n\n\nCode\nlm_spec2 <- linear_reg() %>% \n    set_engine('lm') %>% \n    set_mode('regression')\n\nlm_wf2 <- workflow() %>% \n    add_model(lm_spec2) %>% \n    add_formula(medv ~ dis + rm + crim + chas)\n\nlm_fit2 <- lm_wf2 %>% \n    fit(data = Boston)\n\n\nlm_fit2 %>% \n    tidy()\n\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -29.1      2.57      -11.3  1.20e-26\n2 dis            0.201    0.144       1.40 1.62e- 1\n3 rm             8.19     0.406      20.2  9.12e-67\n4 crim          -0.243    0.0350     -6.94 1.19e-11\n5 chas           3.98     1.10        3.63 3.10e- 4\n\n\nIn this model, all predictors except dis show significant correlation with medv (p-value < 0.05). rm and chas are positively while crim is negatively associated with medv. - rm: when keeping all other variables the same, increase 1 room per dwelling on average results in $8,194.4 (8.1944 * 1000) increase in median house value. - chas: when keeping all other variables the same, having tracts bounds to the Charles river increase median house value $3,982.5 (3.9825 * 1000). chas is a dummy variable where = 1 if tract bounds river and =0 otherwise. Thus =0 (tract do not bound to river) is a baseline here. We will discuss more about baseline in later example. - crim: when keeping all other variables the same, 1 unit increase in per capita crime rate will result a decrease of $243.2 (-0.24318 * 1000) in median house value.\n\n\n\nWith interaction term\nBased on common sense, usually the house is smaller when it is closer to city center. Adding interaction term between rm and dis we assumed that the number of room and the distance to business center are not independent. We are testing the hypothesis that the linear relationship between dis and medv was affected by the the rm. This affect can be linear or non-linear, can be negative or positive.\n\n\nCode\nBoston %>% \n    ggplot(aes(rm)) +\n    geom_histogram() +\n    labs(x = 'mean number of room per dwelling')\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nCode\nBoston %>% \n    mutate(rm = as.integer(rm)) %>% \n    ggplot(aes(x=dis, y=medv, color=as.factor(rm))) +\n    geom_smooth(method = 'lm', se = F) +\n    labs(x = 'mean of distances to five Boston employment centers', y= 'median value of owner-occupied homes', color=\"mean number of room per dwelling\") +\n    theme(legend.position = 'bottom')\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThus, we added interaction term between dis and rm. The thumb of rule to use interaction term is hierarchical principle, which means, if we include an interaction in a model, we should also include the main effects, even if the p-values associated with main effect coefficients are not significant. Thus we should always use * instead of : when adding the interaction term. dis*rm means dis + rm + dis:rm.\n\n\nCode\nlm_wf3 <- workflow() %>% \n    add_model(lm_spec2) %>% \n    add_formula(medv ~ dis*rm + crim + chas)\n\nlm_fit3 <- lm_wf3 %>% \n    fit(data = Boston)\n\n\nlm_fit3 %>% \n    tidy()\n\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -5.03     4.70       -1.07 2.85e- 1\n2 dis           -7.43     1.27       -5.84 9.30e- 9\n3 rm             4.38     0.744       5.89 7.11e- 9\n4 crim          -0.270    0.0341     -7.90 1.80e-14\n5 chas           3.99     1.06        3.77 1.83e- 4\n6 `dis:rm`       1.20     0.198       6.04 3.08e- 9\n\n\nIn this example, all predictors including interaction terms are significant. Interestingly, by adding the interaction between dis and rm, the coefficients associated with dis turn negative from positive when using simple single variable model. To interpret the interaction term,\n\ndis:rm: since interaction term is significant (p-value = 3.077938e-09), thus linear relationship between dis and medv was significantly dependent on the rm, justifying the inclusion of the interaction term in the model.\ndis: when there are 3 ~ 6 rooms in dwelling, one unit further away from five Boston employment centers, it results in $3,835 to $244 ((-7.426 + range(3,6) * 1.197) * 1000) decrease in median value of house. when there are more than 6 (7.426/1.197) rooms in dwelling, one unit further away from five Boston employment centers, it results in at least $953 ((-7.426 + 7 * 1.197) * 1000) increase in median value of house.\nrm: keeping the mean distance to five Boston employment centers as constant dis, one more room in dwelling will increase 1000 * (1.197 * dis + 4.380) in the median value of house. Because the interaction term is positive (1.197), the rate of medv increase in terms of the room number will increase when it is further away from Boston employment centers.\n\n\nWhen to use interaction term\nThe frequently asked question about interaction term is “when should we include interaction term”. The conventional answer is when two predictors are not independent. However, in reality, unless we have very strong prior knowledge about the predictors, it is hard to determine whether two predictors are dependent or not without exploring the data. From the articles/blogs about interaction term I read so far, two methods are generally used to determine whether add interaction term\n\ntry both with and without adding interaction term, if adding interaction term results in significance on interaction term, then use interaction term.\nlike what I did above, plot Y against X1 with X2 as nominal variable (if X2 is not nominal variable itself). If the lines from different X2 levels are parallel, then X1 and X2 are independent and no interaction terms are needed. Otherwise, add interaction term."
  },
  {
    "objectID": "posts/2022-02-28-glm_coefficients.html#logistic-regression",
    "href": "posts/2022-02-28-glm_coefficients.html#logistic-regression",
    "title": "Interpreting the coefficients of Generalized Linear Model",
    "section": "Logistic regression",
    "text": "Logistic regression\nIn the regular linear regression mentioned above, the Y is numeric (aka. quantitative). However, when Y is nominal (aka, qualitative), logistic regression will be used. To make Y still represented by linear combination of predictors, we used logit function (link function) to transform Y (the probability) to \\(ln(\\frac{p}{1-p})\\) (the log odds).\n\\[ln(\\frac{p}{1-p}) = \\sum\\beta X\\] \\(\\beta\\) represents log odds ratio. thus, odds ratio \\(OR = e^\\beta\\).\n\nWhen Y is binomial\nTo evaluate whether a customer will default the credit card default, we build a logistic model with three predictors – whether the customer is a student, the balance on the account and the customer income.\nAgain, for practice purpose, I used tidymodels syntax for demonstration.\n\n\nCode\ndata(\"Default\")\nlr_spec <- logistic_reg() %>% \n    set_engine('glm') %>% \n    set_mode('classification')\n\ndefault_wf <- workflow() %>% \n    add_model(lr_spec) %>% \n    add_formula(default ~ .)\n\ndefault_fit <- default_wf %>% \n    fit(data = Default)\n\ndefault_fit %>% \n    tidy()\n\n\n# A tibble: 4 × 5\n  term            estimate  std.error statistic   p.value\n  <chr>              <dbl>      <dbl>     <dbl>     <dbl>\n1 (Intercept) -10.9        0.492        -22.1   4.91e-108\n2 studentYes   -0.647      0.236         -2.74  6.19e-  3\n3 balance       0.00574    0.000232      24.7   4.22e-135\n4 income        0.00000303 0.00000820     0.370 7.12e-  1\n\n\nIn this model, two predictors (student and balance) are significantly associated with default. To interpret coefficients, we first need to know which is the baseline of default.\n\n\nCode\ncontrasts(Default$default)\n\n\n    Yes\nNo    0\nYes   1\n\n\nBased on the contrasts output, the baseline of default is No. Thus,\n\nstudent: When keeping all other variable constant, compared to non-student (student = 0), a student (student = 1) is less likely to default credit card. The odds ratio is 0.524 (exp(-6.467758e-01)). In other words, if the odds of defaulting credit card as non-student is 1, the odds of defaulting credit card as a student is 0.524 (exp(-6.467758e-01)).\nbalance: When keeping all other variable constant, 1 dollar increase in account balance will result in increasing odds of 1.005 (exp(5.736505e-03)) to default credit card.\n\nNote: above modeling is a bad model since there are high correlation between the predictors (collinearity). I just used it as an example to interpret the coefficients.\n\n\nMultinominal predictors\nUsing multi-nominal predictor diagnosis and other predictors like sex and age time to predict whether the patient survived the brain cancer or not status\n\n\nCode\ndata('BrainCancer')\nBrainCancer <- BrainCancer %>% \n    na.omit()\ncontrasts(BrainCancer$diagnosis)\n\n\n           LG glioma HG glioma Other\nMeningioma         0         0     0\nLG glioma          1         0     0\nHG glioma          0         1     0\nOther              0         0     1\n\n\nIn this example, Meningioma is the baseline for multi-nominal predictor diagnosis.\n\n\nCode\nBrainCancer_rec <- recipe(status ~ ., data = BrainCancer) %>% \n    step_mutate(status = as.factor(status)) %>% \n    step_dummy(diagnosis)\n    \nBrainCancer_wf <- workflow() %>% \n    add_model(lr_spec) %>% \n    add_recipe(BrainCancer_rec)\n\nBrainCancer_fit <- BrainCancer_wf %>% \n    fit(data = BrainCancer)\n\nBrainCancer_fit %>% \n    tidy()\n\n\n# A tibble: 10 × 5\n   term                estimate std.error statistic p.value\n   <chr>                  <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)           3.56      2.57       1.39  0.166  \n 2 sexMale               0.369     0.576      0.640 0.522  \n 3 locSupratentorial     1.09      0.901      1.21  0.227  \n 4 ki                   -0.0695    0.0326    -2.13  0.0332 \n 5 gtv                   0.0382    0.0366     1.04  0.296  \n 6 stereoSRT             0.253     0.771      0.328 0.743  \n 7 time                 -0.0339    0.0155    -2.18  0.0291 \n 8 diagnosis_LG.glioma   1.31      0.844      1.55  0.122  \n 9 diagnosis_HG.glioma   2.37      0.778      3.05  0.00231\n10 diagnosis_Other       0.765     0.940      0.814 0.416  \n\n\nFor multi-nominal predictor diagnosis, the levels (LG glioma, HG glioma and Other) are compared to the baseline Meningioma, and it ends with three terms for coefficient estimation.\nBased on above model, only HG glioma show significant association with survival (p-value < 0.05) when choose Meningioma as baseline. When keeping all other variable constant, compare to Meningioma, the patient with HG glioma are 10 times more (exp(2.37027243)) likely to survive. If we want to compare HG glioma with Other cancer type, simply use exp(2.37027243-0.76482440) to get odds ratio between HG glioma and Other, in which compare to Other, the patient with HG glioma are 5 times more (exp(2.37027243-0.76482440)) likely to survive. However, in this case, we do not know whether this comparison is statistically significant. We can get p-value for this comparison by switching Other as baseline.\n\n\nContrasts matrix\nAnother baseline assignment is using the global average as baseline. To do that, we need to change the contrasts matrix. The following code replace the default contrasts contr.treatment with contr.sum on globalOptions, then use step_dummy from recipe to realize it\n\n\nCode\nBrainCancer_rec %>% \n    prep() %>% \n    bake(new_data = NULL, starts_with(\"diagnosis\")) %>% \n    mutate(diagnosis_orginal = BrainCancer$diagnosis) %>% \n    distinct()\n\n\n# A tibble: 4 × 4\n  diagnosis_LG.glioma diagnosis_HG.glioma diagnosis_Other diagnosis_orginal\n                <dbl>               <dbl>           <dbl> <fct>            \n1                   0                   0               0 Meningioma       \n2                   0                   1               0 HG glioma        \n3                   1                   0               0 LG glioma        \n4                   0                   0               1 Other            \n\n\nCode\ncontr_opt <- getOption(\"contrasts\")\ncontr_opt\n\n\n        unordered           ordered \n\"contr.treatment\"      \"contr.poly\" \n\n\nThe original baseline is Meningioma, each diagnosis_ is compared to the Meningioma.\n\n\nCode\ncontr_sum_opt <- contr_opt\ncontr_sum_opt['unordered'] <- 'contr.sum'\noptions(contrasts = contr_sum_opt)\n\n# my_naming <- function(var, lvl, ordinal = FALSE, sep = \"_\"){\n#     paste(var, levels(BrainCancer$diagnosis)[lvl])\n# }\n\nBrainCancer_rec2 <- recipe(status ~ ., data = BrainCancer) %>% \n    step_mutate(status = as.factor(status)) %>% \n    step_dummy(diagnosis)\n    \nBrainCancer_rec2 %>% \n    prep() %>% \n    bake(new_data = NULL, starts_with(\"diagnosis\")) %>% \n    mutate(diagnosis_orginal = BrainCancer$diagnosis) %>% \n    distinct()\n\n\n# A tibble: 4 × 4\n  diagnosis_X1 diagnosis_X2 diagnosis_X3 diagnosis_orginal\n         <dbl>        <dbl>        <dbl> <fct>            \n1            1            0            0 Meningioma       \n2            0            0            1 HG glioma        \n3            0            1            0 LG glioma        \n4           -1           -1           -1 Other            \n\n\nThus diagnosis_X1, diagnosis_X2 and diagnosis_X3 now represents Meningioma, HG glioma and LG glioma compared to average baseline.\n\n\nCode\nBrainCancer_wf2 <- workflow() %>% \n    add_recipe(BrainCancer_rec2) %>% \n    add_model(lr_spec)\n\nBrainCancer_fit2 <- BrainCancer_wf2 %>% \n    fit(data = BrainCancer)\n\nBrainCancer_fit2 %>% \n    tidy()\n\n\n# A tibble: 10 × 5\n   term         estimate std.error statistic p.value\n   <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)    5.52      2.66       2.07   0.0380\n 2 sex1          -0.184     0.288     -0.640  0.522 \n 3 loc1          -0.545     0.451     -1.21   0.227 \n 4 ki            -0.0695    0.0326    -2.13   0.0332\n 5 gtv            0.0382    0.0366     1.04   0.296 \n 6 stereo1       -0.127     0.386     -0.328  0.743 \n 7 time          -0.0339    0.0155    -2.18   0.0291\n 8 diagnosis_X1  -1.11      0.468     -2.37   0.0177\n 9 diagnosis_X2   0.195     0.594      0.329  0.742 \n10 diagnosis_X3   1.26      0.542      2.33   0.0200\n\n\nBased on the newly trained model BrainCancer_fit2, only Meningioma and LG glioma show significant association with survival (p-value < 0.05) when compared to global average. When keeping all other variable constant, compare to global average, the patient with Meningioma has only 32.9% (exp(-1.11018843)) average survive rate, while the patient with LG glioma are 3.5 times (exp(1.26008400)) more likely to survive.\nMore about coding contrasts in base R syntax can be found at this article.\n\n\nMultinomial outcome\nUsing the same dataset BrainCancer, now I try to predict the diagnosis based on the tumor location (loc), Karnofsky index (ki), Gross tumor volume (gtv) and Stereotactic method (stereo). Here we used multinom_reg() to model multinomial regression\n\n\nCode\noptions(contrasts = contr_opt) # reset contrasts options back to `contr.treatment`\n\nml_spec <- multinom_reg() %>% \n    set_engine('nnet') %>% \n    set_mode('classification')\n\nBrainCancer_rec3 <- recipe(diagnosis ~ loc + ki + gtv + stereo, data = BrainCancer) %>% \n    update_role(diagnosis, new_role = 'outcome') %>% \n    step_normalize(all_numeric_predictors()) %>% \n    step_dummy(all_nominal_predictors())\n\nBrainCancer_wf3 <- workflow() %>% \n    add_model(ml_spec) %>% \n    add_recipe(BrainCancer_rec3)\n\nBrainCancer_fit3 <- BrainCancer_wf3 %>% \n    fit(data = BrainCancer)\n\nBrainCancer_fit3\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nCall:\nnnet::multinom(formula = ..y ~ ., data = data, trace = FALSE)\n\nCoefficients:\n          (Intercept)          ki         gtv loc_Supratentorial stereo_SRT\nLG glioma  -2.3035689  0.23860763 -0.02596393          0.3998414  0.5444269\nHG glioma  -2.5894735  0.03684929  0.15897113          0.9417737  1.3658683\nOther      -0.4158848 -0.29780559  0.14203552         -2.7892771  1.4289732\n\nResidual Deviance: 187.5196 \nAIC: 217.5196 \n\n\nIn the multinomial regression, no p-value were reported. The coefficients represent log odds ratio.\nEach row in the coefficient table corresponds to the model equation. eg. the first row represents the coefficients for LG glioma in comparison to our baseline Meningioma. Each column in the coefficient table corresponds to specific coefficient estimate. Thus, compared to Meningioma, using SRT Stereotactic method is about 4 times (exp(1.3658683)) more likely diagnose HG glioma. A tumor is only 6% (exp(-2.7892771)) chance to be diagnosed as Other instead of Meningioma if it is located at Supratentorial area.\nTo perform above model in base R syntax, please refer to the blog post by Mohit Sharma."
  },
  {
    "objectID": "posts/2022-02-28-glm_coefficients.html#poisson-regression",
    "href": "posts/2022-02-28-glm_coefficients.html#poisson-regression",
    "title": "Interpreting the coefficients of Generalized Linear Model",
    "section": "Poisson regression",
    "text": "Poisson regression\nPoisson regression is used to model count outcome. Unlike regular linear regression, count outcome is not real continuous variable. Instead, it must be positive integer and usually modeled by Poisson distribution rather than normal distribution.\nThe link function for Poisson regression is log function \\(\\ln\\lambda\\) where \\(\\lambda\\) represents the mean of outcome.\nIn the following example, we use Bikeshare data to predict bikers outcome which represents the count of rental bikers\n\n\nCode\ndata('Bikeshare')\n\nBikeshare_rec <- recipe(bikers ~ season + weekday + weathersit + temp + hum + windspeed, data = Bikeshare) %>% \n    step_num2factor(season, levels = c(\"winter\",'spring','summer','fall')) %>%\n    step_num2factor(weekday, transform = function(x) {x+1}, levels = c('sunday','monday','tuesday','wednesday','thursday','friday','saturday')) %>%\n    step_normalize(all_numeric_predictors()) %>%\n    step_dummy(all_nominal_predictors()) %>%\n    I()\n\n# Bikeshare_rec %>% prep() %>% bake(new_data = NULL)\nlibrary(poissonreg)\npr_spec <- poisson_reg() %>% \n    set_engine('glm') %>% \n    set_mode('regression')\n\nBikeshare_wf <- workflow() %>% \n    add_recipe(Bikeshare_rec) %>% \n    add_model(pr_spec)\n\nBikeshare_fit <- Bikeshare_wf %>%\n     parsnip::fit(Bikeshare)\n\nBikeshare_fit %>%\n    tidy()\n\n\n# A tibble: 16 × 5\n   term                       estimate std.error statistic   p.value\n   <chr>                         <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)                  4.58    0.00385   1191.    0        \n 2 temp                         0.426   0.00149    285.    0        \n 3 hum                         -0.256   0.00108   -238.    0        \n 4 windspeed                    0.0404  0.000949    42.6   0        \n 5 season_spring                0.302   0.00384     78.7   0        \n 6 season_summer                0.144   0.00449     32.1   1.12e-226\n 7 season_fall                  0.613   0.00345    177.    0        \n 8 weekday_monday              -0.0464  0.00336    -13.8   1.82e- 43\n 9 weekday_tuesday             -0.0405  0.00336    -12.1   1.42e- 33\n10 weekday_wednesday           -0.0524  0.00342    -15.3   4.88e- 53\n11 weekday_thursday            -0.0804  0.00339    -23.7   2.70e-124\n12 weekday_friday              -0.0151  0.00335     -4.51  6.47e-  6\n13 weekday_saturday            -0.0187  0.00336     -5.58  2.36e-  8\n14 weathersit_cloudy.misty      0.106   0.00223     47.4   0        \n15 weathersit_light.rain.snow  -0.163   0.00425    -38.4   0        \n16 weathersit_heavy.rain.snow  -0.0368  0.167       -0.221 8.25e-  1\n\n\n\n\nCode\ncontrasts(Bikeshare$weathersit)\n\n\n                cloudy/misty light rain/snow heavy rain/snow\nclear                      0               0               0\ncloudy/misty               1               0               0\nlight rain/snow            0               1               0\nheavy rain/snow            0               0               1\n\n\nAll terms except weathersit_heavy.rain.snow are significantly associated with rental bikers number. - when keeping all other variables constant, compared to season_winter, season_spring will increase the mean of rental biker count by 1.35 exp(0.30234965). In other words, there will be 135% bikers rental a bike in spring than winter. - when keeping all other variables constant, every unit increase in temperature will result in on average 1.53 (exp(0.42588059)) rental biker customer.\nnote:above model is not optimal model to predict rental bikers. We use the model without interactions to simplify the question and emphasize interpretation of coefficients in the context of poisson regression . To interpret the coefficients with interaction term, refer to previous regular linear regression example"
  },
  {
    "objectID": "posts/2022-02-28-glm_coefficients.html#final-remarks",
    "href": "posts/2022-02-28-glm_coefficients.html#final-remarks",
    "title": "Interpreting the coefficients of Generalized Linear Model",
    "section": "Final remarks",
    "text": "Final remarks\nIn this post, I focus on interpret the coefficients in three GLM, and show the examples of coefficients associated with both quantitative and qualitative predictors. I also include the examples to interpret coefficients when 1) add interaction term, 2) with multi-nominal outcome and 3) with alternative contrast matrix."
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "",
    "text": "As a DS beginner, I first came across ML in R by studying the book Hands-On Machine Learning with R. The book mainly focuses on the package caret with general introductions to packages like recipe and h2o. Most examples use the workflow in which feature engineering is performed by recipe and the modeling/learning part is done using caret.\nIt was a great pleasure to take the tidymodels workshop hosted by Dr. Alison Hill last week. tidymodels was recently launched as a collection of packages for ML using tidyverse principles. It is built on recipes for feature engineering and parsnip as the major modeling package, and links ML steps together with workflow.\nIn this post, I am going to present the general ML frameworks using caret and tidymodels, independently. The data used as an example is “Watson churn data” from modeldata"
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#required-libraries-and-data",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#required-libraries-and-data",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "0. required libraries and data",
    "text": "0. required libraries and data\nSince tidymodels is a collection of packages like tidyverse, we can just use library(tidymodels) to load all the required libraries for the tidymodels pipeline.\n\n\nCode\nlibrary(tidymodels)\n\n\nFor the caret pipeline, additional helper pacakges, like recipes and rsample, were needed to process the data. Most of those packages are already collected in the tidymodels pipeline.\n\n\nCode\nlibrary(caret)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(vip)\n\n\nFor the data, I used “Watson churn data” from modeldata which is also a part of tidymodels.\n\n\nCode\nlibrary(tidyverse)\n\n\n\n\nCode\nlibrary(modeldata)\ndata(wa_churn)\n# quick view and summarize data\nglimpse(wa_churn)\n# visdat::vis_miss(wa_churn)\nHmisc::describe(wa_churn)\n\n# relevel factors\nwa_churn = wa_churn %>% \n        mutate(churn=relevel(churn, ref=\"No\")) %>% \n        mutate(multiple_lines=relevel(multiple_lines, ref=\"No phone service\")) %>% \n        mutate(internet_service=relevel(internet_service, ref=\"No\")) %>% \n        mutate(online_security=relevel(online_security, ref=\"No internet service\")) %>% \n        mutate(online_backup=relevel(online_backup, ref=\"No internet service\")) %>% \n        mutate(device_protection=relevel(device_protection, ref=\"No internet service\")) %>% \n        mutate(tech_support=relevel(tech_support, ref=\"No internet service\")) %>% \n        mutate(streaming_tv=relevel(streaming_tv, ref=\"No internet service\")) %>% \n        mutate(streaming_movies=relevel(streaming_movies, ref=\"No internet service\")) %>% \n        mutate(contract=relevel(contract, ref=\"Month-to-month\"))\n\n# to simplify the case here, we are going to remove missing variable\nwa_churn = wa_churn %>% \n        na.omit"
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#data-split",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#data-split",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "1. data split",
    "text": "1. data split\nBoth frameworks use rsample::initial_split to split the data into training and testing data. Here, we choose the standard 7:3 split between training and testing, with stratification on the target variable “churn”\n\n\nCode\n# split\nset.seed(123)\ndata_splits = initial_split(wa_churn, strata=\"churn\", prob=0.7)\ndata_train=training(data_splits)\ndata_test=testing(data_splits)\n\n\nTo stratify on the numeric variables, we can add the breaks parameter.\n\n\nCode\ninitial_split(wa_churn, strata=\"tenure\", prob=0.7, breaks=4)"
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#feature-engineer",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#feature-engineer",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "2. feature engineer",
    "text": "2. feature engineer\nGeneral feature engineering steps include\n\nremoving variables with zero variance or near zero variance: step_zv, step_nzv\nlumping nominal variables: step_other\nnormalizing (scale + center) numeric variables (specific for regression-based models): step_scale, step_center, step_normalize\nencoding nominal variables to dummy features: step_novel + step_dummy, step_integer\nvalue transformation to fit normal distribution: step_log, step_YeoJohnson, step_BoxCox\nfeature dimension reduction: step_pca\ndealing with missing values with imputation: step_medianimpute, step_knnimpute, step_bagimpute\n\nFeature engineering is done by recipes in both tidymodels and caret. The functions in recipes starts with step_* and create a blueprint for feature engineering. The complete list of step is at https://tidymodels.github.io/recipes/reference/index.html\n\n\nCode\nrec <- recipe(churn ~ ., data = wa_churn) %>%\n        step_zv(all_predictors()) %>% \n        step_nzv(all_predictors())  %>%\n        step_novel(all_nominal(), -all_outcomes()) %>% \n        step_dummy(all_nominal(), -all_outcomes())\n\n\nThe blueprint will not change the data until it is fit in the modeling step. We can use prep and bake to see “transformed” data in data.frame.\n\n\nCode\nrec %>% \n        prep(wa_churn) %>% \n        bake(wa_churn)\n\n\nOne reason to use recipe is to avoid data leakage. Data leakage is when information from outside the training data set is used to create the model."
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#resample",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#resample",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "3. resample",
    "text": "3. resample\nResampling methods split the training data into additional sets. It will generate train set and validation set. Typical resampling method include cross-validation (cv), repeated cross-validation (repeated cv), leave-one-out and bootstrapping (with replacement).\nWe can use rsample::vfold_cv for both caret and tidymodels pipeline.\n\n\nCode\n# 10 fold cross validation stratified on target variable churn\ncv_folds = rsample::vfold_cv(data=data_train, v=10, strata=churn)\n\n\nHowever to make above cv_folds compatible with caret, we need to used rsample2caret to convert a trainControl list\n\n\nCode\ncv_folds_cr = rsample2caret(cv_folds)\ncv_folds_trCtrl = trainControl(\n        method = \"cv\",\n        verboseIter = FALSE,\n        classProbs = TRUE,\n        summaryFunction = twoClassSummary,\n        returnResamp = \"final\",\n        savePredictions = \"final\",\n        index = cv_folds_cr$index,\n        indexOut = cv_folds_cr$indexOut\n  )\n\n\nOr we can simply use caret function trainControl function to generate split. However, no stratify option is available here.\n\n\nCode\ncv_folds_trCtrl = trainControl(method = \"cv\", number=10)"
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#hyperparameters-grid",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#hyperparameters-grid",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "4. hyperparameters grid",
    "text": "4. hyperparameters grid\nA hyperparameter is a parameter whose value is set before the learning process begins. It is distinguished from other parameters by the fact that it is not used for fitting the machine to the training set. For different models, there are a different number of hyperparameters you can tune. Here I choose to use random forest to model the data. The hyperparameters for random forest from ranger include\n\nthe number of trees – num.trees or trees\ndepth of tree – max.depth\nnumber of features to consider at every split – mtry\nminimum number of samples required to split a node – min.node.size or min_n\nwhether using boostrapping to select samples for training – replace.\nfraction of observation to sample – sample.fraction. Specifying sample.fraction requires replace being set as TRUE\n\nA rule of thumb to start is\n\nnum.trees start with 10x p (p means number of features).\nmax.depth\nmtry: sqrt(p) for classification and p/3 for regression\nmin.node.size default values of 1 for classification and 5 for regression\nreplace and sample.fraction: Default is 1 for sampling with replacement and 0.632 for sampling without replacement.\n\n\n\nCode\nhyp_grid = expand.grid(\n        trees = c(500,1000),\n        mtry=c(5,15),\n        min_n=c(10,20)\n)\n\n\nThe hyperparameters can be checked by function args(rand_forest)\nrf method (from RandomForest) for caret has only one hyperparameter (mtry) by default.\n\n\nCode\nhyp_grid_cr = expand.grid(\n        mtry=5:10\n)"
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#fit-model",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#fit-model",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "5. fit model",
    "text": "5. fit model\nHere is the step where tidymodel and caret start to diverge in syntax. Typically, tidymodel builds a model using workflow pipe which specifies formular/recipe and model, while caret uses train to fit model.\ntidymodel\ndefault version of model fit fit_resamples\n\n\nCode\n# without grid_tune -> fit_resamples() at train\nrf_tm <- rand_forest() %>% \n        set_engine(\"ranger\", importance=\"permutation\") %>% \n        set_mode(\"classification\")\n\nrf_tm_wf <- workflow() %>% \n        add_model(rf_tm) %>% \n        add_recipe(rec)\n\nset.seed(123)\ndefault_tm_fit= rf_tm_wf %>% \n        fit_resamples(\n                resamples = cv_folds,\n                control = control_resamples(save_pred = TRUE)\n                )\n\n\ngrid version of model fit grid_tune\n\n\nCode\n# with grid_tune -> set tune() at model, use tune_grid() at train\nrf_tm <- rand_forest(\n                mtry=tune(), \n                trees=tune(), \n                min_n=tune()\n        ) %>% \n        set_engine(\"ranger\", importance=\"impurity\") %>% \n        set_mode(\"classification\")\n\nrf_tm_wf <- workflow() %>% \n        add_model(rf_tm) %>% \n        add_recipe(rec)\n\nset.seed(123)\ngrid_tm_fit = rf_tm_wf %>% \n        tune_grid(resamples = cv_folds,\n            grid = hyp_grid,\n                control = control_grid(save_pred = TRUE)\n            )\n\n\nNotes: 1. control specification will be control_grid() in grid_tune() 2. grid parameter here can also be a integer which test for top N parameters.\nFollow the thread https://github.com/tidymodels/parsnip/issues/235 to find how to print out default hyperparameters.\ncaret\ndefault version of model fit\n\n\nCode\n# without hyp grid\nset.seed(123)\ndefault_cr_fit=train(\n        rec,\n        data = data_train,\n        method = \"rf\",\n        trControl = cv_folds_trCtrl,\n        metric = \"ROC\"\n)\n\n\ngrid version of model fit\n\n\nCode\n# with grid --- tuneGrid\nset.seed(123)\ngrid_cr_fit=train(\n        rec,\n        data = data_train,\n        method = \"rf\",\n        trControl = cv_folds_trCtrl,\n        tuneGrid = hyp_grid_cr,\n        metric = \"ROC\"\n)"
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#collect-metrics",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#collect-metrics",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "6. collect metrics",
    "text": "6. collect metrics\nMetrics are used to determine how good the model fit. For classification problem, accuracy and ROC/AUC are commonly used. For regression problem, RSEM is the most commonly used approach.\nWe used collect_metrics in tidymodels\n\n\nCode\n# for default model\ndefault_tm_fit %>% collect_metrics()\n\n# for grid tune model\ngrid_tm_fit %>% collect_metrics()\n\n\nlist results stores metrics for caret\n\n\nCode\ndefault_cr_fit$results\n\n\nFrom the results, we can tell that train fit 3 hyperparameters by default."
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#collect-prediction-for-training-data",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#collect-prediction-for-training-data",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "7. collect prediction for training data",
    "text": "7. collect prediction for training data\nBesides model metrics, we also care about what predicted value of target variable is in training data.\ntidymodels\nTo see predicted target value for data_train, we can use collect_predictions.\n\n\nCode\ndefault_tm_fit %>% \n        collect_predictions()\n\n# plot auc\nautoplot(\n        roc_curve(\n                default_tm_fit %>% collect_predictions(), churn, .pred_Yes\n        )\n)\n\n\nNotes: collect_predictions() only works when specifying save_pred = TRUE in control.\ncaret\n\n\nCode\ndefault_cr_fit$pred %>% tbl_df\n\n# plot auc\nautoplot(\n        roc_curve(\n                default_cr_fit$pred %>% tbl_df, \n                obs, Yes\n        )\n)\n\n\nFor both caret and tidymodels, it is possible that each row of the original data point might be represented multiple times per tuning paramete if boostrap or repeated cv is used"
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#collect-prediction-for-testing-data",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#collect-prediction-for-testing-data",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "8. collect prediction for testing data",
    "text": "8. collect prediction for testing data\nFor default fit, only one set of hyperparameters is specified, thus we can just apply the fitted model to data_test. However, for grid fit, we end up with multiple sets of hyperparameters. Thus, before fitting the model, we need to pick the best set of hyperparameters based on metrics on training data (which is summarized using specified rsample method), then apply the best model to test_data\ntidymodels\nlast_fit is a function that is applied to workflow and fits to test data. By default, it generates predictions that can be reported by collect_prediction (no need to specify control in the fit). We can also use collect_metrics to check the metrics in testing data.\n\n\nCode\n# default\ndefault_last_fit = rf_tm_wf %>% \n        last_fit(split = data_splits)\n\ndefault_last_fit %>% \n        collect_metrics()\n\ndefault_last_fit %>% \n        collect_predictions()\n\n\nTo select best set of hyperparameters from grid_tune, we use select_best by specifying which metrics to use. Then we apply this set of hyperparameters to original workflow by finalize_workflow. Finally, like default, apply last_fit to the best workflow and get predictions and metrics for the testing data\n\n\nCode\n# grid tune\nbest_hyp <- grid_tm_fit %>% \n        select_best(metric = \"roc_auc\")\n\nbest_wf <- rf_tm_wf %>%\n        finalize_workflow(best_hyp)\n\ngrid_last_fit <- best_wf %>% \n  last_fit(split = data_splits)\n\ngrid_last_fit %>% \n        collect_metrics()\n\ngrid_last_fit %>% \n        collect_predictions()\n\n\ncaret\nThe predict function can be directly applied to fitted model to test data. For grid fit, it will automatically detect the best hyperparameters (here mtry=5) and apply it to the testing data.\n\n\nCode\n# default\ntest_prediction <- predict(\n    default_cr_fit,\n    newdata = data_test,\n    type = \"prob\") %>%\n  as_tibble() %>%\n        transmute(estimate=Yes) %>%  # for binary result we can randomly pick one, it will be same roc_auc\n  add_column(churn = data_test$churn) \n\n## auc\nroc_auc(test_prediction, churn, estimate)$.estimate\n## accuracy\ntest_prediction %>% \n        mutate(.pred=ifelse(estimate > 0.5, \"Yes\",\"No\")) %>% \n        summarise(accuracy=mean(.pred==churn)) %>% \n        pull(accuracy)\n\n\n\n\nCode\n# grid\n# best hyp\ngrid_cr_fit$results %>% \n        slice(which.max(ROC))\n\ntest_prediction2 <- predict(\n    grid_cr_fit,\n    newdata = data_test,\n    type = \"prob\") %>%\n  as_tibble() %>%\n        transmute(estimate=Yes) %>%  # for binary result we can randomly pick one, it will be same roc_auc\n  add_column(churn = data_test$churn) \n\n## auc\nroc_auc(test_prediction2, churn, estimate)$.estimate\n## accuracy\ntest_prediction2 %>% \n        mutate(.pred=ifelse(estimate > 0.5, \"Yes\",\"No\")) %>% \n        summarise(accuracy=mean(.pred==churn)) %>% \n        pull(accuracy)"
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#importance-of-variables",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#importance-of-variables",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "9. importance of variables",
    "text": "9. importance of variables\nLastly, we can use fit result to find most important variables by vip package or caret function varImp. Be aware that, for tidymodels different importance specified in the model will result in different ranks\ntidymodels\n\n\nCode\nrf_fit <- pull_workflow_fit(default_last_fit$.workflow[[1]])\nvip::vip(rf_fit)$data\nvip::vip(rf_fit, geom = \"point\")\n\n# model-specific variable importance scores are currently not available for objects of class \"_rangermodel_fit\"\n\n\ncaret\n\n\nCode\nvarImp(default_cr_fit, scale = TRUE)$importance %>% tbl_df"
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#summary",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#summary",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "Summary",
    "text": "Summary\nThe following table summarizes the ML workflow using caret versus tidymodels:"
  },
  {
    "objectID": "posts/2020-04-30_caret_vs_tidymodels.html#thank-you",
    "href": "posts/2020-04-30_caret_vs_tidymodels.html#thank-you",
    "title": "ML pipeline with tidymodels vs. caret",
    "section": "Thank you",
    "text": "Thank you\nSpecial thanks to Amy Goodwin Davies who helped editing and proof-reading this post!"
  },
  {
    "objectID": "posts/2022-01-18-chocolate.html",
    "href": "posts/2022-01-18-chocolate.html",
    "title": "TidyTuesday: predict chocolate rating with xgboost",
    "section": "",
    "text": "Load required libraries\nData README is available at here."
  },
  {
    "objectID": "posts/2022-01-18-chocolate.html#clean-data",
    "href": "posts/2022-01-18-chocolate.html#clean-data",
    "title": "TidyTuesday: predict chocolate rating with xgboost",
    "section": "Clean Data",
    "text": "Clean Data\n\n\nCode\nchocolate_raw <- tuesdata$chocolate\nchocolate_raw <- chocolate_raw %>% \n    mutate(cocoa_percent = parse_number(cocoa_percent)) %>% \n    separate(ingredients, c(\"ingredient_num\",\"ingredients\"), sep=\"-\") %>% \n    mutate(\n        ingredient_num=parse_number(ingredient_num),\n        ingredients=str_trim(ingredients)\n    ) %>% \n    mutate(ingredients = map(ingredients, ~str_split(.x, \",\")[[1]])) %>% \n    mutate(most_memorable_characteristics=map(most_memorable_characteristics, ~str_split(.x,\",\")[[1]])) %>% \n    mutate(most_memorable_characteristics=map(most_memorable_characteristics, ~str_trim(.x))) %>% \n    # select(cocoa_percent, ingredient_num, ingredients, most_memorable_characteristics) %>%\n    I()\n\n\n\nConvert gredients to boolean columns\n\nusing unnest to spread out the list column ingredients.\n\n\nCode\ngredients <- chocolate_raw %>% \n    mutate(line_n = row_number()) %>% \n    select(line_n, ingredients) %>% \n    unnest(cols=c(ingredients)) %>% \n    mutate(tmp=1) %>% \n    pivot_wider(names_from=ingredients, values_from=tmp) %>% \n    select(-\"NA\") %>% \n    janitor::clean_names() %>% \n    mutate_at(vars(-line_n), ~ifelse(is.na(.x),0,.x)) %>% \n    I()\n\n\n\nConvert most_memorable_characteristics to boolean columns\n\n\n\nCode\nmost_memorable_characteristics <- chocolate_raw %>% \n    mutate(line_n = row_number()) %>% \n    select(line_n, most_memorable_characteristics) %>% \n    unnest(cols=c(most_memorable_characteristics)) %>% \n    mutate(tmp=1) %>% \n    # distinct(most_memorable_characteristics) %>% \n    # pivot_wider(names_from=most_memorable_characteristics, values_from=tmp) %>% \n    I()\n\n\nThere are 972 most_memorable_characteristics in total\n\n\nCode\nmost_memorable_characteristics %>% \n    # mutate(most_memorable_characteristics = fct_lump_min(most_memorable_characteristics, min=100)) %>% \n    group_by(most_memorable_characteristics) %>% \n    count(sort=T) %>% \n    head(20) %>% \n    ggplot(aes(x=n, y=reorder(most_memorable_characteristics,n))) +\n    geom_col() +\n    geom_text(aes(label=n), color=\"white\", hjust=1) +\n    theme_bw() +\n    labs(x=\"# of chocolates\", y=\"most memorable characteristics\")\n\n\n\n\n\nPick top 12 most_memorable_characteristics to convert to boolean column\n\n\nCode\nmost_memorable_characteristics <- most_memorable_characteristics %>% \n    mutate(most_memorable_characteristics = fct_lump_min(most_memorable_characteristics, min=100)) %>% \n    distinct() %>% \n    pivot_wider(names_from=most_memorable_characteristics, values_from=tmp) %>% \n    mutate_at(vars(-line_n), ~ifelse(is.na(.x),0,.x))\n\n\n\ncreate chocolate_clean data\n\n\n\nCode\nchocolate_clean <-\n    chocolate_raw %>% \n    mutate(line_n=row_number()) %>% \n    select(-ingredients, -most_memorable_characteristics) %>% \n    left_join(gredients) %>%\n    left_join(most_memorable_characteristics) %>%\n    I()"
  },
  {
    "objectID": "posts/2022-01-18-chocolate.html#explore-data",
    "href": "posts/2022-01-18-chocolate.html#explore-data",
    "title": "TidyTuesday: predict chocolate rating with xgboost",
    "section": "Explore Data",
    "text": "Explore Data\nSeveral features are explored in terms of their association with rating.\n\ncountry_of_bean_origin\n\n\n\nCode\nchocolate_clean %>% \n    mutate(country_of_bean_origin = fct_lump(country_of_bean_origin, n=10)) %>% \n    ggplot(aes(x=rating, y=country_of_bean_origin)) +\n    geom_boxplot(aes(fill=country_of_bean_origin)) +\n    theme_bw()\n\n\n\n\n\nBlend and non-blend on country_of_bean_origin shows big difference, thus we convert country_of_bean_origin to country_of_bean_origin_blend\n\n\nCode\nchocolate_clean <- chocolate_clean %>% \n    mutate(country_of_bean_origin_blend = ifelse(country_of_bean_origin==\"Blend\", country_of_bean_origin, \"Non-blend\"))\n\n\n\ncompany_manufacturer and company_location\n\n\n\nCode\nchocolate_clean %>% \n    mutate(company_manufacturer = fct_lump(company_manufacturer, prop=0.01)) %>% \n    ggplot(aes(x=rating, y=reorder(company_manufacturer, rating, median))) +\n    geom_boxplot(aes(fill=company_manufacturer)) +\n    theme_bw() +\n    labs(y=\"company_manufacturer\")\n\n\n\n\n\n\n\nCode\nchocolate_clean %>% \n    mutate(company_location = fct_lump(company_location, n=5)) %>% \n    ggplot(aes(x=rating, y=reorder(company_location, rating))) +\n    geom_boxplot(aes(fill=company_location)) +\n    theme_bw() +\n    labs(y=\"company_location\")\n\n\n\n\n\n\ncocoa_percent\n\n\n\nCode\nchocolate_clean %>% \n    ggplot(aes(x=cocoa_percent, y=rating)) +\n    geom_point(aes(color=as.factor(cocoa))) +\n    theme_bw() +\n    theme(legend.position = \"bottom\") +\n    labs(color=\"cocoa as most_memorable_characteristics\")\n\n\n\n\n\nrating is not as continuous as what i originally imagined. Thus, I convert rating to nominal variable rating_bl using 3 as threshold\n\n\nCode\nchocolate_clean <- chocolate_clean %>% \n    mutate(rating_bl = ifelse(rating >= 3, \">=3\", \"< 3\"))\n\nchocolate_clean %>% \n    group_by(rating_bl) %>% \n    count()\n\n\n# A tibble: 2 × 2\n# Groups:   rating_bl [2]\n  rating_bl     n\n  <chr>     <int>\n1 < 3         566\n2 >=3        1964\n\n\n\n\nCode\nchocolate_clean %>% \n    ggplot(aes(x=cocoa_percent, y=rating_bl)) +\n    geom_boxplot(aes(fill=as.factor(cocoa))) +\n    theme_bw() +\n    labs(y=\"rating\", fill=\"cocoa as most_memorable_characteristics\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nmost_memorable_characteristics\n\n\n\nCode\nchocolate_clean <- chocolate_clean %>% \n    mutate_at(vars(Other:creamy), as.factor)\n\n\nmost_memorable_characteristics like cocoa and creamy positive effect rating, while fatty, earthy, sandy, sour and sweet negatively effect rating.\n\n\nCode\nchocolate_clean %>% \n    select(rating, fatty:creamy) %>% \n    pivot_longer(!rating, names_to=\"most_memorable_characteristics\", values_to=\"yes\") %>%\n    ggplot(aes(y=reorder(most_memorable_characteristics, rating, FUN=median), x=rating)) +\n    geom_boxplot(aes(fill=yes)) +\n    theme_bw() +\n    theme(\n        legend.position = \"bottom\"\n    ) +\n    scale_fill_discrete(labels = c(\"0\"=\"No\", \"1\"=\"Yes\")) +\n    labs(y=\"most_memorable_characteristics\", fill=\"is most_memorable_characteristics?\") +\n    NULL\n\n\n\n\n\n\ningredients\n\n\n\nCode\nchocolate_clean <- \n    chocolate_clean %>% \n    dplyr::rename(igrdt_beans=b, igrdt_sugar=s, igrdt_cocoa=c, igrdt_lecithin=l, igrdt_vanilla=v, igrdt_salt=sa, igrdt_sweeter=s_2) %>% \n    mutate_at(vars(contains(\"igrdt_\")), as.factor)\n\n\ningredient number ingredient_num between 2-3 are associated with higher rating.\n\n\nCode\nchocolate_clean %>% \n    mutate(ingredient_num=as.factor(ingredient_num)) %>% \n    filter(!is.na(ingredient_num)) %>% \n    ggplot(aes(x = ingredient_num, y=rating)) +\n    geom_boxplot()\n\n\n\n\n\ningrediants like beans and sugar positively effect rating, while vanilla, sweeter and salt negatively effect rating.\n\n\nCode\nchocolate_clean %>% \n    select(rating, contains(\"igrdt_\")) %>% \n    pivot_longer(!rating, names_to=\"ingredients\", values_to=\"yes\") %>%\n    mutate(ingredients = gsub(\"igrdt_\",\"\",ingredients)) %>% \n    ggplot(aes(y=reorder(ingredients, rating, FUN=median), x=rating)) +\n    geom_boxplot(aes(fill=yes)) +\n    theme_bw() +\n    theme(\n        legend.position = \"bottom\"\n    ) +\n    scale_fill_discrete(labels = c(\"0\"=\"No\", \"1\"=\"Yes\")) +\n    labs(y=\"ingredients\", fill=\"contain the ingredient?\") +\n    NULL"
  },
  {
    "objectID": "posts/2022-01-18-chocolate.html#ml",
    "href": "posts/2022-01-18-chocolate.html#ml",
    "title": "TidyTuesday: predict chocolate rating with xgboost",
    "section": "ML",
    "text": "ML\nBased on the exploratory analysis, to study the effect on overall rating of chocolates, the following features are selected for building ML models. Plus, using nominal feature rating_bl instead of numeric feature rating as outcome.\n\n\nCode\nchocolate_df <- chocolate_clean %>% \n    select(rating_bl, company_manufacturer, country_of_bean_origin_blend, cocoa_percent, ingredient_num, contains('igrdt_'), cocoa, creamy, fatty, earthy, sandy, sour, sweet) %>% \n    select(-igrdt_cocoa, -igrdt_lecithin) %>% \n    na.omit()\n\n\n\nsplit samples\n\ninitial_split\n\n\n\nCode\nset.seed(123)\nchocolate_split <- initial_split(chocolate_df, strata = rating_bl)\nchocolate_train <- training(chocolate_split)\nchocolate_testing <- testing(chocolate_split)\n\n\n\nresample\n\n\n\nCode\nset.seed(123)\nfolds <- vfold_cv(chocolate_train, v = 10)\nfolds\n\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   <list>             <chr> \n 1 <split [1647/184]> Fold01\n 2 <split [1648/183]> Fold02\n 3 <split [1648/183]> Fold03\n 4 <split [1648/183]> Fold04\n 5 <split [1648/183]> Fold05\n 6 <split [1648/183]> Fold06\n 7 <split [1648/183]> Fold07\n 8 <split [1648/183]> Fold08\n 9 <split [1648/183]> Fold09\n10 <split [1648/183]> Fold10\n\n\n\n\nrecipe\n\n\nCode\nchocolate_rec <- \n    recipe(rating_bl ~ ., data = chocolate_train) %>% \n    step_other(company_manufacturer, threshold=0.01, other=\"otherCompany\") %>% \n    # step_mutate_at(c(\"company_manufacturer\",\"country_of_bean_origin_blend\", \"rating_bl\"), fn = ~as.factor(.x)) %>% \n    step_dummy(all_nominal_predictors()) %>% \n    step_zv(all_predictors())\n\nchocolate_rec\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         16\n\nOperations:\n\nCollapsing factor levels for company_manufacturer\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()\n\n\ncheck preprocessed data.frame\n\n\nCode\nchocolate_rec %>% \n    prep(new_data = NULL) %>% \n    juice()\n\n\n# A tibble: 1,831 × 20\n   cocoa_percent ingre…¹ ratin…² compa…³ compa…⁴ compa…⁵ compa…⁶ compa…⁷ count…⁸\n           <dbl>   <dbl> <fct>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1            70       4 < 3           0       0       0       0       0       1\n 2            70       4 < 3           0       0       0       0       0       1\n 3            60       3 < 3           0       0       0       0       1       1\n 4            70       2 < 3           0       0       0       0       1       1\n 5            70       2 < 3           0       0       0       0       1       1\n 6            75       4 < 3           0       0       0       0       1       1\n 7            75       4 < 3           0       0       0       0       1       1\n 8            75       5 < 3           0       0       0       0       1       1\n 9            75       5 < 3           0       0       0       0       1       1\n10            65       6 < 3           0       0       0       0       1       1\n# … with 1,821 more rows, 11 more variables: igrdt_sugar_X1 <dbl>,\n#   igrdt_vanilla_X1 <dbl>, igrdt_salt_X1 <dbl>, igrdt_sweeter_X1 <dbl>,\n#   cocoa_X1 <dbl>, creamy_X1 <dbl>, fatty_X1 <dbl>, earthy_X1 <dbl>,\n#   sandy_X1 <dbl>, sour_X1 <dbl>, sweet_X1 <dbl>, and abbreviated variable\n#   names ¹​ingredient_num, ²​rating_bl, ³​company_manufacturer_Arete,\n#   ⁴​company_manufacturer_Bonnat, ⁵​company_manufacturer_Fresco,\n#   ⁶​company_manufacturer_Soma, ⁷​company_manufacturer_otherCompany, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\n\ngrid tune xgboost\n\ncreate model boost_tree\n\nDetails about boost_tree can be found https://parsnip.tidymodels.org/reference/details_boost_tree_xgboost.html\nrequire library xgboost installed.\n\n\nCode\nxg_spec <- \n    boost_tree(\n        mtry=tune(), # the number (or proportion) of predictors that will be randomly sampled\n        min_n=tune() # minimum number of data points in a node\n    ) %>% \n    set_engine(\"xgboost\") %>% # importance=\"permutation\"\n    set_mode('classification')\n\n\n\ndefine grid\n\ngrid_max_entropy, grid_regular, grid_random can be used for quickly specify levels for tuned hyperparameters.\nbe aware that mtry usually requires range parameters, it usually contains the sqrt(predictor_num)\n\n\nCode\nxg_grid <- grid_regular(\n    mtry(range = c(3, 10)),\n    min_n(),\n    levels = 5 # each tune how many levels\n)\n\nxg_grid\n\n\n# A tibble: 25 × 2\n    mtry min_n\n   <int> <int>\n 1     3     2\n 2     4     2\n 3     6     2\n 4     8     2\n 5    10     2\n 6     3    11\n 7     4    11\n 8     6    11\n 9     8    11\n10    10    11\n# … with 15 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\ncreate workflow\n\n\n\nCode\nxg_wf <- workflow() %>% \n    add_model(xg_spec) %>% \n    add_recipe(chocolate_rec)\n\nxg_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_other()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: xgboost \n\n\n\ntune model to get result\n\n\n\nCode\nsystem.time(\n    xg_res <- \n        xg_wf %>% \n        tune_grid(\n            resamples = folds,\n            grid = xg_grid\n            )\n    )\n\n\n   user  system elapsed \n 30.965   0.215  31.417 \n\n\n\nevaluate models\n\n\n\nCode\nxg_res %>% \n    collect_metrics() %>% \n    ggplot(aes(x = min_n, y=mean, color=as.factor(mtry))) +\n    facet_wrap(~.metric, scales=\"free\") +\n    geom_point() +\n    geom_line(aes(group=as.factor(mtry))) +\n    theme_bw() +\n    labs(y=\"metrics estimate\", x='minimum number of data points in a node (min_n)', color='the number of predictors that will be randomly sampled (mtry)') +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nselect hyperparameters and finalize wf\n\nshow_best(metric = ) allows to see the top 5 from xg_res %>% collect_metrics()\nselect_best, select_by_pct_loss, select_by_one_std_err select hyperparameters and corresponding .config to a tibble.\n\n\nCode\nxg_tune_hy <- xg_res %>% \n    select_best(metric = \"accuracy\")\n\nxg_tune_hy\n\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1    10    11 Preprocessor1_Model10\n\n\nfinalize model using selected hyperparameters\n\n\nCode\nfinal_wf <- \n  xg_wf %>% \n  finalize_workflow(xg_tune_hy)\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_other()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 10\n  min_n = 11\n\nComputational engine: xgboost \n\n\n\n\nlast_fit model\n\nuse last_fit(split)\n\n\n\nCode\nfinal_fit <- final_wf %>% \n    last_fit(chocolate_split)\n\nfinal_fit\n\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  <list>             <chr>            <list>   <list>   <list>       <list>    \n1 <split [1831/612]> train/test split <tibble> <tibble> <tibble>     <workflow>\n\n\n\ncollect_metrics for overall data\n\n\n\nCode\nfinal_fit %>% \n    collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.786 Preprocessor1_Model1\n2 roc_auc  binary         0.668 Preprocessor1_Model1\n\n\nmetrics are comparable to training data, so not overfiting.\n\ncollect_predictions for test data\n\n\n\nCode\nfinal_fit %>% \n    collect_predictions()\n\n\n# A tibble: 612 × 7\n   id               `.pred_< 3` `.pred_>=3`  .row .pred_class rating_bl .config \n   <chr>                  <dbl>       <dbl> <int> <fct>       <fct>     <chr>   \n 1 train/test split      0.141        0.859     3 >=3         >=3       Preproc…\n 2 train/test split      0.125        0.875    10 >=3         < 3       Preproc…\n 3 train/test split      0.0668       0.933    11 >=3         < 3       Preproc…\n 4 train/test split      0.156        0.844    17 >=3         >=3       Preproc…\n 5 train/test split      0.0668       0.933    24 >=3         >=3       Preproc…\n 6 train/test split      0.0668       0.933    25 >=3         >=3       Preproc…\n 7 train/test split      0.0711       0.929    32 >=3         >=3       Preproc…\n 8 train/test split      0.236        0.764    42 >=3         < 3       Preproc…\n 9 train/test split      0.491        0.509    46 >=3         < 3       Preproc…\n10 train/test split      0.385        0.615    55 >=3         < 3       Preproc…\n# … with 602 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nroc_auc and roc_curve on test data\n\ncalculate roc_auc manually on test data\n\n\nCode\nfinal_fit %>%\n  collect_predictions() %>% \n  roc_auc(truth=rating_bl, `.pred_< 3`)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.668\n\n\nplot roc_curve\n\n\nCode\nfinal_fit %>%\n  collect_predictions() %>% \n  roc_curve(truth=rating_bl, `.pred_< 3`) %>% \n  autoplot()\n\n\n\n\n\n\nextract_workflow() to save final_trained_wf\n\n\n\nCode\nfinal_trained_wf <- final_fit %>% \n    extract_workflow()\n\nfinal_trained_wf\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_other()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 21.7 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 0.526315789473684, \n    min_child_weight = 11L, subsample = 1, objective = \"binary:logistic\"), \n    data = x$data, nrounds = 15, watchlist = x$watchlist, verbose = 0, \n    nthread = 1)\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"0.526315789473684\", min_child_weight = \"11\", subsample = \"1\", objective = \"binary:logistic\", nthread = \"1\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 19 \nniter: 15\nnfeatures : 19 \nevaluation_log:\n    iter training_logloss\n       1        0.6020652\n       2        0.5525599\n---                      \n      14        0.4693209\n      15        0.4688216\n\n\n\nextract_* information from final_trained_wf\n\nextract_fit_engine() is engine-specific model\n\n\n\n\nCode\nfinal_trained_wf %>%\n  extract_fit_engine()\n\n\n##### xgb.Booster\nraw: 21.7 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 0.526315789473684, \n    min_child_weight = 11L, subsample = 1, objective = \"binary:logistic\"), \n    data = x$data, nrounds = 15, watchlist = x$watchlist, verbose = 0, \n    nthread = 1)\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"0.526315789473684\", min_child_weight = \"11\", subsample = \"1\", objective = \"binary:logistic\", nthread = \"1\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 19 \nniter: 15\nnfeatures : 19 \nevaluation_log:\n    iter training_logloss\n       1        0.6020652\n       2        0.5525599\n---                      \n      14        0.4693209\n      15        0.4688216\n\n\n\nextract_fit_parsnip() is parsnip model object\n\n\n\nCode\nfinal_trained_wf %>%\n  extract_fit_parsnip()\n\n\nparsnip model object\n\n##### xgb.Booster\nraw: 21.7 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 0.526315789473684, \n    min_child_weight = 11L, subsample = 1, objective = \"binary:logistic\"), \n    data = x$data, nrounds = 15, watchlist = x$watchlist, verbose = 0, \n    nthread = 1)\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"0.526315789473684\", min_child_weight = \"11\", subsample = \"1\", objective = \"binary:logistic\", nthread = \"1\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 19 \nniter: 15\nnfeatures : 19 \nevaluation_log:\n    iter training_logloss\n       1        0.6020652\n       2        0.5525599\n---                      \n      14        0.4693209\n      15        0.4688216\n\n\n\nextract_recipe or extract_preprocessing to get recipe/preprocessing\n\n\n\nCode\nfinal_trained_wf %>% extract_preprocessor()\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         16\n\nOperations:\n\nCollapsing factor levels for company_manufacturer\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()\n\n\n\n\nfeature importance\n\nvip() plot top 10\nvi_model() return tibble\n\n\n\nCode\nfinal_trained_wf %>%\n  extract_fit_parsnip() %>% \n  vip()"
  },
  {
    "objectID": "posts/2022-01-18-chocolate.html#final-notes",
    "href": "posts/2022-01-18-chocolate.html#final-notes",
    "title": "TidyTuesday: predict chocolate rating with xgboost",
    "section": "Final notes",
    "text": "Final notes\n\nI convert numeric rating to categorical rating using threshold because, based on the exploratory analysis, the rating values are not continuous.\nThe boost_tree did not produce good estimate for the data.\n\nOther models, like rand_forest(), logistic_reg and svm_linear are worth to try.\nTuning other hyperparameters tree_depth, learning_rate and trees are worth to try. I don’t know which tune-able hyperparameter corresponds to regularization gamma.\n\nJulia Silge posted a screencast and blog of using rand_forest() and svm_linear training rating as linear model on the same dataset."
  },
  {
    "objectID": "posts/2019-09-04_network_analysis_part3.html",
    "href": "posts/2019-09-04_network_analysis_part3.html",
    "title": "Network visualization - Part 3",
    "section": "",
    "text": "In the previous two posts, we discussed about IGRAPH object and how to manipulate, measure and cluster it. In this final post of network analysis series, I will focus on the network work visualization.\nNetwork visualization are supported by two aspects — the aesthetics of network elements (aka, vertices and edges) and layout of network. There are multiple packages available for these aspects. I will focus on the basic igraph plot which is base R plot and the application of ggraph which use similar syntax comparable to ggplot2."
  },
  {
    "objectID": "posts/2019-09-04_network_analysis_part3.html#vertex-aesthetics",
    "href": "posts/2019-09-04_network_analysis_part3.html#vertex-aesthetics",
    "title": "Network visualization - Part 3",
    "section": "Vertex aesthetics",
    "text": "Vertex aesthetics\nSpecify aesthetics in vertex attribute\n\n\nCode\n# make female and male color different\nv = as_data_frame(g, what=\"vertice\") %>% as_tibble %>% \n  mutate(color=case_when(gender==\"F\" ~ \"red\", gender==\"M\" ~ \"blue\"))\ng = g %>% set_vertex_attr(\"color\", value=v$color)\nplot(g)\n\n\n\n\n\nCode\n# make age as size\nv = v %>% \n  mutate(size=case_when(age < 30 ~ 10, age < 40 & age >30 ~ 20, age > 40 ~ 30))\ng = g %>% set_vertex_attr(\"size\", value=v$size)\nplot(g)\n\n\n\n\n\nThe methods mentioned above can also be done by specify in plot(). One quick example below show the shape aesthetics. Check igraph valid shape names by names(igraph:::.igraph.shapes)\n\n\nCode\n# make gender as shape\nv = v %>% \n  mutate(shape=case_when(gender==\"F\" ~ \"circle\", gender==\"M\" ~ \"rectangle\"))\n\nplot(g, vertex.shape=v$shape)\nlegend('topleft',legend=unique(v$gender),pch=c(21, 22),pt.bg=c(\"red\",\"blue\"))\n\n\n\n\n\nBe aware that the aesthetics specified by attributes can be overwritten by specifying in plot(). In addition, those aesthetics can also be used to apply to all vertices like plot(g, vertex.shape=\"rectangle\"). The attributes to be manipulated in igraph (using base R) are limited. To find all the plotting attributes, try ?plot.igraph or go to https://igraph.org/r/doc/plot.common.html\nWe can also draw attention to certain nodes by mark.groups in plot\n\n\nCode\n# mark dept\ng = g %>% set_vertex_attr(\"dept\",value=c(\"sale\",\"IT\",\"sale\",\"IT\",\"sale\")) %>% \n  set_edge_attr(\"same.dept\",value=c(F,F,T,F,T,T))\nv = as_data_frame(g, \"vertices\")\nplot(g, \n     mark.groups=list(\n       unlist(v %>% filter(dept==\"sale\") %>% select(name)),\n       unlist(v %>% filter(dept==\"IT\") %>% select(name))\n       ), \n     mark.col=c(\"#C5E5E7\",\"#ECD89A\"), mark.border=NA)\n\n\n\n\n\nggraph is a ggplot version of graph plotting. Using graph object as input, it can convert vertice attributes to plot attribute automatically or manually.\n\n\nCode\nv = v %>% \n  mutate(age_range=case_when(age < 30 ~ 20, age < 40 & age >30 ~ 30, age > 40 ~ 40))\ng = g %>% set_vertex_attr(\"age_range\", value=v$age_range)\nggraph(g, layout = \"kk\") +\n  geom_node_point(aes(size=age_range, color=gender), alpha=0.5) +\n  geom_node_text(aes(label=name)) + \n  geom_edge_link() +\n  scale_size_continuous(breaks=c(20,30,40), range = c(2, 6)) +\n  theme_void() \n\n\n\n\n\nAlmost all the {ggplots} theme, scale functions are available for {ggraph}. Refer to rdocumentation for more details."
  },
  {
    "objectID": "posts/2019-09-04_network_analysis_part3.html#edge-aesthetics",
    "href": "posts/2019-09-04_network_analysis_part3.html#edge-aesthetics",
    "title": "Network visualization - Part 3",
    "section": "Edge aesthetics",
    "text": "Edge aesthetics\nSimilar to vertex aesthetics, edge plotting aesthetics can be manipulated both {igraph} default plotting and {ggraph} plotting\n\n\nCode\n# use linetype present whether come from same department, and line width presents friendship\ne = as_data_frame(g, what=\"edges\") %>% as_tibble %>% \n  mutate(width=friendship) %>% \n  mutate(lty=ifelse(same.dept,1,2))\nplot(\n  g %>% set_edge_attr(\"width\",value=e$width) %>% set_edge_attr(\"lty\",value=e$lty),\n  edge.arrow.size=0.8,\n  edge.curved=T\n)\nlegend(\"topleft\", legend=unique(v$gender),pch=21,pt.bg=c(\"red\",\"blue\"), title=\"gender\", box.lty=0)\nlegend(\"left\",legend=unique(e$same.dept),lty=c(1,2), title = \"same.dept\",box.lty=0)\nlegend(\"topright\", legend=sort(unique(e$friendship)), lwd=sort(unique(e$friendship)), title=\"friendship\", box.lty=0)\n\n\n\n\n\nUsing {ggraph} to show edges attribute is much easier.\n\n\nCode\nggraph(g, layout=\"kk\") +\n  geom_edge_link(aes(edge_width=friendship, edge_linetype=same.dept), arrow = arrow(angle=5, length = unit(0.3, \"inches\"))) +\n  geom_node_point(aes(color=gender), size=6) +\n  geom_node_text(aes(label=name), nudge_y = -0.1, nudge_x = -0.1) +\n  scale_edge_width(range = c(1, 2)) +\n  theme_void()"
  },
  {
    "objectID": "posts/2019-09-04_network_analysis_part3.html#facet",
    "href": "posts/2019-09-04_network_analysis_part3.html#facet",
    "title": "Network visualization - Part 3",
    "section": "Facet",
    "text": "Facet\nOne big advantage of {ggraph} is to use facet. It can be facet_edges or facet_nodes or facet_graph. Here I will only show example of facet_nodes.\n\n\nCode\ng = g %>% set_vertex_attr(\"dept\",value=c(\"sale\",\"IT\",\"sale\",\"IT\",\"sale\")) %>% \n  set_edge_attr(\"same.dept\",value=c(F,F,T,F,T,T))\n\n#  facet based on the dept\nggraph(g, layout=\"kk\") +\n  facet_nodes(~dept, drop = F) +\n  geom_edge_link(aes(edge_width=friendship, linetype=same.dept), arrow = arrow(angle=5, length = unit(0.3, \"inches\"))) +\n  geom_node_point(aes(color=gender), size=6) +\n  geom_node_text(aes(label=name), nudge_y = -0.1, nudge_x = -0.1) +\n  scale_edge_width(range = c(1, 2))"
  },
  {
    "objectID": "posts/2020-06-25_data_table.html",
    "href": "posts/2020-06-25_data_table.html",
    "title": "Transition from dplyr to data.table",
    "section": "",
    "text": "Code\n# dplyr version\nsystem.time({\n        old_result = gene_lookup_old(gene_frag, open_frag, frag_int, my_gene_id, open_oe_only, selected_cells)\n})\n# user  system elapsed \n# 2.727   0.122   2.898  \n\n\n\n\nCode\n# data.table version\nsystem.time({\n        new_result = gene_lookup(gene_frag, open_frag, frag_int, my_gene_id, open_oe_only, selected_cells)\n})\n# user  system elapsed \n# 0.505   0.054   0.293 \n\n\nLast year, Hadley Wickham and his team launched a hybrid package dtplyr which uses the syntax of dplyr but runs data.table on the backend. It is a brilliant idea and I believe a lot of dplyr fan would love it, but dtplyr is still slower than data.table. Considering speed is the highest priority in some applications, I would like to switch to data.table completely. It took me only half day to get used to data.table syntax anyway. Plus, this post, which shows using pipe in data.table, makes this transition more smooth and pleasant.\nIn this blog, I will list data.table and dplyr/tidyr syntax back to back, providing an easy reference for dplyr users who want to quickly pick up data.table.\n\nread file\nread_delim and read_csv are the two functions commonly used to read-in data from dplyr\n\n\nCode\nuntidy_mtcars_df = read_delim(\"https://raw.githubusercontent.com/sckinta/example_code/master/data_examples/untidy_mtcars1.csv\", delim=\",\")\nuntidy_mtcars_df = read_csv(\"https://raw.githubusercontent.com/sckinta/example_code/master/data_examples/untidy_mtcars1.csv\")\nclass(untidy_mtcars_df)\n\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nThe equivalent command in data.table is fread. Like “read_delim”, fread automatically searches for header and defines column data type, but it is faster and automatically searches for separators as well.\n\n\nCode\nuntidy_mtcars_dt = fread(\"https://raw.githubusercontent.com/sckinta/example_code/master/data_examples/untidy_mtcars1.csv\")\nclass(untidy_mtcars_dt)\n\n\n[1] \"data.table\" \"data.frame\"\n\n\nTo switch between tibble and data.table class, use tbl_df to convert data.table to tibble, and as.data.table() to convert tibble/data.frame to data.table.\n\n\nsubset rows\nSubsetting rows is done by the function filter from dplyr.\n\n\nCode\n# subset Merc cars \nuntidy_mtcars_df %>% \n  filter(grepl(\"Merc\",model))\n\n\nSubsetting rows in data.table is very similar to the base R, placing boolean vector at row index. The index separator , can even be omitted. Also if boolean expression contains column variables, we can use the variable names directly instead of using df$var.\nPipe can be used in the format of %>% .[] to connect the code.\n\n\nCode\n# subset Merc cars \nuntidy_mtcars_dt %>% \n  .[grepl(\"Merc\",model)]\n\n\n\n\nsubset columns\nSubsetting columns is done by the function select from dplyr\n\n\nCode\n# subset columns -- model, mpg, cyl\nuntidy_mtcars_df %>% \n  select(model, mpg, cyl)\n\n# de-select columns with partial names \"ar\"\nuntidy_mtcars_df %>% \n  select(-contains(\"ar\"))\n\n\ndata.table uses variable names to subset columns. Like base R, the variable name vector is a character class placed at column index position. Index separator , cannot be omitted here.\n\n\nCode\n# subset columns -- model, mpg, cyl\nuntidy_mtcars_dt %>% \n  .[,c(\"model\",\"mpg\",\"cyl\")]\n\n# de-select columns with partial names \"ar\"\ncol_ar = colnames(untidy_mtcars_dt)[grepl(\"ar\",colnames(untidy_mtcars_dt))]\nuntidy_mtcars_dt %>% \n  .[,-..col_ar]\n\n\n\n\nadd new variable\ndplyr uses mutate to add column.\n\n\nCode\n# assign car size based on the weight\nuntidy_mtcars_df %>% \n  mutate(size=case_when(\n    wt < 3 ~ \"small\",\n    wt > 3 & wt < 5 ~ \"median\",\n    wt > 5 ~ \"large\"\n  ))\n\n\ndata.table uses := to assign values to column variables. Be aware that, different from mutate which returns the updated data frame without assignment, data.table is modified by reference and returned invisibly when := or any set* functions are used. If you do not want to change the original data.table, take a copy first DT2 = copy(DT).\n\n\nCode\nuntidy_mtcars_dt2=copy(untidy_mtcars_dt)\nuntidy_mtcars_dt2 %>% \n  .[,size:=case_when(wt < 3 ~ \"small\",\n    wt > 3 & wt < 5 ~ \"median\",\n    wt > 5 ~ \"large\")]\n\n\n:= can also be used for multiple column assignment and functional form. More details refer to the usage manual\n\n\npivot\nTable pivoting is done by the functions spread and gather (or pivot_wider and pivot_longer) from tidyr.\n\n\nCode\n# wide to long (gather) -- combine automatic, manual two columns into transmission\ntidy_mtcars_df = untidy_mtcars_df %>% \n  gather(key=\"transmission\",value=\"tmp\",automatic,manual) %>% \n  filter(tmp==1) %>% \n  select(-tmp)\n\n# wide to long (pivot_longer) -- combine automatic, manual two columns into transmission\nuntidy_mtcars_df %>% \n  pivot_longer(cols=c(\"automatic\",\"manual\"),names_to=\"transmission\",values_to=\"tmp\") %>% \n  filter(tmp==1) %>% \n  select(-tmp)\n\n\n# A tibble: 32 × 12\n   model       mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb trans…¹\n   <chr>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <chr>  \n 1 Mazda RX4  21       6  160    110  3.9   2.62  16.5     0     4     4 manual \n 2 Mazda RX…  21       6  160    110  3.9   2.88  17.0     0     4     4 manual \n 3 Datsun 7…  22.8     4  108     93  3.85  2.32  18.6     1     4     1 manual \n 4 Hornet 4…  21.4     6  258    110  3.08  3.22  19.4     1     3     1 automa…\n 5 Hornet S…  18.7     8  360    175  3.15  3.44  17.0     0     3     2 automa…\n 6 Valiant    18.1     6  225    105  2.76  3.46  20.2     1     3     1 automa…\n 7 Duster 3…  14.3     8  360    245  3.21  3.57  15.8     0     3     4 automa…\n 8 Merc 240D  24.4     4  147.    62  3.69  3.19  20       1     4     2 automa…\n 9 Merc 230   22.8     4  141.    95  3.92  3.15  22.9     1     4     2 automa…\n10 Merc 280   19.2     6  168.   123  3.92  3.44  18.3     1     4     4 automa…\n# … with 22 more rows, and abbreviated variable name ¹​transmission\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n# long to wide (spread) -- make transmission to group automatic and manual\ntidy_mtcars_df %>% \n  mutate(tmp=1) %>% \n  spread(transmission, tmp) %>% \n  mutate_if(function(x){any(is.na(x))}, function(x){ifelse(is.na(x),0,1)})\n\n\n# A tibble: 32 × 13\n   model       mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb autom…¹\n   <chr>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n 1 Hornet 4…  21.4     6  258    110  3.08  3.22  19.4     1     3     1       1\n 2 Hornet S…  18.7     8  360    175  3.15  3.44  17.0     0     3     2       1\n 3 Valiant    18.1     6  225    105  2.76  3.46  20.2     1     3     1       1\n 4 Duster 3…  14.3     8  360    245  3.21  3.57  15.8     0     3     4       1\n 5 Merc 240D  24.4     4  147.    62  3.69  3.19  20       1     4     2       1\n 6 Merc 230   22.8     4  141.    95  3.92  3.15  22.9     1     4     2       1\n 7 Merc 280   19.2     6  168.   123  3.92  3.44  18.3     1     4     4       1\n 8 Merc 280C  17.8     6  168.   123  3.92  3.44  18.9     1     4     4       1\n 9 Merc 450…  16.4     8  276.   180  3.07  4.07  17.4     0     3     3       1\n10 Merc 450…  17.3     8  276.   180  3.07  3.73  17.6     0     3     3       1\n# … with 22 more rows, 1 more variable: manual <dbl>, and abbreviated variable\n#   name ¹​automatic\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nCode\n# long to wide (pivot_wider) -- \ntidy_mtcars_df %>% \n  mutate(tmp=1) %>% \n  pivot_wider(names_from=transmission, values_from=tmp) %>% \n  mutate_if(function(x){any(is.na(x))}, function(x){ifelse(is.na(x),0,1)})\n\n\n# A tibble: 32 × 13\n   model       mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb autom…¹\n   <chr>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n 1 Hornet 4…  21.4     6  258    110  3.08  3.22  19.4     1     3     1       1\n 2 Hornet S…  18.7     8  360    175  3.15  3.44  17.0     0     3     2       1\n 3 Valiant    18.1     6  225    105  2.76  3.46  20.2     1     3     1       1\n 4 Duster 3…  14.3     8  360    245  3.21  3.57  15.8     0     3     4       1\n 5 Merc 240D  24.4     4  147.    62  3.69  3.19  20       1     4     2       1\n 6 Merc 230   22.8     4  141.    95  3.92  3.15  22.9     1     4     2       1\n 7 Merc 280   19.2     6  168.   123  3.92  3.44  18.3     1     4     4       1\n 8 Merc 280C  17.8     6  168.   123  3.92  3.44  18.9     1     4     4       1\n 9 Merc 450…  16.4     8  276.   180  3.07  4.07  17.4     0     3     3       1\n10 Merc 450…  17.3     8  276.   180  3.07  3.73  17.6     0     3     3       1\n# … with 22 more rows, 1 more variable: manual <dbl>, and abbreviated variable\n#   name ¹​automatic\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe data.table uses dcast for long-to-wide and melt for wide-to-long pivoting.\n\n\nCode\n# wide to long (melt) -- combine automatic, manual two columns into transmission\ntidy_mtcars_dt = untidy_mtcars_dt %>% \n  melt(measure.vars=c(\"automatic\",\"manual\"), variable.name=\"transmission\", value.name=\"tmp\") %>% \n  .[tmp==1] %>% \n  .[,-c(\"tmp\")]\n\n# long to wide (dcast) -- split transmission column to automatic manual\ntidy_mtcars_dt %>% \n  .[, tmp:=1]\n\nuntidy_mtcars_dt = tidy_mtcars_dt %>%   \n  dcast(model + mpg + cyl + disp + hp + drat + wt + qsec + vs + gear + carb ~transmission, value.var=\"tmp\")\n\nuntidy_mtcars_dt %>% \n  .[, c(\"manual\",\"automatic\") := replace(.SD,is.na(.SD),0), .SDcols=c(\"manual\",\"automatic\")]\n\n\n\n\nmerge\ndplyr uses SQL-like join functions to merge two or more tables together. There are inner_join, full_join, left_join, right_join, semi_join and anti_join.\n\n\nCode\n# generate two tables\ntidy_mtcars_df_engine = tidy_mtcars_df %>% \n  select(model, mpg, cyl, hp, vs)\ntidy_mtcars_df_other = tidy_mtcars_df %>% \n  select(model, disp, qsec, transmission, gear, carb)\n\n# left_join\nleft_join(\n  tidy_mtcars_df_engine,\n  tidy_mtcars_df_other\n)\n\n\ndata.table, from the other end, use merge for all the joins. By default, merge is inner_join, but we can adjust by variables to fulfill other join functions.\n\n\nCode\ntidy_mtcars_dt_engine = tidy_mtcars_df_engine %>% as.data.table()\ntidy_mtcars_df_other = tidy_mtcars_df_other %>% as.data.table()\n\n# inner_join\nmerge(\n  tidy_mtcars_dt_engine,\n  tidy_mtcars_df_other,\n  all=FALSE\n)\n\n# left_join\nmerge(\n  tidy_mtcars_dt_engine,\n  tidy_mtcars_df_other,\n  all.x=T\n)\n\n# right_join\nmerge(\n  tidy_mtcars_dt_engine,\n  tidy_mtcars_df_other,\n  all.y=T\n)\n\n# full_join\nmerge(\n  tidy_mtcars_dt_engine,\n  tidy_mtcars_df_other,\n  all=T\n)\n\n\n\n\ngroup and summarize\ndplyr uses group_by and summarize to calculate the new variable based on the group.\n\n\nCode\n# calculate weight mean for each group of differrent cylinder number\nuntidy_mtcars_df %>% \n  group_by(cyl) %>% \n  summarise(wt_mean=mean(wt)) %>% \n  ungroup()\n\n\n# A tibble: 3 × 2\n    cyl wt_mean\n  <dbl>   <dbl>\n1     4    2.29\n2     6    3.12\n3     8    4.00\n\n\ndata.table uses by to specify the group, and = to summarize.\n\n\nCode\nuntidy_mtcars_dt %>% \n  .[,.(wt_mean=mean(wt)),by=cyl]\n\n\n   cyl  wt_mean\n1:   8 3.999214\n2:   4 2.285727\n3:   6 3.117143\n\n\n\n\ngroup and subsetting\ndplyr uses group_by and slice to subset rows within the group.\n\n\nCode\n# choose the heaviest cart for each cylinder group\nuntidy_mtcars_df %>% \n  group_by(cyl) %>% \n  slice(which.max(wt)) %>% \n  ungroup()\n\n\n# A tibble: 3 × 13\n  model        mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb autom…¹\n  <chr>      <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 Merc 240D   24.4     4  147.    62  3.69  3.19  20       1     4     2       1\n2 Valiant     18.1     6  225    105  2.76  3.46  20.2     1     3     1       1\n3 Lincoln C…  10.4     8  460    215  3     5.42  17.8     0     3     4       1\n# … with 1 more variable: manual <dbl>, and abbreviated variable name\n#   ¹​automatic\n# ℹ Use `colnames()` to see all variable names\n\n\ndata.table uses .SD and by to subset within the group.\n\n\nCode\n# choose the heaviest cart for each cylinder group\nuntidy_mtcars_dt %>% \n  .[,.SD[which.max(wt)], by=cyl]\n\n\n   cyl               model  mpg  disp  hp drat    wt  qsec vs gear carb\n1:   8 Lincoln Continental 10.4 460.0 215 3.00 5.424 17.82  0    3    4\n2:   4           Merc 240D 24.4 146.7  62 3.69 3.190 20.00  1    4    2\n3:   6             Valiant 18.1 225.0 105 2.76 3.460 20.22  1    3    1\n   automatic manual\n1:         1      0\n2:         1      0\n3:         1      0\n\n\nFor more .SD reading, refer to https://cran.r-project.org/web/packages/data.table/vignettes/datatable-sd-usage.html\n\n\nlast bits\nThere are other accessory functions useful in dplyr and some of them have corresponding functions in data.table too.\n\n\n\n\n \n  \n    usage \n    dplyr \n    data.table \n  \n \n\n  \n    get unique rows \n    distinct() \n    unique() \n  \n  \n    sort by column(s) \n    arrange() \n    setorder()/setorderv() \n  \n  \n    change column names \n    rename(new=old) \n    setnames(old, new)"
  },
  {
    "objectID": "posts/2021-11-02_tidyTues_ultraRace.html",
    "href": "posts/2021-11-02_tidyTues_ultraRace.html",
    "title": "TidyTuesday: predict ultra race time",
    "section": "",
    "text": "Load required libraries"
  },
  {
    "objectID": "posts/2021-11-02_tidyTues_ultraRace.html#data-skim",
    "href": "posts/2021-11-02_tidyTues_ultraRace.html#data-skim",
    "title": "TidyTuesday: predict ultra race time",
    "section": "data skim",
    "text": "data skim\nData README is available at here.\n\n\nCode\nultra_rankings <- tuesdata$ultra_rankings\nrace <- tuesdata$race\n\nultra_join <-\n    ultra_rankings %>% \n    left_join(race, by=\"race_year_id\")\n\n\n\n\nCode\nskimr::skim(ultra_join)\n\n\n\nData summary\n\n\nName\nultra_join\n\n\nNumber of rows\n137803\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n9\n\n\nDate\n1\n\n\ndifftime\n1\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nrunner\n0\n1.00\n3\n52\n0\n73629\n0\n\n\ntime\n17791\n0.87\n8\n11\n0\n72840\n0\n\n\ngender\n30\n1.00\n1\n1\n0\n2\n0\n\n\nnationality\n0\n1.00\n3\n3\n0\n133\n0\n\n\nevent\n0\n1.00\n4\n57\n0\n435\n0\n\n\nrace\n0\n1.00\n3\n63\n0\n371\n0\n\n\ncity\n15599\n0.89\n2\n30\n0\n308\n0\n\n\ncountry\n77\n1.00\n4\n17\n0\n60\n0\n\n\nparticipation\n0\n1.00\n4\n5\n0\n4\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2012-01-14\n2021-09-03\n2017-10-13\n711\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nstart_time\n0\n1\n0 secs\n82800 secs\n05:00:00\n39\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrace_year_id\n0\n1.00\n26678.70\n20156.18\n2320\n8670.0\n21795.0\n40621\n72496.0\n▇▃▃▂▂\n\n\nrank\n17791\n0.87\n253.56\n390.80\n1\n31.0\n87.0\n235\n1962.0\n▇▁▁▁▁\n\n\nage\n0\n1.00\n46.25\n10.11\n0\n40.0\n46.0\n53\n133.0\n▁▇▂▁▁\n\n\ntime_in_seconds\n17791\n0.87\n122358.26\n37234.38\n3600\n96566.0\n114167.0\n148020\n296806.0\n▁▇▆▁▁\n\n\ndistance\n0\n1.00\n154.08\n39.22\n0\n160.9\n162.6\n168\n179.1\n▁▁▁▁▇\n\n\nelevation_gain\n0\n1.00\n6473.94\n3293.50\n0\n3910.0\n6640.0\n9618\n14430.0\n▅▆▆▇▁\n\n\nelevation_loss\n0\n1.00\n-6512.20\n3305.73\n-14440\n-9618.0\n-6810.0\n-3950\n0.0\n▁▇▆▅▅\n\n\naid_stations\n0\n1.00\n9.58\n7.56\n0\n0.0\n12.0\n16\n56.0\n▇▇▁▁▁\n\n\nparticipants\n0\n1.00\n510.75\n881.25\n0\n0.0\n65.0\n400\n2900.0\n▇▁▁▁▁"
  },
  {
    "objectID": "posts/2021-11-02_tidyTues_ultraRace.html#eda",
    "href": "posts/2021-11-02_tidyTues_ultraRace.html#eda",
    "title": "TidyTuesday: predict ultra race time",
    "section": "EDA",
    "text": "EDA\nWe want to estimate the time (time_in_seconds) for runner to finish based on the features.\n\nthe effect of gender and age\n\n\nCode\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    filter(!is.na(gender)) %>% \n    filter(age > 10, age < 100) %>% \n    mutate(age_decade = 5* (age %/% 5)) %>% \n    select(time_in_seconds, gender, age, age_decade) %>% \n    group_by(age_decade, gender) %>% \n    summarise(\n        time_in_seconds_sd = sd(time_in_seconds),\n         time_in_seconds = mean(time_in_seconds)\n    ) %>% \n    ggplot(aes(x = age_decade, color=gender, group=gender)) +\n    geom_point(aes(y=time_in_seconds)) +\n    geom_line(aes(y=time_in_seconds)) +\n    geom_errorbar(aes(ymin=time_in_seconds - time_in_seconds_sd, ymax=time_in_seconds + time_in_seconds_sd), width=0.2, alpha=0.7) +\n    scale_color_viridis_d() +\n    labs(x = \"age\", y = \"time (second)\") +\n    scale_y_continuous(labels = scales::label_comma())\n\n\n\n\n\n\n\nthe effect of nationality, age, gender\n\n\nCode\nultra_join %>% \n    mutate(nationality = fct_lump(nationality, prop=0.05)) %>% \n    count(nationality, sort=TRUE) \n\n\n# A tibble: 4 × 2\n  nationality     n\n  <fct>       <int>\n1 Other       50563\n2 USA         47259\n3 FRA         28905\n4 GBR         11076\n\n\n\n\nCode\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    filter(!is.na(gender)) %>% \n    filter(age > 10, age < 100) %>% \n    mutate(nationality = fct_lump(nationality, prop=0.05)) %>% \n    ggplot(aes(x = age, fill=nationality), group=nationality) +\n    facet_wrap(vars(gender)) +\n    geom_bar(stat=\"density\", alpha=0.5)\n\n\n\n\n\nnationality\n\n\nCode\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    filter(!is.na(gender)) %>% \n    filter(age > 10, age < 100) %>% \n    mutate(nationality = fct_lump(nationality, prop=0.05)) %>% \n    ggplot(aes(x=fct_reorder(nationality, time_in_seconds), y=time_in_seconds, fill=nationality)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +\n    labs(x=\"runner's nationality\", fill=NULL, y=\"time (second)\") +\n    scale_y_continuous(labels = scales::label_comma())\n\n\n\n\n\n\n\neffect of distance\n\n\nCode\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    filter(distance >= 150) %>% \n    ggplot(aes(x=distance, y=time_in_seconds)) +\n    geom_point(alpha=0.1, size=1) +\n    geom_smooth() +\n    labs(y=\"time (second)\") +\n    scale_y_continuous(labels = scales::label_comma())\n\n\n\n\n\n\n\neffect of elevation\n\n\nCode\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    filter(distance >= 150) %>% \n    mutate(elevation = ifelse(\n        elevation_gain > abs(elevation_loss), elevation_gain,  abs(elevation_loss)\n        )) %>% \n    ggplot(aes(x=elevation , y=time_in_seconds)) +\n    geom_point(alpha=0.1, size=1) +\n    geom_smooth() +\n    labs(y=\"time (second)\") +\n    scale_y_continuous(labels = scales::label_comma())\n\n\n\n\n\n\n\neffect of date\nThe year of the race\n\n\nCode\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    mutate(\n        race_year=lubridate::year(date), \n        race_month=lubridate::month(date)\n    ) %>% \n    group_by(race_year) %>% \n    summarise(\n        time_in_seconds_sd=mean(time_in_seconds),\n        time_in_seconds=mean(time_in_seconds)\n    ) %>% \n    ungroup() %>% \n    ggplot(aes(x=race_year, y=time_in_seconds)) +\n    geom_point() +\n    geom_line() +\n    geom_errorbar(aes(ymin=time_in_seconds - time_in_seconds_sd, ymax=time_in_seconds + time_in_seconds_sd), alpha=0.5)\n\n\n\n\n\nThe month of race can be the proxy to estimate the season when race was hosted. However, here I did not take the geographic information (hemisphere) into consideration.\n\n\nCode\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    mutate(\n        race_year=lubridate::year(date), \n        race_month=lubridate::month(date)\n    ) %>% \n    group_by(race_month) %>% \n    summarise(\n        time_in_seconds_sd=mean(time_in_seconds),\n        time_in_seconds=mean(time_in_seconds)\n    ) %>% \n    ungroup() %>% \n    ggplot(aes(x=race_month, y=time_in_seconds)) +\n    geom_point() +\n    geom_line() +\n    geom_errorbar(aes(ymin=time_in_seconds - time_in_seconds_sd, ymax=time_in_seconds + time_in_seconds_sd), alpha=0.5)"
  },
  {
    "objectID": "posts/2021-11-02_tidyTues_ultraRace.html#learning-models",
    "href": "posts/2021-11-02_tidyTues_ultraRace.html#learning-models",
    "title": "TidyTuesday: predict ultra race time",
    "section": "learning models",
    "text": "learning models\nHere I will perform two distinct models – linear regression and random forest to predict the race time using runner’s gender, age, nationality, elevation and distance of race.\n\ndata budget\ninistal split to train and test\n\n\nCode\nultra_df <- ultra_join %>% \n  filter(!is.na(time_in_seconds)) %>% \n  filter(!is.na(gender)) %>% \n  filter(age > 10, age < 100) %>% \n  filter(distance >= 150) %>% \n  mutate(elevation = ifelse(\n        elevation_gain > abs(elevation_loss), \n        elevation_gain,\n        abs(elevation_loss)\n        )\n  ) %>% \n  select(time_in_seconds, age, gender, nationality, distance, elevation)\n\nset.seed(2021)\nultra_split <- initial_split(ultra_df, strata = time_in_seconds)\nultra_train <- training(ultra_split)\nultra_test <- testing(ultra_split)\n\n\ncreate resamples for cross validation\n\n\nCode\nset.seed(124)\nultra_folds <- vfold_cv(ultra_train, v=10)\n\n\n\n\nrecipes for feature engineer\n\n\nCode\nultra_rec <- recipe(time_in_seconds ~., data = ultra_train) %>% \n  step_other(nationality) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_string2factor(all_nominal_predictors()) %>% \n  # step_dummy(all_nominal_predictors()) %>% \n  I()\n\n# want to test whether dummy variables affect the model behave\nind_rec <- ultra_rec %>% \n  step_dummy(all_nominal_predictors())\n\n\n\n\nfit linear model\nspecify models\n\n\nCode\nlm_spec <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n\nDoes linear model need dummy variable? Using workflow_set to test\n\n\nCode\nlm_wf <- workflow_set(\n  preproc = list(\"nodummy\"=ultra_rec, \"dummy\"=ind_rec),\n  models = list(lm_spec)\n)\n\nlm_rs <- workflow_map(\n  lm_wf, 'fit_resamples', resamples=ultra_folds\n  )\n\nlm_rs %>% collect_metrics()\n\n\n# A tibble: 4 × 9\n  wflow_id           .config preproc model .metric .esti…¹    mean     n std_err\n  <chr>              <chr>   <chr>   <chr> <chr>   <chr>     <dbl> <int>   <dbl>\n1 nodummy_linear_reg Prepro… AsIs    line… rmse    standa… 2.39e+4    10 6.94e+1\n2 nodummy_linear_reg Prepro… AsIs    line… rsq     standa… 5.66e-1    10 2.73e-3\n3 dummy_linear_reg   Prepro… AsIs    line… rmse    standa… 2.39e+4    10 6.94e+1\n4 dummy_linear_reg   Prepro… AsIs    line… rsq     standa… 5.66e-1    10 2.73e-3\n# … with abbreviated variable name ¹​.estimator\n\n\nBased on the r-square value, the linear model with age, distance, elevation, gender and nationality explained ~57% variance of time_in_seconds.\nUsing dummy variable or not does not change the metrics. In fact, the number of coefficients will be exactly same no matter whether using dummy or not. Below shows coefficients of linear regression by fitting the “nodummy_linear_reg” workflow to the training data.\n\n\nCode\nlm_coef <- lm_rs %>% \n  extract_workflow('nodummy_linear_reg') %>% \n  fit(ultra_train) %>% \n  tidy()\n\nlm_coef\n\n\n# A tibble: 9 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)       142711.     217.      658.  0        \n2 age                 4220.      83.3      50.6 0        \n3 genderW             6315.     236.       26.8 1.82e-157\n4 nationalityGBR    -25432.     389.      -65.3 0        \n5 nationalityJPN    -20211.     406.      -49.8 0        \n6 nationalityUSA    -30025.     302.      -99.6 0        \n7 nationalityother  -19682.     254.      -77.6 0        \n8 distance            2630.      99.2      26.5 2.65e-154\n9 elevation          17421.     117.      149.  0        \n\n\n\n\nCode\nlm_coef %>% \n  filter(term!=\"(Intercept)\") %>% \n  ggplot(aes(x = estimate, y = fct_reorder(term, estimate))) +\n  geom_col(aes(fill=(estimate < 0)), alpha = 0.5) +\n  geom_errorbar(aes(xmin=estimate - std.error, xmax = estimate + std.error), width=0.5) +\n  theme(legend.position = 'none') +\n  labs(fill=NULL, y = NULL)\n\n\n\n\n\nElevation, being a women (compare to being a men), age and distance positively affect race time, while racers from JPN/GBR/USA/other (compare to racers from FRA) finish the race in shorter time.\n\n\nfit random forest model using workflow\nUsing random forest as model to get Resampling results\n\n\nCode\nrf_spec <- rand_forest() %>% \n  set_engine('ranger') %>% \n  set_mode('regression')\n\nrf_wf <- workflow() %>% \n  add_model(rf_spec) %>% \n  add_recipe(ultra_rec)\n\n# resample evaluate \nrf_rs  <- rf_wf %>% \n  fit_resamples(\n    resamples = ultra_folds\n  )\n\n\n\n\nCode\nrf_rs  %>% \n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n  std_err .config             \n  <chr>   <chr>          <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   18535.       10 57.2     Preprocessor1_Model1\n2 rsq     standard       0.738    10  0.00219 Preprocessor1_Model1\n\n\nCompared to linear model shown above, random forest with same predictors can explain more variance of Y (74% vs. 56%) and show smaller rmse (1.8e4 vs. 2.4e4).\n\n\nCode\nbind_rows(\n  rf_rs  %>% \n    collect_metrics() %>% \n    select(.metric, mean, std_err) %>% \n    mutate(model = \"random forest\"),\n  lm_rs  %>% \n    collect_metrics() %>% \n    filter(wflow_id == 'nodummy_linear_reg') %>% \n    select(.metric, mean, std_err) %>% \n    mutate(model = \"linear reg\")\n) %>% \n  ggplot(aes(x = model, y = mean)) +\n  facet_wrap(vars(.metric), scales = 'free') +\n  geom_point() +\n  geom_errorbar(aes(ymin=mean - std_err, ymax=mean + std_err), width=0)\n\n\n\n\n\nNotes: above plot can also be done by autoplot if we perform the comparison between linear regression and random forest models using workflow_set.\n\n\nlast_fit test data using random forest result\n\n\nCode\nrf_final_rs <- rf_wf %>% \n  last_fit(ultra_split)\n\n\n\n\nCode\nrf_final_rs %>% \n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard   18558.    Preprocessor1_Model1\n2 rsq     standard       0.737 Preprocessor1_Model1\n\n\nDifferent from fit_resample results, these metrics are calculated on the test data. The value is very close to the values done on training data (resample data), thus the model is not over-fitted.\n\n\nCode\nfinal_wf <- rf_final_rs %>% \n  extract_workflow()\n\nfinal_wf\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_other()\n• step_normalize()\n• step_string2factor()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      83042 \nNumber of independent variables:  5 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       342750489 \nR squared (OOB):                  0.7386915 \n\n\nThe above trained workflow from last_fit can be saved in .rda for future prediction\n\n\nCode\n# using final_wf for prediction\nfinal_wf %>% \n  predict(new_data = ultra_train %>% dplyr::slice(1)) \n\n\n# A tibble: 1 × 1\n    .pred\n    <dbl>\n1 108741."
  },
  {
    "objectID": "posts/2021-11-02_tidyTues_ultraRace.html#what-techniques-i-learned",
    "href": "posts/2021-11-02_tidyTues_ultraRace.html#what-techniques-i-learned",
    "title": "TidyTuesday: predict ultra race time",
    "section": "what techniques i learned",
    "text": "what techniques i learned\n\ndeal with high-levels nominal features (fct_lump and step_other) in EDA and modeling\nworkflow_set and map_workflow to create multiple workflows for model and/or recipes comparison.\nfit_resample for cross-validation. The metrics collected from cross-validation results are used for workflow comparison.\nlast_fit model and save trained workflow for future use"
  },
  {
    "objectID": "posts/2019-07-11_network_analysis_part1.html",
    "href": "posts/2019-07-11_network_analysis_part1.html",
    "title": "Network Analysis in R - Part 1",
    "section": "",
    "text": "Network analysis, also called graph analysis, is to study the complexity of the inter-relationships between actors of all sorts and provides an architectural view of individual actor connections.\nIt has been applied to many fields, like social network and gene network, and useful for any systematic studies on individual relationship 1.\nI will create a three part series 2 of network analysis and visualization using R packages {igraph} and {ggraph}. In this post, I will focus on general introduction of the terminology and R objects used in network analysis."
  },
  {
    "objectID": "posts/2019-07-11_network_analysis_part1.html#create-a-graph",
    "href": "posts/2019-07-11_network_analysis_part1.html#create-a-graph",
    "title": "Network Analysis in R - Part 1",
    "section": "Create a graph",
    "text": "Create a graph\nThere are many ways to create graph from scratch.\n\nEasy graph:graph_from_literal, make_graph\nCreate from user data: graph_from_edgelist, graph_from_adjacency_matrix, graph_from_data_frame\nRandom graphs: sample_gnp, sample_gnm, sample_pa, sample_smallworld, etc.\n\nThe following are a few examples of ways frequently used by me to make graph .\n\nedge list matrix\nThe example below generates a directed graph from a list of vertex pair (edge)\n\n\nCode\nedge_list <-\n    data.frame(from = c(1, 2, 2, 3, 4), to = c(2, 3, 4, 2, 1)) %>% \n    as.matrix()\n\ng <- graph_from_edgelist(edge_list,directed = TRUE)\n\ng\n\n\nIGRAPH 59ad3e6 D--- 4 5 -- \n+ edges from 59ad3e6:\n[1] 1->2 2->3 2->4 3->2 4->1\n\n\nCode\nplot(g)\n\n\n\n\n\n\n\none-mode graph from a vector of edge\nIf the edge is given in a single vector, the default make_graph will use the order of vector to make one-mode graph. It means that the 1st edge points from the 1st element to the 2nd element, the 2nd edge from the 3rd element to the 4th element, etc. If the length of vector is odd number, it will end with last element connecting back to the 1st element, and throw a warning.\n\n\nCode\ng <- make_graph(letters[1:10], directed = T)\ng\n\n\nIGRAPH 12185c3 DN-- 10 5 -- \n+ attr: name (v/c)\n+ edges from 12185c3 (vertex names):\n[1] a->b c->d e->f g->h i->j\n\n\nCode\nplot(g)\n\n\n\n\n\n\n\nusing adjacent matrix\nThe example below generates a undirected graph from a binary matrix, in which nodes are matrix colname and rowname.\n\n\nCode\nset.seed(123)\n\nadj_matrix <-\n    matrix(sample(0:1, 100, replace = TRUE, prob = c(0.8, 0.1)), nc = 10)\n\ncolnames(adj_matrix) <- letters[1:10]\n\nrownames(adj_matrix) <- letters[1:10]\n\nadj_matrix\n\n\n  a b c d e f g h i j\na 0 1 1 1 0 0 0 0 0 0\nb 0 0 0 1 0 0 0 0 0 0\nc 0 0 0 0 0 0 0 0 0 0\nd 0 0 1 0 0 0 0 0 0 0\ne 1 0 0 0 0 0 0 0 0 0\nf 0 1 0 0 0 0 0 0 0 0\ng 0 0 0 0 0 0 0 0 1 0\nh 1 0 0 0 0 0 0 0 1 0\ni 0 0 0 0 0 1 0 0 0 0\nj 0 1 0 0 0 0 0 0 0 0\n\n\nCode\ng <-\n    graph_from_adjacency_matrix(adj_matrix, mode = \"undirected\", weighted = T)\n\ng\n\n\nIGRAPH 81fe3c0 UNW- 10 12 -- \n+ attr: name (v/c), weight (e/n)\n+ edges from 81fe3c0 (vertex names):\n [1] a--b a--c a--d a--e a--h b--d b--f b--j c--d f--i g--i h--i\n\n\nCode\nplot(g)\n\n\n\n\n\nThere are other modes 3 available with details.\n\n\nUsing named data.frame\nThis is my favorite. The graph generated from data.frame can add all attributes at once. The below example is from official website. The attributes for nodes (actors) are age and gender, and edge (relationship) attributes include same.dept, friendship and advice.\n\n\nCode\nactors <- data.frame(\n  name=c(\"Alice\", \"Bob\", \"Cecil\", \"David\",\"Esmeralda\"),\n  age=c(48,33,45,34,21),\n  gender=c(\"F\",\"M\",\"F\",\"M\",\"F\"))\n\nrelations <- data.frame(\n  from=c(\"Bob\", \"Cecil\", \"Cecil\", \"David\",\"David\", \"Esmeralda\"),\n  to=c(\"Alice\", \"Bob\", \"Alice\", \"Alice\", \"Bob\", \"Alice\"),\n  same.dept=c(FALSE,FALSE,TRUE,FALSE,FALSE,TRUE),\n  friendship=c(4,5,5,2,1,1), \n  advice=c(4,5,5,4,2,3)\n  )\n\nactor_relation_g <- graph_from_data_frame(relations, directed=TRUE, vertices=actors)\n\nactor_relation_g\n\n\nIGRAPH 05d16d7 DN-- 5 6 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from 05d16d7 (vertex names):\n[1] Bob      ->Alice Cecil    ->Bob   Cecil    ->Alice David    ->Alice\n[5] David    ->Bob   Esmeralda->Alice\n\n\nCode\nplot(actor_relation_g)\n\n\n\n\n\n\n\nCreate random graph\nRandom graph generation is useful when it comes to simulation.\n\nsample_gnp and sample_gnm generate random graph by assuming the same constant probability for every possible edge.\n\nThe required parameters for gnp include: 1) The number of vertices in the graph n and 2) The probability for drawing an edge between two arbitrary vertices p\n\n\n\n\nCode\n# sample 10 vertex, every possible edge is created with the same constant probability 0.3\nset.seed(12)\ngr <- sample_gnp(10, 0.3)\ngr\n\n\nIGRAPH eaef67a U--- 10 11 -- Erdos-Renyi (gnp) graph\n+ attr: name (g/c), type (g/c), loops (g/l), p (g/n)\n+ edges from eaef67a:\n [1] 2-- 5 3-- 5 5-- 6 4-- 7 3-- 8 6-- 8 1-- 9 3-- 9 8-- 9 3--10 8--10\n\n\nCode\nplot(gr)\n\n\n\n\n\n-  The required parameters for *gmp* include: 1) The number of vertices in the graph `n` and 2) The number of edges in the graph `m`.\n\n\nCode\n# sample 10 vertex, create a 15-edge graph\nset.seed(123)\ngr <- sample_gnm(10, 15)\ngr\n\n\nIGRAPH a2ec496 U--- 10 15 -- Erdos-Renyi (gnm) graph\n+ attr: name (g/c), type (g/c), loops (g/l), m (g/n)\n+ edges from a2ec496:\n [1] 1-- 2 1-- 4 2-- 4 3-- 4 5-- 6 2-- 7 3-- 7 5-- 7 2-- 8 3-- 8 6-- 8 1-- 9\n[13] 4-- 9 5--10 9--10\n\n\nCode\nplot(gr)\n\n\n\n\n\n\nRandom scale free network, which means a network whose degree of nodes distribution follows a power law. sample_pa generates scale-free graphs according to the Barabasi-Albert model. We start with a single vertex and no edges in the first time step. Then we add one vertex in each time step and the new vertex initiates some edges to old vertices. The probability that an old vertex is chosen is given by \\(p(i) ~ k_i * power + zero.appeal\\)\n\n\n\nCode\nset.seed(123)\n\ngr <- sample_pa(100, power = 2)\n\nplot(g, vertex.label= NA, edge.arrow.size=0.02,vertex.size = 0.5)\n\n\n\n\n\nCode\nplot(density(degree_distribution(gr)))\n\n\n\n\n\nThere are many other random graphs using different models. To find more, try ?igraph::sample_[TAB]."
  },
  {
    "objectID": "posts/2019-07-11_network_analysis_part1.html#extract-vertexedge-and-their-attributes",
    "href": "posts/2019-07-11_network_analysis_part1.html#extract-vertexedge-and-their-attributes",
    "title": "Network Analysis in R - Part 1",
    "section": "Extract vertex/edge and their attributes",
    "text": "Extract vertex/edge and their attributes\nTo get vertex list and their attributes from graph object, we use V(graph)$\"<attribute_name>\" to convert graph object to vector. Using IGRAPH actor_relation_g created in previous chunk as example, we will get actor node (name), age (attribute 1) and gender (attribute 2)\n\n\nCode\nV(actor_relation_g)$name\n\n\n[1] \"Alice\"     \"Bob\"       \"Cecil\"     \"David\"     \"Esmeralda\"\n\n\nCode\nV(actor_relation_g)$age\n\n\n[1] 48 33 45 34 21\n\n\nCode\nV(actor_relation_g)$gender\n\n\n[1] \"F\" \"M\" \"F\" \"M\" \"F\"\n\n\nWe can also get all the vertex attributes to a data.frame using igraph::as_data_frame()\n\n\nCode\nigraph::as_data_frame(actor_relation_g, what = \"vertices\")\n\n\n               name age gender\nAlice         Alice  48      F\nBob             Bob  33      M\nCecil         Cecil  45      F\nDavid         David  34      M\nEsmeralda Esmeralda  21      F\n\n\nSimilarly, to get edge list and their attributes from graph object, we use E(graph)$\"<attribute_name>\" to convert graph object to vector. OR using igraph::as_data_frame() to convert all edges to a data.frame\n\n\nCode\n# edge attributes\nE(actor_relation_g)$same.dept\n\n\n[1] FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\nCode\nE(actor_relation_g)$friendship\n\n\n[1] 4 5 5 2 1 1\n\n\nCode\n# edge data.frame\nigraph::as_data_frame(actor_relation_g, what = \"edges\")\n\n\n       from    to same.dept friendship advice\n1       Bob Alice     FALSE          4      4\n2     Cecil   Bob     FALSE          5      5\n3     Cecil Alice      TRUE          5      5\n4     David Alice     FALSE          2      4\n5     David   Bob     FALSE          1      2\n6 Esmeralda Alice      TRUE          1      3"
  },
  {
    "objectID": "posts/2019-07-11_network_analysis_part1.html#add-vertex-and-edges",
    "href": "posts/2019-07-11_network_analysis_part1.html#add-vertex-and-edges",
    "title": "Network Analysis in R - Part 1",
    "section": "Add vertex and edges",
    "text": "Add vertex and edges\nVertices and edges can be added to existing graph by add_<vertices|edges>() or + <vertices|edges>(). Please be aware that vertices of added new edges must be from known vertices already in the graph.\n\n\nCode\n# add vertices \nactor_relation_g %>% \n    add_vertices(2, name=c(\"Lisa\",\"Zack\")) # the first argument is number of vertex\n\n\nIGRAPH 80dfeea DN-- 7 6 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from 80dfeea (vertex names):\n[1] Bob      ->Alice Cecil    ->Bob   Cecil    ->Alice David    ->Alice\n[5] David    ->Bob   Esmeralda->Alice\n\n\nCode\nactor_relation_g + vertices(c(\"Lisa\",\"Zack\"))\n\n\nIGRAPH de6630f DN-- 7 6 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from de6630f (vertex names):\n[1] Bob      ->Alice Cecil    ->Bob   Cecil    ->Alice David    ->Alice\n[5] David    ->Bob   Esmeralda->Alice\n\n\nTo add new edge, the new edge must be between known vertices already in the graph.\n\n\nCode\n# add connected edges (even number of vertices). \nactor_relation_g %>% \n    add_edges(c(\"Alice\",\"Bob\"))\n\n\nIGRAPH d7c4121 DN-- 5 7 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from d7c4121 (vertex names):\n[1] Bob      ->Alice Cecil    ->Bob   Cecil    ->Alice David    ->Alice\n[5] David    ->Bob   Esmeralda->Alice Alice    ->Bob  \n\n\nCode\nactor_relation_g + edge(c(\"Alice\",\"Bob\"))\n\n\nIGRAPH ad4c43b DN-- 5 7 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from ad4c43b (vertex names):\n[1] Bob      ->Alice Cecil    ->Bob   Cecil    ->Alice David    ->Alice\n[5] David    ->Bob   Esmeralda->Alice Alice    ->Bob  \n\n\nThe edges can be also be added by + path(). The path is a igraph.path object that each element is connected to the next, but it is not a IGRAPH object.\n\n\nCode\n# add paths. The vertices must be from known vertices already in the graph \nactor_relation_g + path(\"Alice\",\"Bob\",\"Cecil\")\n\n\nIGRAPH 48d4748 DN-- 5 8 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from 48d4748 (vertex names):\n[1] Bob      ->Alice Cecil    ->Bob   Cecil    ->Alice David    ->Alice\n[5] David    ->Bob   Esmeralda->Alice Alice    ->Bob   Bob      ->Cecil\n\n\nIf a new vertex needs to be added to current graph, using add graph method instead.\n\n\nCode\nactor_relation_g + make_graph(c(\"Alice\",\"Bob\",\"Bob\",\"Melisa\")) # this create same path as above\n\n\nIGRAPH def4db0 DN-- 6 8 -- \n+ attr: age (v/n), gender (v/c), name (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from def4db0 (vertex names):\n[1] Esmeralda->Alice  David    ->Bob    David    ->Alice  Cecil    ->Bob   \n[5] Cecil    ->Alice  Bob      ->Melisa Bob      ->Alice  Alice    ->Bob"
  },
  {
    "objectID": "posts/2019-07-11_network_analysis_part1.html#delete-vertex-and-edges",
    "href": "posts/2019-07-11_network_analysis_part1.html#delete-vertex-and-edges",
    "title": "Network Analysis in R - Part 1",
    "section": "Delete vertex and edges",
    "text": "Delete vertex and edges\nDelete can be done by delete_<vertices|edges>() using either index or name of vertices|edges.\n\n\nCode\n### remove the vertices whose age is younger than 30\nvertex_df = actor_relation_g %>% \n    igraph::as_data_frame(what=\"vertices\") %>% \n        dplyr::as_tibble() %>% \n        dplyr::mutate(index=row_number()) %>% \n        dplyr::filter(age < 30)\n\n# remove vertices by index number \nactor_relation_g %>% delete_vertices(vertex_df$index)\n\n\nIGRAPH 82b2916 DN-- 4 5 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from 82b2916 (vertex names):\n[1] Bob  ->Alice Cecil->Bob   Cecil->Alice David->Alice David->Bob  \n\n\nCode\n# remove vertice by name \nactor_relation_g %>% delete_vertices(vertex_df$name)\n\n\nIGRAPH 464f2cc DN-- 4 5 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from 464f2cc (vertex names):\n[1] Bob  ->Alice Cecil->Bob   Cecil->Alice David->Alice David->Bob  \n\n\n\n\nCode\n### remove the edges with friendship <= 1 \nedge_df = actor_relation_g %>% \n    igraph::as_data_frame(what=\"edges\") %>% \n    dplyr::as_tibble() %>% \n    mutate(index=row_number()) %>% \n    mutate(name=paste(from,to,sep=\"|\")) %>% \n    filter(friendship <= 1)\n\n# remove vertice by index\nactor_relation_g %>% delete_edges(edge_df$index)\n\n\nIGRAPH 8643b08 DN-- 5 4 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from 8643b08 (vertex names):\n[1] Bob  ->Alice Cecil->Bob   Cecil->Alice David->Alice\n\n\nCode\n# remove vertice by name \nactor_relation_g %>% delete_edges(edge_df$name)\n\n\nIGRAPH 05c4850 DN-- 5 4 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from 05c4850 (vertex names):\n[1] Bob  ->Alice Cecil->Bob   Cecil->Alice David->Alice"
  },
  {
    "objectID": "posts/2019-07-11_network_analysis_part1.html#setdelete-the-attributes-of-vertex-and-edges",
    "href": "posts/2019-07-11_network_analysis_part1.html#setdelete-the-attributes-of-vertex-and-edges",
    "title": "Network Analysis in R - Part 1",
    "section": "Set/Delete the attributes of vertex and edges",
    "text": "Set/Delete the attributes of vertex and edges\nThe attributes of vertices and edges can be added or deleted to existing graph by set_vertex_attr()/set_edge_attr() or delete_vertex_attr()/delete_edge_attr().\n\n\nCode\n# add a new attr \"relationship\" for people in the same dept\nedge_df <-\n    actor_relation_g %>% igraph::as_data_frame(what = \"edges\") %>%\n    mutate(relationship = ifelse(same.dept, \"collegue\", NA))\n\nactor_relation_g %>%\n    set_edge_attr(\"relationship\", which(!is.na(edge_df$relationship)), edge_df$relationship[!is.na(edge_df$relationship)]\n                  )\n\n\nIGRAPH 05d16d7 DN-- 5 6 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n), relationship (e/c)\n+ edges from 05d16d7 (vertex names):\n[1] Bob      ->Alice Cecil    ->Bob   Cecil    ->Alice David    ->Alice\n[5] David    ->Bob   Esmeralda->Alice"
  },
  {
    "objectID": "posts/2019-07-11_network_analysis_part1.html#merge-graph",
    "href": "posts/2019-07-11_network_analysis_part1.html#merge-graph",
    "title": "Network Analysis in R - Part 1",
    "section": "Merge graph",
    "text": "Merge graph\nGraphs union/intersection is to merge two or more graphs into one graph using the shared the vertices.\n\n\nCode\n# create new graph\ng2 <- graph_from_literal(\"David\"-+\"Charlie\"+-+\"Lisa\",\n                        \"Lisa\"+-+\"David\"+-\"Jim\",\n                        \"Zack\"+-\"Esmeralda\"-+\"Bob\",\n                        \"Zack\"+-\"Charlie\",\n                        \"Lisa\"+-\"Lisa\",\n                        \"Bob\"-+\"Alice\"+-\"Esmeralda\"\n                        )\n\n#### union graph\ng3 <- igraph::union(actor_relation_g,g2)\n\n#### graph intersection\ng4 <- igraph::intersection(actor_relation_g,g2)\n\n### plot new graphs\npar(mfrow=c(2,2)) \nplot(actor_relation_g, edge.arrow.size=.4)\nplot(g2, edge.arrow.size=.4)\nplot(g3, edge.arrow.size=.4)\nplot(g4, edge.arrow.size=.4)"
  },
  {
    "objectID": "posts/2019-07-11_network_analysis_part1.html#induce-subgraph",
    "href": "posts/2019-07-11_network_analysis_part1.html#induce-subgraph",
    "title": "Network Analysis in R - Part 1",
    "section": "Induce subgraph",
    "text": "Induce subgraph\nThe subgraph can be induced by either vertex or edge names/index. The edge names are in the form from|to.\n\n\nCode\n# induce a subgraph using a list of vertices\nigraph::induced_subgraph(actor_relation_g, v=c(\"Alice\",\"Bob\",\"Cecil\"))\n\n\nIGRAPH 4f5b3fb DN-- 3 3 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from 4f5b3fb (vertex names):\n[1] Bob  ->Alice Cecil->Bob   Cecil->Alice\n\n\nCode\n# induce a subgraph using edges\nigraph::subgraph.edges(actor_relation_g, c(\"Bob|Alice\",\"David|Bob\",\"Cecil|Alice\"), delete.vertices = TRUE)\n\n\nIGRAPH a710864 DN-- 4 3 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from a710864 (vertex names):\n[1] Bob  ->Alice Cecil->Alice David->Bob  \n\n\nCode\n# induce a subgraph using edges attribute (friendship score stronger than 3)\ne1 = E(actor_relation_g)[E(actor_relation_g)$friendship > 3]\nigraph::subgraph.edges(actor_relation_g, e1, delete.vertices = TRUE)\n\n\nIGRAPH 7e403b5 DN-- 3 3 -- \n+ attr: name (v/c), age (v/n), gender (v/c), same.dept (e/l),\n| friendship (e/n), advice (e/n)\n+ edges from 7e403b5 (vertex names):\n[1] Bob  ->Alice Cecil->Bob   Cecil->Alice"
  },
  {
    "objectID": "posts/2020-02-11_all_about_git.html",
    "href": "posts/2020-02-11_all_about_git.html",
    "title": "All about git",
    "section": "",
    "text": "1. initiate a new repo at github\nRepository, aka repo, is a collection of codes, data and documentation designated for project(s). As far as you have github account, you can create public repo(s) through github webpage easily. Follow the step 1-5 on this website, then you will create a repo with a few clicks. New repo usually comes with a README.md file. Using markdown format, you can describe the project in this README.md file which will be loaded to your repo main page when you open it. Here is the repo I created for Rladies-Philly PAWS projects.\n\n\n2. Local vs. remote\nOne concept need to be clarified here is local vs. remote. Github is the most popular cloud-based service hosting repos. Those repo is managed by git installed at remote (aka, github here). So what is git? Git is a version control system that lets you manage and keep track of your source code history. It can also be installed at local and work as local version control system. In that case, your snapshot of each version will be saved at local instead of cloud.\n\n\nCode\n# to initiate repo at local\n# suppose you have a project working now called repoX\n# all scripts/data/documentation are saved in a folder in your computer called repoX. \n# Now you want to start git version control for this project\ncd repoX/\ngit init\ngit add -A\ngit commit -m \"initiate version control\"\n\n\nThe above code can be run on any shell-like terminal. And congrats, you have sucessefully create version control at local folder repoX/. git commit basically create a snapshot of this folder. If you want to change it back to this moment in the future, you can do it with commit number (it is hash code). It is important to write a meaningful message (like here “initiate version control”) to remind yourself what the snapshot is like. We will discuss how to recover using commit number in a little bit.\n\n\n3. clone, pull and push\nTo communicate between your local and remote github, you can access through git by downloading (pull) remote to local and uploading (push) local repo to the cloud (github).\nIf you initiate repo from github first (step 1 above), you can first clone that repo to local. This repo will remember the remote address and allow you later pull from and push to the remote\n\n\nCode\n# here I use rladiesPHL/PAWS_return_noshow.git as my example\n\n# clone the repo to local\ngit clone https://github.com/rladiesPHL/PAWS_return_noshow.git\n\n\nAnyone can clone a public repo to their local computer. However, to be able to pull and push, you need to be included as collaborators for that repo specially, or you are the repo owner yourself. To add someone as collaborator, follow the steps by clicking on the webpage. Once you are the owner/collaborator, you can do following command to download and upload.\n\n\nCode\n# initiate clone will create a folder at local called \"PAWS_return_noshow\", go to that folder\ncd PAWS_return_noshow/\n# pull (the update) from PAWS_return_noshow (since the clone remember the remote address)\ngit pull\n\n# you can do your analysis, do your update at local now\n\n# when it is time to upload your analysis to cloud, you first want to take a snapshot of what you have done so far\ngit add -A\ngit commit -m \"my update\"\n\n# now you can push your analysis to the github\ngit push origin master\n\n\n\n\n4. fork\nAbove example is to push your analysis directly to the origin’s master branch. So what is origin? (what is the master branch will be explained in the next). Put it simple, you can consider origin as the place where is first downloaded. For example, I download PAWS_return_noshow repo from rladiesPHL account and the origin here will be rladiesPHL repo address (https://github.com/rladiesPHL/PAWS_return_noshow.git).\n\n\nCode\n# to quick check your repo remote origin\ngit remote -v\n\n\n\n\nCode\n# origin    https://github.com/rladiesPHL/PAWS_return_noshow.git (fetch)\n# origin    https://github.com/rladiesPHL/PAWS_return_noshow.git (push)\n\n\nWhy is origin important? The origin determines which repo push and pull will go to/from. Some repo won’t allow you to push and pull because you are not the owner or collaborator. If you do not want request pull and push permission from the owner, you can fork the repo to your own github account. Here fork is like to clone a remote repo belonging to other poeple’s account at that snapshot to your own account. You can develope/make commits on repo without any push/pull permission obstables.\nThe easiest way to fork a repo is from webpage. You can follow the instruction on this help page.\nBe aware, if you git clone the forked repo from your github account, the “origin” is your own account repo. This repo is functionally independent from the upstream repo, although at top of your own repo page it will show “This branch is X commits ahead of/behind XXX:master.” when the upstream repo makes commits after forking. What if you want your own “forked” repo remember where it comes so that you can merge the future changes from the upstream repo to your “forked” repo?\n\n\nCode\n# here I show an example of a forked repo at my own account (sckinta/datahack2020) linking back to the upstream account (CodeForPhilly/datahack2020)\n\n# add a repo description called \"upstream\" and associated this name with upstream repo. Here \"upstream\" can be any name (eg. up, ori, ...)\ngit remote add upstream https://github.com/CodeForPhilly/datahack2020.git\n\n# check remote info again you will find now repo have two remote associated with it. one is called \"origin\" and another is called \"upstream\"\ngit remote -v\n\n\n\n\nCode\n# origin    https://github.com/sckinta/datahack2020.git (fetch)\n# origin    https://github.com/sckinta/datahack2020.git (push)\n# upstream  https://github.com/CodeForPhilly/datahack2020.git (fetch)\n# upstream  https://github.com/CodeForPhilly/datahack2020.git (push)\n\n\nTo update your forked repo at github, you need three steps: 1) fetch the upstream repo to your local repo; 2) merge updated fetch content into the main branch at local; 3) push updated local to remote forked repo\n\n\nCode\n# fecth upstream to local\ngit fetch upstream\n\n# Merge the updated fetch content into the main branch at local\ngit merge upstream/master\n\n# Update, push to remote(fork) master branch\ngit push origin master\n\n\nI highly suggest pull your forked repo to local first before fetch upstream. It will guarantee when you merge the upstream it will not cause the conflicts.\n\n\n5. branch\nAfter fork origin, another useful tool for collobarative project is using branch. “Branch”, as it is named, means a branch of analysis derived from the mainstream (which is by default named “master”). You can create branches on your own repo or the repo you have been invited as collaborator. This is the biggest difference between branch and a repo fork. To add a branch at local, using code git checkout.\n\n\nCode\n# Create and switch to a new branch (say, branch \"chun\")\ngit checkout -b chun\n\n# go back to the master\ngit checkout master\n\n\nNow you can do your analysis in the repo fold. When you are ready to commit your new analysis, how will your repo know this analysis added to branch “chun”? Simple, using git checkout switch to chun branch and commit there. You can also push your new branch to remote, where the branch will show up under the //tree\n\n\nCode\n# for example I push my new analysis to branch \"chun\" and finally push it to sckinta/datahack2020\n\n# switch to branch chun\ngit checkout chun\n\n# make your new commit\ngit add -A\ngit commit -m \"new analysis\"\n\n# push it to github branch\ngit push origin chun\n\n\nIf you want to continue on other collaborator’s branch (say “abc”), you can pull that branch to local.\n\n\nCode\n# download branch abc to your analysis\ngit pull origin abc\n\n# check how many branches current local repo contains\ngit branch\n\n\n\n\nCode\n# *chun\n#   master\n#   abc\n\n\nAfter everybody did their analysis on their own branch, your group finally determine we are going to merge branch “abc” to master and delete the branch “abc”.\n\n\nCode\n# go to the master first\ngit checkout master\n\n# merge branch \"abc\" in\ngit merge abc\n\n# delete old branch\ngit checkout -d abc\n\n\nOccasionally, this process doesn’t go smoothly. Conflicts may occur when you try to merge multiple branches in. Then you may need advance tools like mergetool and opendiff. Here I won’t explain them. Please refer togit tutorial page for further reading. All the simple branch and merge has also been best explained on git tutorial.\n\n\n6. Recover a certain commit\nOne major reason we want to use version control is that we can revert to a old snapshot/commit if we want. To check the commits done to the current repo, you can try git log. The log is reported in reverse chronical order.\n\n\nCode\ngit log --oneline\n\n\n\n\nCode\n# ea03bb2 (HEAD -> chun, origin/chun) clean and EDA on incident county\n# 3c659ec (upstream/master, origin/master, origin/HEAD, master) Merge pull request #7 from CodeForPhilly/branch_dubois\n# c76c701 (upstream/branch_dubois) updated outrigger & added presentation\n# 29992c6 Merge pull request #6 from rjake/jake\n# 52c88ea Create psp_overdose_events.csv\n# 36495db Add codebook\n# 714f848 gitignore data files\n# 51e5974 added presentation slides\n\n\nTo revert to a commit\n\n\nCode\n# since we are currently at chun branch, we better go back to master where \"3c659ec\" is at\ngit checkout master\n\n# revert to a commit\ngit revert 3c659ec\n\n# the above command can also be\ngit reset --hard 3c659ec\n\n\nRemember all of above is only updated at local. If you want to make it show up at github, do a add, commit and push series.\n\n\n7. link to your remote account at local\nAfter introduce all above basic commands for git, the last thing I want to share is to set up the local git remote account. I probably shoud put it at #2.remote vs. local# part.\nTo globally set github account at local can save your effort to put account name and password everytime you want to push/pull to your own account.\n\n\nCode\n# for example I set global account as rladiesPHL. This will save the global configuration to a ~/.gitconfig file. It will prompt password for you to input\ngit config --global user.email \"philly@rladies.org\"\ngit config --global user.name \"rladiesPHL\"\n\n\nHowver, sometimes I want to switch back to my personal account temperally to do a quick push. I wish git will prompt account and password for me to input\n\n\nCode\n# reset global account a little bit\ngit config --local credential.helper \"\"\n\n# when you push, it will prompt account and password for me to input\ngit push origin master\n\n\nAll above are the frequently used git commands I used. Hope it will help anyone who is willing to use git version in their future project."
  },
  {
    "objectID": "posts/2020-8-22_options.html",
    "href": "posts/2020-8-22_options.html",
    "title": "Taking options from command line",
    "section": "",
    "text": "Usually the options following the scripts have two types\n\ndirect inputs (with default definition within the script).\nthe “true”” options with “-” or “–” to allow optional manipulation\n\nFor the second type of options, it becomes a little bit complicated. First, this type options can be further grouped based whether there is argument value followed specified option (“options with argument” vs “options without argument”). In addition, it can also be classified by whether this option is mandatory or optional (although all mandatory options can be converted to optional by specifying the default value).\nIn this post, I will catch up on the options taken-in scripting in Bash, R and Perl.\n\nBash\nBash script takes in first type of options using special variables based on the input orders $1, $2, … For the unknown number of inputs, $@ array is used to represents all arguments after script file ($0).\nFor the second type of options, there are two methods to take in options. One method is to use while :; do; done to read through all arguments ($@) after scripts by considering --option as an argument itself and shift it off in the loop. For each --option, we can use case; esac matching to specify what exact value should be.\nIn the following script, I listed the examples of “mandatory non-empty option argument”, “optional empty option argument” and “optional non-empty option argument”.\n\n\nCode\n#!/bin/bash\n\n## specifiy usage function\nusage()\n{\n        echo \"Usage: bash $0 [-h] -p p1 [-v] [-o output_file] bam1 bam2\" 1>&2\n}\n\n## setting defaults\nverbose=0 # default for optional empty option argument\n# p1=0 # all mandatory options can be converted to optional by specifying the default value\n\nwhile :; do\n    case $1 in\n        -p | --para ) # mandatory non-empty option argument (mandatory enforced later, or we can set default to make it optional)\n                if [[ \"$2\" && ! $2 =~ \"-\" ]]; then\n                        p1=$2\n                        shift\n                else\n                        echo 'ERROR: --para requires non-empty option argument'\n                        exit\n                fi\n        ;;\n        -v | --verbose ) # optional empty option argument (with default)\n                verbose=$((verbose + 1))\n        ;;\n        -o | --output ) # optional non-empty option argument\n                if [[ -f $2 ]]; then # prevent overwrite into a file exist in directory\n                        printf 'WARNING: --output argument %s is a file existing in directory\\n' \"$2\" >&2\n                        echo \"Are you sure about overwriting?\"\n                        echo \"Press any key to continue\"\n                        while [ true ] ; do\n                                read -n 1\n                                if [ $? = 0 ] ; then\n                                        break 1 # break the while [ true ] loop\n                                fi\n                        done\n                fi\n                output=$2\n                shift\n        ;;\n        -h | --help )           \n                usage\n                exit\n        ;;\n        -?*)\n                printf 'WARN: Unknown option (ignored): %s\\n' \"$1\" >&2\n                exit\n        ;;\n        *) # Default case: No more options, so break out of the loop.\n                break\n    esac\n    shift\ndone\n\n# mandatory argument\nif [[ -z $p1 ]]; then\n        echo 'ERROR: --para is mandatory argument'\n        exit\nfi\n\n# input after options are put into $@\nbams=$@\n\n# a simple function to execute \nprint_out()\n{\n        for bam in ${bams[@]}; do\n                echo \"$bam\"\n        done\n}\n\n# show what --para take in\necho \"$p1\"\n\n# execute function output\nif [[ ! -z $output ]]; then\n        print_out > $output\nelse\n        print_out\nfi\n\n\nThe second method is to use getopts function with function-specific variables $OPTARG and $OPTIND to track the option value and option number. It can only take in the short format “-” options. The : following the -o will be passed to $OPTARG, thus, the different between “options with argument” and “options without argument” are shown in o: and o in getopts format.\n\n\nCode\nwhile getopts \":ho:\" opt; do\n        case ${opt} in\n                h )\n                        echo \"usage: bash $0 -o output_file folder1 folder2 ...\"\n                        exit\n                ;;\n                o )\n                        output=$OPTARG\n                ;;\n                \\? )\n                        echo \"Invalid option: $OPTARG\" 1>&2\n                        exit\n                ;;\n                : )\n                        echo \"Invalid option: $OPTARG requires an argument\" 1>&2\n                        exit\n                ;;\n        esac\ndone\nshift $((OPTIND -1))\ndirs=$@\n\n\nPersonally, I would recommend the first method. The additional reading can be found http://mywiki.wooledge.org/BashFAQ/035\n\n\nR\nMost R users execute the R script in Rstudio or R Console, and may never need to take in options. However, to execute R script in HPC environment, we submit Rscript script.R to the cluster for the jobs requiring high resources from command line.\nFor first type of options, commandArgs is all you need. It parses all arguments after script.R to the arguments vector.\n\n\nCode\nargs = commandArgs(trailingOnly=TRUE)\nfile1=args[1]\nfile2=args[2]\n\n\nFor the second type of options, package optparse is useful. Function make_option is used to specify each option type (matching pattern, option type, default value, …). To distinguish “options with argument” and “options without argument”, we can specify action argument in make_option function.\n\noptions with argument: action=\"store\", type=\"character\" (# this is default)\noptions without argument: action=\"store_true\" (# by default, type=\"logical\")\n\nAfter making option list, we use parse_args(OptionParser(option_list)) to assign options to a list value (with long flag option as list element name).\n\n\nCode\nlibrary(optparse)\n\noption_list = list(\n  # parameter 1 \n  make_option(\n    c(\"-p\",\"--para\"),\n    type=\"integer\", \n    default=1, \n    help=\"parameter 1 [default= %default]\"\n    ),\n  # optional output\n    make_option(\n      c(\"-o\", \"--out\"), \n      type=\"character\", \n      default=stdout(), \n    help=\"output file name [default= STDOUT]\", \n      metavar=\"character\"\n     ),\n  # verbose\n  make_option(\n      c(\"-v\", \"--verbose\"), \n      action=\"store_true\",\n      default=F\n     )\n)\n \nopts = parse_args(OptionParser(option_list=option_list))\nopts\n\n\nThings need to be cautious\n\nfinal list, by default, have help function, thus no need to specify -h. To visualize the help page\n\n\n\nCode\nparse_args(OptionParser(option_list=option_list), args = c(\"--help\"))\n\n\n\nlong flag option is required.\ndefault argument in function make_option must not be NULL, otherwise, the option will not be included in the final list.\nThere are other useful arguments including dest, callback and metavar. Learn more from\n\nBesides package optparse, argparser is another popular package. Please read this blog for tutorial.\n\n\nPerl\nPerl script takes every argument (after script) from command line into a special array @ARGV. We can easily read first type of options by parsing through @ARGV. This is very similar to commandArgs in R.\n\n\nCode\n#!/usr/bin/perl\nmy $usage=\"$0 file1 [file2 file3...]\nThis script is to print out first column of each file\nIt requires at least one input file \n\";\n\nif (scalar @ARGV < 1){\n  die $usage; # ensure there are arguments following the script\n}else{\n  for (my $i=0; $i < scalar @ARGV; $i++){ # go through each input file\n    open IN, \"<$ARGV[$i]\";\n    while (<IN>){\n      chomp;\n      my @items=split(/\\t/,$_);\n      print \"$items[0]\\n\";\n    }\n    close IN;\n  }\n}\n\n\nIn above script, another special variable $0 was used. It represents the script name itself (for example we can save above script as “print_col1.pl”). Thus, when the script is not followed by an input file, it will print usage\n\nprint_col1.pl file1 [file2 file3…]\nThis script is to print out first column of each file It requires at least one input file\n\nFor the second type of options, perl uses a module Getopt to parse options. The following script shows an example to print sequence length based on file format (fasta vs fastq).\n\n\nCode\n#!/usr/bin/perl\nuse Getopt::Long;\n\nmy $usage=\"$0 [--format fasta] [--seqN] [--header] file [file2 file3 ...]\nthis script is to calculate sequence file from fastq/fasta file\n--format fasta|fastq # default is fasta\n--seqN integer # default is everything\n--header # default no header added\noutput directly to STDOUT as seq_name[tab]length\n\";\n\nmy $format=\"fasta\"; # set default as fasta format.\nmy $seqN=0; # set default for number of sequence to print (0 here means print all sequences)\nmy $header = 0; # option variable with default value (false)\nGetOptions(\n        \"format=s\" => \\$format, # the option here will read as string (s)\n        \"seqN=i\" => \\$seqN, # the option here will read as numeric (i)\n        \"header\"  => \\$header  # flag: if --header specified, it will become true\n);\n\nmy $n;\nif ($seqN!=0){\n  $n=0;\n}\nif (scalar @ARGV < 1){\n        die $usage;\n}else{\n        OUTER: for (my $i=0; $i < scalar @ARGV; $i++){\n                if ($header!=0){\n                  print \"seq_name\\tseq_len\\n\";\n                }\n                my $file=$ARGV[$i];\n                open IN, \"<$file\";\n                if ($format eq \"fasta\"){\n                        my $header;\n                        my $seq;\n                        while (<IN>){\n                                chomp;\n                                if(/^>/){\n                                        if($header){\n                                                my $len=length($seq);\n                                                print \"$header\\t$len\\n\";\n                                                $n++;\n                                                if ($seqN!=0 && $n==$seqN){\n                                                  last OUTER;\n                                                }\n                                        }\n                                        s/^>//;\n                                        my @header=split(/\\s+/, $_);\n                                        $header=$header[0];\n                                        $seq=\"\";\n                                }else{\n                                        $seq=$seq.$_;\n                                }\n                        }\n                        my $len=length($seq);\n                        print \"$header\\t$len\\n\";\n                }\n                elsif($format eq \"fastq\"){\n                        my $header;\n                        my $seq;\n                        my $line;\n                        while (<IN>){\n                                chomp;\n                                if ($line % 4==0){\n                                        if($header){\n                                                my $len=length($seq);\n                                                print \"$header\\t$len\\n\";\n                                                $n++;\n                                                if ($seqN!=0 && $n==$seqN){\n                                                  last OUTER;\n                                                }\n                                        }\n                                        s/^@//;\n                                        my @header=split(/\\s+/, $_);\n                                        $header=$header[0];\n                                        \n                                }elsif($line % 4==1){\n                                        $seq=$_;\n                                }\n                                $line++;\n                        }\n                        my $len=length($seq);\n                        print \"$header\\t$len\\n\";\n                }\n                close IN;\n        }\n}\n\n\nFor more usage example of Getopt, please refer to its perldoc page."
  },
  {
    "objectID": "posts/2019-08-24_network_analysis_part2.html",
    "href": "posts/2019-08-24_network_analysis_part2.html",
    "title": "Network Analysis in R - Part 2",
    "section": "",
    "text": "In last post, I covered the basic components of IGRAPH objects and how to manipulate IGRAPH. You may notice that most of those manipulation do not really require a IGRAPH object to play with. However, in this post, you will realize the advantage of using IGRAPH over data.frame in network analysis.\nIn this session, we are going to use a new un-directed graph called gr generated by sample_gnp()."
  },
  {
    "objectID": "posts/2019-08-24_network_analysis_part2.html#degree-and-strength",
    "href": "posts/2019-08-24_network_analysis_part2.html#degree-and-strength",
    "title": "Network Analysis in R - Part 2",
    "section": "Degree and strength",
    "text": "Degree and strength\nDegree measures the number of edges connected to given vertex. In igraph, we use degree. Be aware that, for directed graph, the node degree can be “in-degree” (the edge number pointing to the node) and “out-degree” (the edge number pointing from node). We can also summaries the all degree by using degree_distribution.\n\n\nCode\n# get degree for each node \ndegree(gr, v=1:10)\n\n\n [1] 1 1 4 1 3 2 1 4 3 2\n\n\nCode\n# degree distribution\ndegree_distribution(gr) # probability for degree 0,1,2,3,4\n\n\n[1] 0.0 0.4 0.2 0.2 0.2\n\n\nstrength is weighted version of degree, by summing up the edge weights of the adjacent edges for each vertex.\n\n\nCode\n# add random weight attribute\nset.seed(12)\ngr2 <- gr %>% set_edge_attr(\n  \"weight\",index=E(gr),\n  value=sample(seq(0,1,0.05),size=length(E(gr)))\n)\n# calculate strength\nstrength(gr2, weights = E(gr)$weight)\n\n\n [1] 1.00 0.05 2.65 0.20 1.45 1.20 0.20 2.85 1.85 1.65"
  },
  {
    "objectID": "posts/2019-08-24_network_analysis_part2.html#orderdistance-and-path",
    "href": "posts/2019-08-24_network_analysis_part2.html#orderdistance-and-path",
    "title": "Network Analysis in R - Part 2",
    "section": "Order/distance and path",
    "text": "Order/distance and path\nOrder measures the edge number from one node to the other. In igraph package, we use distances function to get order between two vertices. For directed graph, in mode only follow the paths toward the first node, while out mode goes away from the first node. If no connection can be made, Inf will be return.\n\n\nCode\n# count all edges from 1 to 10, regardless of direction \ndistances(gr, v=1, to=10, mode=\"all\", weights = NA)\n\n\n     [,1]\n[1,]    3\n\n\nCode\n# pairwise distance table \ndistances(gr, mode=\"all\")\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    0    4    2  Inf    3    3  Inf    2    1     3\n [2,]    4    0    2  Inf    1    2  Inf    3    3     3\n [3,]    2    2    0  Inf    1    2  Inf    1    1     1\n [4,]  Inf  Inf  Inf    0  Inf  Inf    1  Inf  Inf   Inf\n [5,]    3    1    1  Inf    0    1  Inf    2    2     2\n [6,]    3    2    2  Inf    1    0  Inf    1    2     2\n [7,]  Inf  Inf  Inf    1  Inf  Inf    0  Inf  Inf   Inf\n [8,]    2    3    1  Inf    2    1  Inf    0    1     1\n [9,]    1    3    1  Inf    2    2  Inf    1    0     2\n[10,]    3    3    1  Inf    2    2  Inf    1    2     0\n\n\nTo get detail route from one node to the other, we use path.\n\n\nCode\n# shortest path to connect\nall_shortest_paths(gr, 1,10)$res\n\n\n[[1]]\n+ 4/10 vertices, from 2d81952:\n[1]  1  9  8 10\n\n[[2]]\n+ 4/10 vertices, from 2d81952:\n[1]  1  9  3 10\n\n\nCode\n# all path to connect\nall_simple_paths(gr, 1,10)\n\n\n[[1]]\n+ 7/10 vertices, from 2d81952:\n[1]  1  9  3  5  6  8 10\n\n[[2]]\n+ 5/10 vertices, from 2d81952:\n[1]  1  9  3  8 10\n\n[[3]]\n+ 4/10 vertices, from 2d81952:\n[1]  1  9  3 10\n\n[[4]]\n+ 5/10 vertices, from 2d81952:\n[1]  1  9  8  3 10\n\n[[5]]\n+ 7/10 vertices, from 2d81952:\n[1]  1  9  8  6  5  3 10\n\n[[6]]\n+ 4/10 vertices, from 2d81952:\n[1]  1  9  8 10"
  },
  {
    "objectID": "posts/2019-08-24_network_analysis_part2.html#transitivity",
    "href": "posts/2019-08-24_network_analysis_part2.html#transitivity",
    "title": "Network Analysis in R - Part 2",
    "section": "Transitivity",
    "text": "Transitivity\nTransitivity measures the probability that the adjacent vertices of a vertex are connected. This is also called the clustering coefficient, a proxy to determine how well connected the graph is. This property is very important in social networks, and to a lesser degree in other networks.\n\n\nCode\n# two extreme classes -- full graph and ring graph\ng1 = make_full_graph(10)\nplot(g1)\n\n\n\n\n\nCode\ntransitivity(g1)\n\n\n[1] 1\n\n\nCode\ng2 = make_ring(10)\nplot(g2)\n\n\n\n\n\nCode\ntransitivity(g2)\n\n\n[1] 0\n\n\nThere are multiple different types of transitivity can be calculated (weighted or un-weighted). Also, the transitivity can be calculated locally for a sub-graph by specifying vertex ids. See details by ?transitivity"
  },
  {
    "objectID": "posts/2019-08-24_network_analysis_part2.html#centrality",
    "href": "posts/2019-08-24_network_analysis_part2.html#centrality",
    "title": "Network Analysis in R - Part 2",
    "section": "Centrality",
    "text": "Centrality\nCentrality indices identify the most important vertices within a graph. In other words, the “hub” of network. However, this “importance” can be conceived in two ways:\n\nrelation to a type of flow or transfer across the network.\ninvolvement in the cohesiveness of the network\n\nThe simplest of centrality indicator is degree centrality (centr_degree), aka, a node is important if it has most neighbors.\nBesides degree centrality, there are\n\ncloseness centrality (centr_clo) - a node is important if it takes the shortest mean distance from a vertex to other vertices\nbetween-ness centrality (centr_betw) - a node is important if extent to which a vertex lies on paths between other vertices are high.\neigenvector centrality (centr_eigen) - a node is important if it is linked to by other important nodes.\n\n\n\nCode\ncentr_degree(gr, mode=\"all\")\n\n\n$res\n [1] 1 1 4 1 3 2 1 4 3 2\n\n$centralization\n[1] 0.2\n\n$theoretical_max\n[1] 90\n\n\nCode\ncentr_clo(gr, mode = \"all\")\n\n\n$res\n [1] 0.3888889 0.3888889 0.7000000 1.0000000 0.5833333 0.5384615 1.0000000\n [8] 0.6363636 0.5833333 0.5000000\n\n$centralization\n[1] 0.8690613\n\n$theoretical_max\n[1] 4.235294\n\n\nCode\ncentr_betw(gr, directed = FALSE)\n\n\n$res\n [1] 0.0 0.0 8.0 0.0 6.5 1.0 0.0 4.5 6.0 0.0\n\n$centralization\n[1] 0.1666667\n\n$theoretical_max\n[1] 324\n\n\nCode\ncentr_eigen(gr,directed = FALSE)\n\n\n$vector\n [1] 2.519712e-01 1.933127e-01 1.000000e+00 2.093280e-17 5.762451e-01\n [6] 5.244145e-01 1.036823e-17 9.869802e-01 7.511000e-01 6.665713e-01\n\n$value\n[1] 2.980897\n\n$options\n$options$bmat\n[1] \"I\"\n\n$options$n\n[1] 10\n\n$options$which\n[1] \"LA\"\n\n$options$nev\n[1] 1\n\n$options$tol\n[1] 0\n\n$options$ncv\n[1] 0\n\n$options$ldv\n[1] 0\n\n$options$ishift\n[1] 1\n\n$options$maxiter\n[1] 1000\n\n$options$nb\n[1] 1\n\n$options$mode\n[1] 1\n\n$options$start\n[1] 1\n\n$options$sigma\n[1] 0\n\n$options$sigmai\n[1] 0\n\n$options$info\n[1] 0\n\n$options$iter\n[1] 8\n\n$options$nconv\n[1] 1\n\n$options$numop\n[1] 26\n\n$options$numopb\n[1] 0\n\n$options$numreo\n[1] 16\n\n\n$centralization\n[1] 0.6311756\n\n$theoretical_max\n[1] 8\n\n\nMany other centrality indicators refer to wiki page of Centrality."
  },
  {
    "objectID": "posts/2019-08-24_network_analysis_part2.html#decompose-graph",
    "href": "posts/2019-08-24_network_analysis_part2.html#decompose-graph",
    "title": "Network Analysis in R - Part 2",
    "section": "decompose graph",
    "text": "decompose graph\nTo split graph into connected sub-graph, decompose.graph calculates the connected components of your graph. A component is a sub-graph in which all nodes are inter-connected.\n\n\nCode\n# decompose graph to connected components\ndg <- decompose.graph(gr)\ndg\n\n\n[[1]]\nIGRAPH 9072470 U--- 8 10 -- Erdos-Renyi (gnp) graph\n+ attr: name (g/c), type (g/c), loops (g/l), p (g/n)\n+ edges from 9072470:\n [1] 2--4 3--4 4--5 3--6 5--6 1--7 3--7 6--7 3--8 6--8\n\n[[2]]\nIGRAPH cea9efc U--- 2 1 -- Erdos-Renyi (gnp) graph\n+ attr: name (g/c), type (g/c), loops (g/l), p (g/n)\n+ edge from cea9efc:\n[1] 1--2\n\n\nCode\n# summary statics graph components\ncomponents(gr)\n\n\n$membership\n [1] 1 1 1 2 1 1 2 1 1 1\n\n$csize\n[1] 8 2\n\n$no\n[1] 2\n\n\nCode\n# plot components\ncoords <- layout_(gr, nicely())\nplot(gr, layout=coords,\n     mark.groups = split(as_ids(V(gr)), components(gr)$membership)\n     )"
  },
  {
    "objectID": "posts/2019-08-24_network_analysis_part2.html#cliques",
    "href": "posts/2019-08-24_network_analysis_part2.html#cliques",
    "title": "Network Analysis in R - Part 2",
    "section": "Cliques",
    "text": "Cliques\nClique is a special sub-graph in which every two distinct vertices are adjacent. The direction is usually ignored for clique calculations\n\n\nCode\n# extract cliques that contain more than 3 vertices\ncliques(gr, min=3)\n\n\n[[1]]\n+ 3/10 vertices, from 2d81952:\n[1]  3  8 10\n\n[[2]]\n+ 3/10 vertices, from 2d81952:\n[1] 3 8 9\n\n\nCode\n# get cliques with largest number of vertices\nlargest_cliques(gr)\n\n\n[[1]]\n+ 3/10 vertices, from 2d81952:\n[1] 8 3 9\n\n[[2]]\n+ 3/10 vertices, from 2d81952:\n[1]  8  3 10\n\n\nCode\n# plot cliques\ncl <- cliques(gr, 3)\n\ncoords <- layout_(gr, nicely())\nplot(gr, layout=coords,\n     mark.groups=lapply(cl, function(g){as_ids(g)}), \n     mark.col=c(\"#C5E5E7\",\"#ECD89A\"))"
  },
  {
    "objectID": "posts/2019-08-24_network_analysis_part2.html#communities-and-modules",
    "href": "posts/2019-08-24_network_analysis_part2.html#communities-and-modules",
    "title": "Network Analysis in R - Part 2",
    "section": "Communities and modules",
    "text": "Communities and modules\nGraph communities structure is defined if the nodes of the network can be easily grouped into (potentially overlapping) sets of nodes such that each set of nodes is densely connected internally. Modularity is always used as a measure the strength of division of a network into community for optimization methods in detecting community structure in networks.\nThere are many algorithms to cluster graph to communities.\n\ncluster_edge_betweenness a hierarchical decomposition process where edges are removed in the decreasing order of their edge betweenness scores.\n\ncluster_optimal - a top-down hierarchical approach that optimizes the modularity function\ncluster_walktrap - an approach based on random walks\ncluster_fast_greedy\ncluster_label_prop\ncluster_leading_eigen\ncluster_Louvain\ncluster_spinglass\n\nWhich cluster method to use? Refer to this stackoverflow post for more information.\n\n\nCode\n# cluster graph using walktrap method, turn a ”communities” object\nwtc <- cluster_walktrap(gr) \nwtc\n\n\nIGRAPH clustering walktrap, groups: 3, mod: 0.33\n+ groups:\n  $`1`\n  [1]  1  3  8  9 10\n  \n  $`2`\n  [1] 2 5 6\n  \n  $`3`\n  [1] 4 7\n  \n\n\nCode\n# find membership for each vertex\nmembership(wtc)\n\n\n [1] 1 2 1 3 2 2 3 1 1 1\n\n\nCode\n# calculate modularity for walktrap clustering on this graph\nmodularity(wtc) \n\n\n[1] 0.3305785\n\n\nCode\n# plot community\ncoords <- layout_(gr, nicely())\nplot(wtc, gr, layout=coords)\n\n\n\n\n\nTo learn more about graph clustering:\n\nNCSU course slide Introduction to Graph Cluster Analysis\nMIT open course Finding Clusters in Graphs"
  },
  {
    "objectID": "posts/2020-03-30_shinyapp_tips.html",
    "href": "posts/2020-03-30_shinyapp_tips.html",
    "title": "External persistent data I/O using ShinyApp",
    "section": "",
    "text": "Shiny App is a fantastic application in Rstudio and makes the data processing more accessible (and fun!). Most easy shiny apps are made to represent data based on a given user input which is read into memory or temporal file by R and spit out tables or figures in the same process. However, to make an app that need to keep the user input data for persistent storage and present in the future process require some external data I/O.\nOne of example app is survey app, in which user inputs will be accumulated for future presentation. Shiny rstudio presents this topic in an article written in 2017. However, my recent trial of those methods caused some troubles, either the packages/functions are deprecated or more strict authorization applied. In this post, I am going to introduce three persistent storage I have tried in my recent projects and complement that 2017 article with the updates."
  },
  {
    "objectID": "posts/2020-03-30_shinyapp_tips.html#data-input-app",
    "href": "posts/2020-03-30_shinyapp_tips.html#data-input-app",
    "title": "External persistent data I/O using ShinyApp",
    "section": "Data input app",
    "text": "Data input app\nTo start, I want to mention a tutorial on how to make survey app. In the tutorial, it mentioned how to read, save and re-load user input data from shiny app on a local machine. The critical part include:\n\nCreate a table field to store each widget input (keep widget inputId and table field name same)\nSave each user input data with a unique name in provided storage directory (sprintf(\"%s_%s.rds\", as.integer(Sys.time()), digest::digest(data)))\n\nReload data file by file and field by field.\n\nReset survey by update widget\n\nIn the tutorial example, the “provided storage directory” is in a local machine. Here I am going to introduce three external storage methods (AWS, dropbox and google spreadsheet) in the context of this dummy survey app I experiment with for Rladies Philly mentor-ship program.\nIn this dummy app, following widgets were made.\n\n\nCode\n# define global options\ntypes=c(\"Speaker\",\"Mentor\")\nexpertises=c(\"Academia to industry transition\",\"Transition to new field/industry\",\"Project/team management\",\"Making data science more accessible\",\"Working with big datasets\",\"Language research\",\"Data cleaning\",\"Capacity building\",\"Global health\",\"Data visualization\",\"Package creation\",\"Geospatial science\",\"Ecological modeling\",\"Mental health\",\"Building scalable tools\",\"Reproducible research\",\"App development\")\nemployment=c(\"Academic\",\"Pharmaceutical\",\"Financial\",\"Business\",\"Research\",\"Quality assurance\",\"Government/public sector\")\nmeets=c(\"In-person\",\"Remote (e.g. by phone or online)\")\ngenders=c(\"She/her\", \"He/him\", \"They/them\",\"Other\")\n\n\n# define user input widgets, put inputId into a field vector for late saveData/loadData\nfields <- c(\"name_wig\", \"gender_wig\", \"linkedin_wig\", \"photo_wig\",\n            \"type_wig\", \"expertise_wig\", \"employment_wig\", \"meet_wig\")\n\n# user input widgets\nname_wig <- textInput(\"name_wig\", \"Name:\", \"\")\ngender_wig  <- radioButtons(\n        \"gender_wig\", \n        \"Pronouns:\",\n        genders, \n        inline = TRUE,\n        selected = \"none\"\n)\nlinkedin_wig <- textInput(\"linkedin_wig\",\"LinkedIn Profile Link:\",\"\")\nphoto_wig <- fileInput(\"photo_wig\", \"Your photo (eg. .jpeg, .png)\", accept = c(\"jpeg\",\"png\"))\ntype_wig <- checkboxGroupInput(\n        \"type_wig\",\n        \"Available as mentor and/or speaker?\", \n        types\n)\nexpertise_wig <- selectizeInput(\n        inputId = \"expertise_wig\",\n        label = \"Areas of expertise\", \n        choices =  expertises,\n        multiple = T,\n        options = list(create = TRUE)\n)\nemployment_wig <- selectizeInput(\n        inputId = \"employment_wig\",\n        label = \"Primary type of employment\", \n        choices =  employment,\n        multiple = F,\n        options = list(create = TRUE)\n)\nmeet_wig <- checkboxGroupInput(\n        \"meet_wig\",\n        \"If you are willing to serve as a mentor, \\nwhat is your preferred method of communication with your mentees?\", \n        meets\n)\n\n# button widgets\nclear_wig <- actionButton(\"clear\", \"Clear Form\")\nsubmit_wig <- actionButton(\"submit\", \"Submit\")"
  },
  {
    "objectID": "posts/2020-03-30_shinyapp_tips.html#aws",
    "href": "posts/2020-03-30_shinyapp_tips.html#aws",
    "title": "External persistent data I/O using ShinyApp",
    "section": "AWS",
    "text": "AWS\nIn 2017 rstudio article, {aws.s3} package is used for communication between app and AWS.S3 external database. {aws.s3} can be installed through.\n\n\nCode\ninstall.packages(\"aws.s3\", repos = \"https://cloud.R-project.org\")\n\n\nWhen I was making the app, the CRAN repo was orphan. The github repo of aws.s3 could not easily be installed while publishing the app on shinyapps.io or rstudio connect, because their github repo missed creator assignment in DESCRIPTION. Also Now it is back to normal with new commit.\n\nAuthentication\nNext step is to set up aws.s3, same as 2017 rstudio artical, use the code below to set up in R\n\n\nCode\ns3BucketName <- \"<bucket_name>\"\nSys.setenv(\"AWS_ACCESS_KEY_ID\" = \"<AWS_ACCESS_KEY_ID>\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"<AWS_SECRET_ACCESS_KEY>\",\n           \"AWS_DEFAULT_REGION\" = \"us-east-2\")\n\n\nTo use aws.s3, we first need to have a AWS account and set up s3 bucket. To set up a s3 bucket, you can sign in to the Console and click S3 under “Storage”. Under Amazon S3, you can create a bucket with a unique bucket name (Keep this name to s3BucketName) and selected region (Remember this selected region, it will become value for AWS_DEFAULT_REGION. Mine is us-east-2). Then you will be back to the bucket list page.\nTo obtain the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, drop down your profile name on the top right menu, click “My Security Credentials”,\n\nThen at “Access keys (access key ID and secret access key” click “Create New Access Key”. Remember to save this, you cannot find this access key listed later. \n\n\nsaveData\nIn the demo app, each user entry include text input and a picture file. To make the picture file and text input match for each entry, I keep the same prefix and save new image name as one variable in data.frame.\nThe saveData function code:\n\n\nCode\nsaveData <- function(input) {\n        # create a empty data frame\n        data <- data.frame(matrix(nrow=1,ncol=0))\n        # loop through every field\n        for (x in fields) {\n                var <- input[[x]]\n                if (x == \"photo_wig\" & length(var)!=0){\n                  # fileInput widget with value\n                        img_file=var$datapath\n                        if (grepl(\"\\\\.jpg|\\\\.JPG|\\\\.jpeg|\\\\.JPEG\",img_file)){\n                                img_format=\".jpeg\"\n                        }\n                        if (grepl(\"\\\\.png|\\\\.PNG\",img_file)){\n                                img_format=\".png\"\n                        }\n                }else if (x == \"photo_wig\" & length(var)==0){\n                  # fileInput widget without value, assign a place holder image saved in bucket\n                        img_file=\"unknown.jpg\"\n                }\n                else{\n                        if (length(var)==0){\n                          # text widgets without value\n                                data[[x]] <- \" \"\n                        }\n                        else if (length(var) > 1 ) {\n                          # text widgets (checkboxGroupInput) with multiple values\n                                \n                                data[[x]] <- list(var)\n                        } else {\n                          # text widgets with single value\n                                data[[x]] <- var\n                        }\n                }\n        }\n        # input timestamp\n        data$submit_time <- date()\n        \n        # Create a unique file name\n        name1=as.integer(Sys.time())\n        name2=digest::digest(data)\n        fileName <- sprintf(\n                \"%s_%s.rds\", \n                name1, \n                name2\n        )\n        \n        # rename imagefilename and save image file to s3\n        if (img_file!=\"unknown.jpg\"){\n                img_newName <-sprintf(\n                        paste0(\"%s_%s\",img_format), \n                        name1, \n                        name2\n                )\n                file.rename(from=img_file, to=file.path(tempdir(),img_newName))\n                # save the image file to aws s3\n                aws.s3::put_object(\n                  file = file.path(tempdir(),img_newName), \n                  object = img_newName, \n                  bucket = s3BucketName, \n                  check_region = F, acl = \"public-read\"\n                  )\n        }else{\n                img_newName = \"unknown.jpg\"\n        }\n        data[\"photo_wig\"]=paste0(\"https://rladiesmentor.s3.us-east-2.amazonaws.com/\",img_newName)\n        \n        # save df as rds to the aws s3\n        aws.s3::s3save(data, bucket = s3BucketName, object = fileName)\n        \n        \n}\n\n\n\n\nloadData\nTo retrive the data from bucket, we can use following loadData function\n\n\nCode\nloadData <- function() {\n        # read all the rds files into a list\n        files <- sapply(aws.s3::get_bucket(s3BucketName), function(x){x[[\"Key\"]]})\n        files <- files[grepl(\"\\\\.rds\",files)]\n        if (length(files) == 0) {\n                # create an empty data frame with additional timestamp column if no entries at aws s3\n                field_list <- c(fields, \"submit_time\")\n                data <- data.frame(matrix(ncol = length(field_list), nrow = 0))\n                names(data) <- field_list\n        } else {\n                # load data s3load entry by entry if there are entries at aws s3\n                data <- lapply(files, function(x) {\n                        aws.s3::s3load(x, bucket = s3BucketName)\n                        data\n                })\n                \n                # concatenate all data together into one data.frame\n                data <- do.call(rbind, data)\n        }\n        \n        colnames(data) = c(\"name\",\"pronoun\",\"linkedin\", \"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\",\"submit.timestamp\",\"photo.link\")\n        \n\n        # make image src as one output column\n        out = tibble(\n                photo=sapply(data$photo.link,function(pic){paste0('<img src=',pic,' height=52></img>')})\n        )\n        # make name column a link\n        out = out %>%\n                mutate(name=mapply(function(url,text){paste0(\"<a href='\",url,\"'>\",text,\"</a>\")}, data$linkedin, data$name))\n        \n        # output data frame for dataTableRender\n        out = bind_cols(\n                out %>% as.data.frame(),\n                data[,c(\"pronoun\",\"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\")]\n        )\n        out\n}\n\n\nTo make the image file readable by link, you have to change the bucket public access permission, and make anyone can read it."
  },
  {
    "objectID": "posts/2020-03-30_shinyapp_tips.html#dropbox",
    "href": "posts/2020-03-30_shinyapp_tips.html#dropbox",
    "title": "External persistent data I/O using ShinyApp",
    "section": "Dropbox",
    "text": "Dropbox\nrdrop2 is the package R used to communicate with dropbox, and can be directly installed from CRAN.\n\nAuthentication\nAfter installation, we need to authenticate R to access your dropbox (like AWS authentication key). Instead of obtaining directly from website, first time drop_auth() will direct you to web browser for dropbox authentication.\n\n\nCode\nlibrary(rdrop2)\n# you just need to run this part once (no need included in shinyapp code)\ndrop_auth()\n\n# for remote use (deploy app to shinyapps.io or rstudio connect), you can save your auth to rds and load it to host platform\ntoken <- drop_auth()\nsaveRDS(token, file = \"token.rds\")\n\n\nCaution: this token authorize anyone with token file an access to all the files in your dropbox account.\nWhen you are ready to use the token to allow access the data at remote setting, you can do\n\n\nCode\n# this part should be included in your shinyapp code\ntoken <- load(\"token.rds\")\ndrop_acc(dtoken = token)\n\n\n\n\nsaveData\nUnlike AWS S3, I choose to aggregate individual entries into one csv file (You can do the same thing in AWS S3 too). The saveData function for dropbox is\n\n\nCode\nsaveData <- function(input) {\n        # read previously stored csv file\n        old_df = rdrop2::drop_read_csv(\"mentors.csv\")\n        \n        # save one user entry to a new data frame (like AWS above)\n        data <- data.frame(matrix(nrow=1,ncol=0))\n        for (x in fields) {\n                var <- input[[x]]\n                if (x == \"photo_wig\" & length(var)!=0){\n                        img_file=var$datapath\n                        if (grepl(\"\\\\.jpg|\\\\.JPG|\\\\.jpeg|\\\\.JPEG\",img_file)){\n                                img_format=\".jpeg\"\n                        }\n                        if (grepl(\"\\\\.png|\\\\.PNG\",img_file)){\n                                img_format=\".png\"\n                        }\n                }else if (x == \"photo_wig\" & length(var)==0){\n                        img_file=\"unknown.jpg\"\n                }\n                else{\n                        if (length(var)==0){\n                                data[[x]] <- \" \"\n                        }\n                        else if (length(var) > 1 ) {\n                                # handles lists from checkboxGroup and multiple Select\n                                data[[x]] <- list(var)\n                        } else {\n                                # all other data types\n                                data[[x]] <- var\n                        }\n                }\n        }\n        data$submit_time <- date()\n        # Create a unique file name\n        name1=as.integer(Sys.time())\n        name2=digest::digest(data)\n        fileName <- sprintf(\n                \"%s_%s.rds\", \n                name1, \n                name2\n        )\n        \n        # rename and save imagefilename\n        if (img_file!=\"unknown.jpg\"){\n                img_newName <-sprintf(\n                        paste0(\"%s_%s\",img_format), \n                        name1, \n                        name2\n                )\n                file.rename(from=img_file, to=file.path(tempdir(),img_newName))\n                rdrop2::drop_upload(file.path(tempdir(),img_newName))\n        }else{\n                img_newName = \"unknown.jpg\"\n        }\n        \n        # add phone name to data column\n        data[\"photo_wig\"]=img_newName\n        colnames(data) = c(\"name\",\"pronoun\",\"linkedin\", \"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\",\"submit.timestamp\",\"photo.link\")\n        \n        # append new entry to the old_df\n        new_df = bind_rows(old_df, data)\n        # write new_df csv to a temp file\n        write.csv(new_df, file=file.path(tempdir(),\"mentors.csv\"))\n        # upload this temp file to dropbox\n        rdrop2::drop_upload(file.path(tempdir(),\"mentors.csv\"))\n}\n\n\n\n\nloadData\nFrom above example, you may notice that all the file need to be saved at local for a moment before uploading dropbox. In other words, rdrop2 only deals file level data. Thus, if you want to retrieve unstructural file (not csv), you have to download the file to local, then show it. It will not work for links (because no way to set public access permissions in dropbox). Thus at loadData, I cannot make the image readable unless I download data to the local. The following example only show the data frame load, comment out the image part.\n\n\nCode\nloadData <- function() {\n        # read csv\n        data <- drop_read_csv(\"mentors.csv\")\n        if (nrow(data) == 0) {\n                # create empty data frame with correct columns\n                field_list <- c(fields, \"submit_time\")\n                data <- data.frame(matrix(ncol = length(field_list), nrow = 0))\n                names(data) <- field_list\n        } \n        \n        # drop_get(\"jigglypuff.jpeg\")\n        # data\n        # out = tibble(\n        #         photo=sapply(data$photo.link,function(pic){paste0('<img src=',pic,' height=52></img>')})\n        # )\n        # out = out %>%\n        #         mutate(name=mapply(function(url,text){paste0(\"<a href='\",url,\"'>\",text,\"</a>\")}, data$linkedin, data$name))\n        # out = bind_cols(\n        #         out %>% as.data.frame(),\n        #         data[,c(\"pronoun\",\"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\")]\n        # )\n        out=data[,c(\"name\",\"pronoun\",\"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\")]\n        out\n}"
  },
  {
    "objectID": "posts/2020-03-30_shinyapp_tips.html#googlesheets",
    "href": "posts/2020-03-30_shinyapp_tips.html#googlesheets",
    "title": "External persistent data I/O using ShinyApp",
    "section": "googlesheets",
    "text": "googlesheets\nTwo packages googledrive and googlesheets4 are required for googlesheet data I/O. The main reason is that googlesheets4 have updated their security setting and made spreadsheet direct writing impossible. The way to get around is to use googledrive::drive_download to download the file to local, update the dataframe and save to a local file with same name like before, then use googledrive::drive_update to push the new file to the google drive. It is very similar to rdrop2 file-level communication method. (Note: both googledrive and googlesheets4 needs gargle_oauth).\n\nAuthentication\nGooglesheets used gargle_oauth to prompt a web page for authentication. The code to set up authentication at local\n\n\nCode\n# you just need to run this part once (no need included in shinyapp code)\ngargle::drive_auth()\ngooglesheets4::sheets_auth()\n\n\nUsually you do not need to explicitly prompt auth using above code. Using functions in googledrive and googlesheets4 will automatically trigger the authentication.\nAfter authentication, you can check your tokens by\n\n\nCode\ngargle::gargle_oauth_sitrep()\n\n\nThe authentication step automatically generated token files under ~/.R/gargle/gargle-oauth/. If the app work in local, that is all we need to do. If you want to deploy to hosting platform, we need to make this authentication non-interactive (no need for web browser to prompt a page). One way is to make your token files available for remote server access.\nTo make tokens available for remote server access, you can copy the email account authentication to the same directory app.R saved at. Since we have tokens associated with both googledrive and googlesheets4, we will end up have two token files. To move both token files to app directory. Using following shell code\n\n\nCode\nmkdir .secret/\ncd .secret/\ncp ~/.R/gargle/gargle-oauth/*youremailname* .\n\n\nWhen it is time to depoly, select .secret/ to upload to platform. In the app.R code, we just need to add following line to designate project-specific cache.\n\n\nCode\noptions(\n        gargle_oauth_cache = \".secret\",\n        gargle_oauth_email = TRUE\n)\n\n\nThis is not the most secure way, but easiest way. If you want to explore more secure way for this purpose, please ref to non-interacive authentication in gargle\n\n\nsaveData\nAs alreadly mentioned, googledrive use file-level communication. We first used drive_fine to find which spreadsheet to read, then download using googledrive::drive_download, finally update/unload spreadsheet googledrive::drive_update.\n\n\nCode\nsaveData <- function(input) {\n        # download previous spreadsheet to tempfile\n        tmpDir=file.path(tempdir(),\"mentors.csv\")\n        mentors=drive_find(pattern = \"mentors\", type = \"spreadsheet\")\n        drive_download(as_id(mentors), type=\"csv\", path=tmpDir, overwrite=T)\n        \n        # read spreadsheet to df\n        df = read_csv(tmpDir)\n        \n        # read input to data\n        data <- data.frame(matrix(nrow=1,ncol=0))\n        for (x in fields) {\n                var <- input[[x]]\n                if (length(var)==0){\n                        data[[x]] <- \" \"\n                }\n                else if (length(var) > 1 ) {\n                        # handles lists from checkboxGroup and multiple Select\n                        data[[x]] <- paste(var,collapse = \", \")\n                } else {\n                        # all other data types\n                        data[[x]] <- var\n                }\n        }\n        \n        data$submit_time <- Sys.time()\n        colnames(data) = c(\"name\",\"pronoun\",\"linkedin\", \"email\",\"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\",\"submit.timestamp\")\n        \n        # append new data\n        df = bind_rows(df, data)\n        \n        # write into tempfile\n        write_csv(df, path=tmpDir, na=\" \")\n        \n        # update mentors spreadsheet\n        mentors <- mentors %>% \n          drive_update(\n                tmpDir,\n                name=\"mentors\"\n        )\n        # drive_rm(mentors)\n}\n\n\n\n\nloadData\ngooglesheets have a function read_sheet to read googlesheets directly to data.frame.\n\n\nCode\nloadData <- function() {\n        # read spreadsheet\n        sheet_id=drive_find(pattern = \"mentors\", type = \"spreadsheet\")$id\n        data=read_sheet(sheet_id)\n        # data\n        names = tibble(\n                name=mapply(\n                        function(url,text){\n                                if(url!=\" \"){\n                                        paste0(\"<a href='\",url,\"'>\",text,\"</a>\")\n                                }else if (url!=\" \"){\n                                        paste0(\"<a href='\",url,\"'>\",text,\"</a>\")\n                                }\n                        }, \n                        data$linkedin, data$name\n                        )\n        )\n        links = tibble(\n                links=mapply(\n                        function(email, linkedin,text){\n                                if(email!=\" \" & linkedin==\" \"){\n                                        paste0(\"<a href=mailto:\",email,\">\",\"Email\",\"</a>\")\n                                } else if (linkedin!=\" \" & email==\" \"){\n                                        paste0(\"<a href='\",linkedin,\"'>\",\"LinkedIn\",\"</a>\")\n                                } else {\n                                        paste(\n                                                paste0(\"<a href=mailto:\",email,\">\",\"Email\",\"</a>\"),\n                                                paste0(\"<a href='\",linkedin,\"'>\",\"LinkedIn\",\"</a>\")\n                                        )\n                                }\n                        }, \n                        data$email, data$linkedin, data$name\n                )\n        )\n        out = bind_cols(\n                names %>% as.data.frame(),\n                data[,c(\"pronoun\",\"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\")],\n                links %>% as.data.frame()\n        )\n        out\n}"
  },
  {
    "objectID": "posts/2020-03-30_shinyapp_tips.html#final-remarks",
    "href": "posts/2020-03-30_shinyapp_tips.html#final-remarks",
    "title": "External persistent data I/O using ShinyApp",
    "section": "Final remarks",
    "text": "Final remarks\nIn this post, we introduce three ways to load and save data to external storage clound. AWS s3 is most secure and fleasible among three. It can store and load unstructure data easily, thus it does not require much memory cache from host server. But it is not free when data is very big. Dropbox can save both tubular and unstructural data, but retrieve unstructure requires downloading file to cache. Googlesheets can only read/save tubular data. Both dropbox and googlesheets have some secure concerns, but you can create a free account and designate that account for app development/test only to reduce concerns for security. The complete codes for finished app can be accessed from my github."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Learning with sckinta",
    "section": "",
    "text": "R-Ladies Book Club: Chapter 6 linear model selection and regularization\nChapter 6 from ISLR2\n2022-08-18 HTML slides\n\n\n\nR-Ladies Book Club: Introduction to Tidymodels\nThe opening talk at 2022 R-Ladies Philly Book Club\n2022-07-14 HTML slides\n\n\n\nNetwork analysis and visualization\nR-Ladies Philly workshop on network analyses using R igraph object and visualization with ggraph package.\n2019-10-08 PDF slides"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning with sckinta",
    "section": "",
    "text": "Hi! Thank you for stopping by! I am a computational biologist at the Amgen Inc., and volunteer as a co-organizer at Rladies Philly. Originally from China, I came to the U.S. in 2011 and completed my PhD in Biology at the University of Virginia in 2017. Starting as a Perl programmer, I picked up R and Python by taking online courses and attending local meetup workshops. I enjoy learning new programming skills and believe learning is a life-time mission. I would like to dedicate this personal website to sharing the study notes and projects in my learning journey. Hopefully it will help or motivate other self-learners."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Learning with sckinta",
    "section": "",
    "text": "R\n\n\nGLM\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2022\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2022\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2021\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nbash\n\n\nperl\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2020\n\n\n21 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ndata wrangle\n\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2020\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nML\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2020\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2020\n\n\n32 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbash\n\n\ngit\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2020\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nnetwork\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2019\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nnetwork\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2019\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nnetwork\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2019\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Learning with sckinta",
    "section": "",
    "text": "options: eval, echo, output, warning, error, include https://quarto.org/docs/computations/execution-options.html\n\nglobal set options in YAML\n\nexecute:\necho: true\nwarning: false\n\ncode-specific options\n\n#| echo: false"
  }
]
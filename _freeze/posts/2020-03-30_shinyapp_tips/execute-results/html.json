{
  "hash": "8c69fa88192cfe5b60e42e31af5ec47b",
  "result": {
    "markdown": "---\ntitle: 'External persistent data I/O using ShinyApp'\ndate: \"2020-03-30\"\ncategories: [\"R\", \"shiny\"]\nimage: 'https://www.acsgrp.com/sites/default/files/sb-5.png'\n---\n\n\n\n\nShiny App is a fantastic application in Rstudio and makes the data processing more accessible (and fun!). Most easy shiny apps are made to represent data based on a given user input which is read into memory or temporal file by R and spit out tables or figures in the same process. However, to make an app that need to keep the user input data for persistent storage and present in the future process require some external data I/O.\n\nOne of example app is survey app, in which user inputs will be accumulated for future presentation. Shiny rstudio presents this topic in an [article](https://shiny.rstudio.com/articles/persistent-data-storage.html) written in 2017. However, my recent trial of those methods caused some troubles, either the packages/functions are deprecated or more strict authorization applied. In this post, I am going to introduce three persistent storage I have tried in my recent projects and complement that 2017 article with the updates.\n\n## Data input app\n\nTo start, I want to mention a [tutorial](https://gupsych.github.io/tquant/data-input.html) on how to make survey app. In the tutorial, it mentioned how to read, save and re-load user input data from shiny app on a local machine. The critical part include:\n\n-   Create a table field to store each widget input (keep widget `inputId` and table field name same)\n-   Save each user input data with a unique name in provided storage directory (`sprintf(\"%s_%s.rds\", as.integer(Sys.time()), digest::digest(data))`)\\\n-   Reload data file by file and field by field.\\\n-   Reset survey by `update` widget\n\nIn the tutorial example, the \"provided storage directory\" is in a local machine. Here I am going to introduce three external storage methods (AWS, dropbox and google spreadsheet) in the context of this dummy [survey app](https://sckinta.shinyapps.io/SpeakerSignup/) I experiment with for Rladies Philly mentor-ship program.\n\nIn this dummy app, following widgets were made.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define global options\ntypes=c(\"Speaker\",\"Mentor\")\nexpertises=c(\"Academia to industry transition\",\"Transition to new field/industry\",\"Project/team management\",\"Making data science more accessible\",\"Working with big datasets\",\"Language research\",\"Data cleaning\",\"Capacity building\",\"Global health\",\"Data visualization\",\"Package creation\",\"Geospatial science\",\"Ecological modeling\",\"Mental health\",\"Building scalable tools\",\"Reproducible research\",\"App development\")\nemployment=c(\"Academic\",\"Pharmaceutical\",\"Financial\",\"Business\",\"Research\",\"Quality assurance\",\"Government/public sector\")\nmeets=c(\"In-person\",\"Remote (e.g. by phone or online)\")\ngenders=c(\"She/her\", \"He/him\", \"They/them\",\"Other\")\n\n\n# define user input widgets, put inputId into a field vector for late saveData/loadData\nfields <- c(\"name_wig\", \"gender_wig\", \"linkedin_wig\", \"photo_wig\",\n            \"type_wig\", \"expertise_wig\", \"employment_wig\", \"meet_wig\")\n\n# user input widgets\nname_wig <- textInput(\"name_wig\", \"Name:\", \"\")\ngender_wig  <- radioButtons(\n        \"gender_wig\", \n        \"Pronouns:\",\n        genders, \n        inline = TRUE,\n        selected = \"none\"\n)\nlinkedin_wig <- textInput(\"linkedin_wig\",\"LinkedIn Profile Link:\",\"\")\nphoto_wig <- fileInput(\"photo_wig\", \"Your photo (eg. .jpeg, .png)\", accept = c(\"jpeg\",\"png\"))\ntype_wig <- checkboxGroupInput(\n        \"type_wig\",\n        \"Available as mentor and/or speaker?\", \n        types\n)\nexpertise_wig <- selectizeInput(\n        inputId = \"expertise_wig\",\n        label = \"Areas of expertise\", \n        choices =  expertises,\n        multiple = T,\n        options = list(create = TRUE)\n)\nemployment_wig <- selectizeInput(\n        inputId = \"employment_wig\",\n        label = \"Primary type of employment\", \n        choices =  employment,\n        multiple = F,\n        options = list(create = TRUE)\n)\nmeet_wig <- checkboxGroupInput(\n        \"meet_wig\",\n        \"If you are willing to serve as a mentor, \\nwhat is your preferred method of communication with your mentees?\", \n        meets\n)\n\n# button widgets\nclear_wig <- actionButton(\"clear\", \"Clear Form\")\nsubmit_wig <- actionButton(\"submit\", \"Submit\")\n```\n:::\n\n\n## AWS\n\nIn 2017 rstudio article, `{aws.s3}` package is used for communication between app and AWS.S3 external database. `{aws.s3}` can be installed through.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"aws.s3\", repos = \"https://cloud.R-project.org\")\n```\n:::\n\n\n*When I was making the app, the CRAN repo was orphan. The [github repo of aws.s3](https://github.com/cloudyr/aws.s3) could not easily be installed while publishing the app on shinyapps.io or rstudio connect, because their github repo missed creator assignment in DESCRIPTION. Also Now it is back to normal with new [commit](https://github.com/cloudyr/aws.s3/issues/335).*\n\n#### Authentication\n\nNext step is to set up aws.s3, same as 2017 rstudio artical, use the code below to set up in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3BucketName <- \"<bucket_name>\"\nSys.setenv(\"AWS_ACCESS_KEY_ID\" = \"<AWS_ACCESS_KEY_ID>\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"<AWS_SECRET_ACCESS_KEY>\",\n           \"AWS_DEFAULT_REGION\" = \"us-east-2\")\n```\n:::\n\n\nTo use aws.s3, we first need to have a AWS account and set up s3 bucket. To set up a s3 bucket, you can sign in to the [Console](https://aws.amazon.com/console/) and click S3 under \"Storage\". Under Amazon S3, you can create a bucket with a unique bucket name (Keep this name to `s3BucketName`) and selected region (Remember this selected region, it will become value for `AWS_DEFAULT_REGION`. Mine is us-east-2). Then you will be back to the bucket list page.\n\nTo obtain the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, drop down your profile name on the top right menu, click \"My Security Credentials\",\n\n![](https://i.ibb.co/nRL7M41/pic1.png)\n\nThen at \"Access keys (access key ID and secret access key\" click \"Create New Access Key\". Remember to save this, you cannot find this access key listed later. ![](https://i.ibb.co/89ZXJrd/pic2.png)\n\n#### saveData\n\nIn the [demo app](https://sckinta.shinyapps.io/SpeakerSignup/), each user entry include text input and a picture file. To make the picture file and text input match for each entry, I keep the same prefix and save new image name as one variable in data.frame.\n\nThe `saveData` function code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveData <- function(input) {\n        # create a empty data frame\n        data <- data.frame(matrix(nrow=1,ncol=0))\n        # loop through every field\n        for (x in fields) {\n                var <- input[[x]]\n                if (x == \"photo_wig\" & length(var)!=0){\n                  # fileInput widget with value\n                        img_file=var$datapath\n                        if (grepl(\"\\\\.jpg|\\\\.JPG|\\\\.jpeg|\\\\.JPEG\",img_file)){\n                                img_format=\".jpeg\"\n                        }\n                        if (grepl(\"\\\\.png|\\\\.PNG\",img_file)){\n                                img_format=\".png\"\n                        }\n                }else if (x == \"photo_wig\" & length(var)==0){\n                  # fileInput widget without value, assign a place holder image saved in bucket\n                        img_file=\"unknown.jpg\"\n                }\n                else{\n                        if (length(var)==0){\n                          # text widgets without value\n                                data[[x]] <- \" \"\n                        }\n                        else if (length(var) > 1 ) {\n                          # text widgets (checkboxGroupInput) with multiple values\n                                \n                                data[[x]] <- list(var)\n                        } else {\n                          # text widgets with single value\n                                data[[x]] <- var\n                        }\n                }\n        }\n        # input timestamp\n        data$submit_time <- date()\n        \n        # Create a unique file name\n        name1=as.integer(Sys.time())\n        name2=digest::digest(data)\n        fileName <- sprintf(\n                \"%s_%s.rds\", \n                name1, \n                name2\n        )\n        \n        # rename imagefilename and save image file to s3\n        if (img_file!=\"unknown.jpg\"){\n                img_newName <-sprintf(\n                        paste0(\"%s_%s\",img_format), \n                        name1, \n                        name2\n                )\n                file.rename(from=img_file, to=file.path(tempdir(),img_newName))\n                # save the image file to aws s3\n                aws.s3::put_object(\n                  file = file.path(tempdir(),img_newName), \n                  object = img_newName, \n                  bucket = s3BucketName, \n                  check_region = F, acl = \"public-read\"\n                  )\n        }else{\n                img_newName = \"unknown.jpg\"\n        }\n        data[\"photo_wig\"]=paste0(\"https://rladiesmentor.s3.us-east-2.amazonaws.com/\",img_newName)\n        \n        # save df as rds to the aws s3\n        aws.s3::s3save(data, bucket = s3BucketName, object = fileName)\n        \n        \n}\n```\n:::\n\n\n#### loadData\n\nTo retrive the data from bucket, we can use following `loadData` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloadData <- function() {\n        # read all the rds files into a list\n        files <- sapply(aws.s3::get_bucket(s3BucketName), function(x){x[[\"Key\"]]})\n        files <- files[grepl(\"\\\\.rds\",files)]\n        if (length(files) == 0) {\n                # create an empty data frame with additional timestamp column if no entries at aws s3\n                field_list <- c(fields, \"submit_time\")\n                data <- data.frame(matrix(ncol = length(field_list), nrow = 0))\n                names(data) <- field_list\n        } else {\n                # load data s3load entry by entry if there are entries at aws s3\n                data <- lapply(files, function(x) {\n                        aws.s3::s3load(x, bucket = s3BucketName)\n                        data\n                })\n                \n                # concatenate all data together into one data.frame\n                data <- do.call(rbind, data)\n        }\n        \n        colnames(data) = c(\"name\",\"pronoun\",\"linkedin\", \"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\",\"submit.timestamp\",\"photo.link\")\n        \n\n        # make image src as one output column\n        out = tibble(\n                photo=sapply(data$photo.link,function(pic){paste0('<img src=',pic,' height=52></img>')})\n        )\n        # make name column a link\n        out = out %>%\n                mutate(name=mapply(function(url,text){paste0(\"<a href='\",url,\"'>\",text,\"</a>\")}, data$linkedin, data$name))\n        \n        # output data frame for dataTableRender\n        out = bind_cols(\n                out %>% as.data.frame(),\n                data[,c(\"pronoun\",\"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\")]\n        )\n        out\n}\n```\n:::\n\n\nTo make the image file readable by link, you have to change the bucket public access permission, and make anyone can read it.\n\n## Dropbox\n\n`rdrop2` is the package R used to communicate with dropbox, and can be directly installed from CRAN.\n\n#### Authentication\n\nAfter installation, we need to authenticate R to access your dropbox (like AWS authentication key). Instead of obtaining directly from website, first time `drop_auth()` will direct you to web browser for dropbox authentication.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rdrop2)\n# you just need to run this part once (no need included in shinyapp code)\ndrop_auth()\n\n# for remote use (deploy app to shinyapps.io or rstudio connect), you can save your auth to rds and load it to host platform\ntoken <- drop_auth()\nsaveRDS(token, file = \"token.rds\")\n```\n:::\n\n\nCaution: this token authorize anyone with token file an access to all the files in your dropbox account.\n\nWhen you are ready to use the token to allow access the data at remote setting, you can do\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# this part should be included in your shinyapp code\ntoken <- load(\"token.rds\")\ndrop_acc(dtoken = token)\n```\n:::\n\n\n#### saveData\n\nUnlike AWS S3, I choose to aggregate individual entries into one csv file (You can do the same thing in AWS S3 too). The `saveData` function for dropbox is\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveData <- function(input) {\n        # read previously stored csv file\n        old_df = rdrop2::drop_read_csv(\"mentors.csv\")\n        \n        # save one user entry to a new data frame (like AWS above)\n        data <- data.frame(matrix(nrow=1,ncol=0))\n        for (x in fields) {\n                var <- input[[x]]\n                if (x == \"photo_wig\" & length(var)!=0){\n                        img_file=var$datapath\n                        if (grepl(\"\\\\.jpg|\\\\.JPG|\\\\.jpeg|\\\\.JPEG\",img_file)){\n                                img_format=\".jpeg\"\n                        }\n                        if (grepl(\"\\\\.png|\\\\.PNG\",img_file)){\n                                img_format=\".png\"\n                        }\n                }else if (x == \"photo_wig\" & length(var)==0){\n                        img_file=\"unknown.jpg\"\n                }\n                else{\n                        if (length(var)==0){\n                                data[[x]] <- \" \"\n                        }\n                        else if (length(var) > 1 ) {\n                                # handles lists from checkboxGroup and multiple Select\n                                data[[x]] <- list(var)\n                        } else {\n                                # all other data types\n                                data[[x]] <- var\n                        }\n                }\n        }\n        data$submit_time <- date()\n        # Create a unique file name\n        name1=as.integer(Sys.time())\n        name2=digest::digest(data)\n        fileName <- sprintf(\n                \"%s_%s.rds\", \n                name1, \n                name2\n        )\n        \n        # rename and save imagefilename\n        if (img_file!=\"unknown.jpg\"){\n                img_newName <-sprintf(\n                        paste0(\"%s_%s\",img_format), \n                        name1, \n                        name2\n                )\n                file.rename(from=img_file, to=file.path(tempdir(),img_newName))\n                rdrop2::drop_upload(file.path(tempdir(),img_newName))\n        }else{\n                img_newName = \"unknown.jpg\"\n        }\n        \n        # add phone name to data column\n        data[\"photo_wig\"]=img_newName\n        colnames(data) = c(\"name\",\"pronoun\",\"linkedin\", \"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\",\"submit.timestamp\",\"photo.link\")\n        \n        # append new entry to the old_df\n        new_df = bind_rows(old_df, data)\n        # write new_df csv to a temp file\n        write.csv(new_df, file=file.path(tempdir(),\"mentors.csv\"))\n        # upload this temp file to dropbox\n        rdrop2::drop_upload(file.path(tempdir(),\"mentors.csv\"))\n}\n```\n:::\n\n\n#### loadData\n\nFrom above example, you may notice that all the file need to be saved at local for a moment before uploading dropbox. In other words, rdrop2 only deals file level data. Thus, if you want to retrieve unstructural file (not csv), you have to download the file to local, then show it. It will not work for links (because no way to set public access permissions in dropbox). Thus at loadData, I cannot make the image readable unless I download data to the local. The following example only show the data frame load, comment out the image part.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloadData <- function() {\n        # read csv\n        data <- drop_read_csv(\"mentors.csv\")\n        if (nrow(data) == 0) {\n                # create empty data frame with correct columns\n                field_list <- c(fields, \"submit_time\")\n                data <- data.frame(matrix(ncol = length(field_list), nrow = 0))\n                names(data) <- field_list\n        } \n        \n        # drop_get(\"jigglypuff.jpeg\")\n        # data\n        # out = tibble(\n        #         photo=sapply(data$photo.link,function(pic){paste0('<img src=',pic,' height=52></img>')})\n        # )\n        # out = out %>%\n        #         mutate(name=mapply(function(url,text){paste0(\"<a href='\",url,\"'>\",text,\"</a>\")}, data$linkedin, data$name))\n        # out = bind_cols(\n        #         out %>% as.data.frame(),\n        #         data[,c(\"pronoun\",\"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\")]\n        # )\n        out=data[,c(\"name\",\"pronoun\",\"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\")]\n        out\n}\n```\n:::\n\n\n## googlesheets\n\nTwo packages `googledrive` and `googlesheets4` are required for googlesheet data I/O. The main reason is that googlesheets4 have updated their security setting and made spreadsheet direct writing impossible. The way to get around is to use `googledrive::drive_download` to download the file to local, update the dataframe and save to a local file with same name like before, then use `googledrive::drive_update` to push the new file to the google drive. It is very similar to `rdrop2` file-level communication method. (Note: both `googledrive` and `googlesheets4` needs `gargle_oauth`).\n\n#### Authentication\n\nGooglesheets used `gargle_oauth` to prompt a web page for authentication. The code to set up authentication at local\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# you just need to run this part once (no need included in shinyapp code)\ngargle::drive_auth()\ngooglesheets4::sheets_auth()\n```\n:::\n\n\nUsually you do not need to explicitly prompt auth using above code. Using functions in `googledrive` and `googlesheets4` will automatically trigger the authentication.\n\nAfter authentication, you can check your tokens by\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngargle::gargle_oauth_sitrep()\n```\n:::\n\n\nThe authentication step automatically generated token files under `~/.R/gargle/gargle-oauth/`. If the app work in local, that is all we need to do. If you want to deploy to hosting platform, we need to make this authentication non-interactive (no need for web browser to prompt a page). One way is to make your token files available for remote server access.\n\nTo make tokens available for remote server access, you can copy the email account authentication to the same directory app.R saved at. Since we have tokens associated with both `googledrive` and `googlesheets4`, we will end up have two token files. To move both token files to app directory. Using following shell code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmkdir .secret/\ncd .secret/\ncp ~/.R/gargle/gargle-oauth/*youremailname* .\n```\n:::\n\n\nWhen it is time to depoly, select .secret/ to upload to platform. In the app.R code, we just need to add following line to designate project-specific cache.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(\n        gargle_oauth_cache = \".secret\",\n        gargle_oauth_email = TRUE\n)\n```\n:::\n\n\nThis is not the most secure way, but easiest way. If you want to explore more secure way for this purpose, please ref to [non-interacive authentication in gargle](https://gargle.r-lib.org/articles/non-interactive-auth.html)\n\n#### saveData\n\nAs alreadly mentioned, googledrive use file-level communication. We first used `drive_fine` to find which spreadsheet to read, then download using `googledrive::drive_download`, finally update/unload spreadsheet `googledrive::drive_update`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveData <- function(input) {\n        # download previous spreadsheet to tempfile\n        tmpDir=file.path(tempdir(),\"mentors.csv\")\n        mentors=drive_find(pattern = \"mentors\", type = \"spreadsheet\")\n        drive_download(as_id(mentors), type=\"csv\", path=tmpDir, overwrite=T)\n        \n        # read spreadsheet to df\n        df = read_csv(tmpDir)\n        \n        # read input to data\n        data <- data.frame(matrix(nrow=1,ncol=0))\n        for (x in fields) {\n                var <- input[[x]]\n                if (length(var)==0){\n                        data[[x]] <- \" \"\n                }\n                else if (length(var) > 1 ) {\n                        # handles lists from checkboxGroup and multiple Select\n                        data[[x]] <- paste(var,collapse = \", \")\n                } else {\n                        # all other data types\n                        data[[x]] <- var\n                }\n        }\n        \n        data$submit_time <- Sys.time()\n        colnames(data) = c(\"name\",\"pronoun\",\"linkedin\", \"email\",\"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\",\"submit.timestamp\")\n        \n        # append new data\n        df = bind_rows(df, data)\n        \n        # write into tempfile\n        write_csv(df, path=tmpDir, na=\" \")\n        \n        # update mentors spreadsheet\n        mentors <- mentors %>% \n          drive_update(\n                tmpDir,\n                name=\"mentors\"\n        )\n        # drive_rm(mentors)\n}\n```\n:::\n\n\n#### loadData\n\n`googlesheets` have a function `read_sheet` to read googlesheets directly to data.frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloadData <- function() {\n        # read spreadsheet\n        sheet_id=drive_find(pattern = \"mentors\", type = \"spreadsheet\")$id\n        data=read_sheet(sheet_id)\n        # data\n        names = tibble(\n                name=mapply(\n                        function(url,text){\n                                if(url!=\" \"){\n                                        paste0(\"<a href='\",url,\"'>\",text,\"</a>\")\n                                }else if (url!=\" \"){\n                                        paste0(\"<a href='\",url,\"'>\",text,\"</a>\")\n                                }\n                        }, \n                        data$linkedin, data$name\n                        )\n        )\n        links = tibble(\n                links=mapply(\n                        function(email, linkedin,text){\n                                if(email!=\" \" & linkedin==\" \"){\n                                        paste0(\"<a href=mailto:\",email,\">\",\"Email\",\"</a>\")\n                                } else if (linkedin!=\" \" & email==\" \"){\n                                        paste0(\"<a href='\",linkedin,\"'>\",\"LinkedIn\",\"</a>\")\n                                } else {\n                                        paste(\n                                                paste0(\"<a href=mailto:\",email,\">\",\"Email\",\"</a>\"),\n                                                paste0(\"<a href='\",linkedin,\"'>\",\"LinkedIn\",\"</a>\")\n                                        )\n                                }\n                        }, \n                        data$email, data$linkedin, data$name\n                )\n        )\n        out = bind_cols(\n                names %>% as.data.frame(),\n                data[,c(\"pronoun\",\"signUp.type\",\"expertises\",\"primary.employment\",\"preferred.mentor.method\")],\n                links %>% as.data.frame()\n        )\n        out\n}\n```\n:::\n\n\n## Final remarks\n\nIn this post, we introduce three ways to load and save data to external storage clound. AWS s3 is most secure and fleasible among three. It can store and load unstructure data easily, thus it does not require much memory cache from host server. But it is not free when data is very big. Dropbox can save both tubular and unstructural data, but retrieve unstructure requires downloading file to cache. Googlesheets can only read/save tubular data. Both dropbox and googlesheets have some secure concerns, but you can create a free account and designate that account for app development/test only to reduce concerns for security. The complete codes for finished app can be accessed from my [github](https://github.com/sckinta/example_code/tree/master/shinyapp_examples).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
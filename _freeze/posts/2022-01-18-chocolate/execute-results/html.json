{
  "hash": "eb9ab2751e0b66624fda69850be4c7cd",
  "result": {
    "markdown": "---\ntitle: \"TidyTuesday: predict chocolate rating with xgboost\"\ndate: \"2022-01-23\"\ncategories: [\"R\", \"tidyTuesday\", \"tidymodels\"]\nimage: 'https://www.history.com/.image/ar_4:3%2Cc_fill%2Ccs_srgb%2Cfl_progressive%2Cq_auto:good%2Cw_1200/MTc3OTk5Njc1MTU5MjI1OTY1/valentines-day-chocolate-gettyimages-923430892.jpg'\nparams:\n  data_date: '2022-01-18'\n---\n\n\n\n\nLoad required libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\n# library(lubridate)\nlibrary(vip)\n```\n:::\n\n\nData README is available at [here](%60r%20readme_link%60).\n\n## Clean Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_raw <- tuesdata$chocolate\nchocolate_raw <- chocolate_raw %>% \n    mutate(cocoa_percent = parse_number(cocoa_percent)) %>% \n    separate(ingredients, c(\"ingredient_num\",\"ingredients\"), sep=\"-\") %>% \n    mutate(\n        ingredient_num=parse_number(ingredient_num),\n        ingredients=str_trim(ingredients)\n    ) %>% \n    mutate(ingredients = map(ingredients, ~str_split(.x, \",\")[[1]])) %>% \n    mutate(most_memorable_characteristics=map(most_memorable_characteristics, ~str_split(.x,\",\")[[1]])) %>% \n    mutate(most_memorable_characteristics=map(most_memorable_characteristics, ~str_trim(.x))) %>% \n    # select(cocoa_percent, ingredient_num, ingredients, most_memorable_characteristics) %>%\n    I()\n```\n:::\n\n\n-   Convert gredients to boolean columns\n\nusing `unnest` to spread out the list column `ingredients`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngredients <- chocolate_raw %>% \n    mutate(line_n = row_number()) %>% \n    select(line_n, ingredients) %>% \n    unnest(cols=c(ingredients)) %>% \n    mutate(tmp=1) %>% \n    pivot_wider(names_from=ingredients, values_from=tmp) %>% \n    select(-\"NA\") %>% \n    janitor::clean_names() %>% \n    mutate_at(vars(-line_n), ~ifelse(is.na(.x),0,.x)) %>% \n    I()\n```\n:::\n\n\n-   Convert most_memorable_characteristics to boolean columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmost_memorable_characteristics <- chocolate_raw %>% \n    mutate(line_n = row_number()) %>% \n    select(line_n, most_memorable_characteristics) %>% \n    unnest(cols=c(most_memorable_characteristics)) %>% \n    mutate(tmp=1) %>% \n    # distinct(most_memorable_characteristics) %>% \n    # pivot_wider(names_from=most_memorable_characteristics, values_from=tmp) %>% \n    I()\n```\n:::\n\n\nThere are 972 most_memorable_characteristics in total\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmost_memorable_characteristics %>% \n    # mutate(most_memorable_characteristics = fct_lump_min(most_memorable_characteristics, min=100)) %>% \n    group_by(most_memorable_characteristics) %>% \n    count(sort=T) %>% \n    head(20) %>% \n    ggplot(aes(x=n, y=reorder(most_memorable_characteristics,n))) +\n    geom_col() +\n    geom_text(aes(label=n), color=\"white\", hjust=1) +\n    theme_bw() +\n    labs(x=\"# of chocolates\", y=\"most memorable characteristics\")\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nPick top 12 most_memorable_characteristics to convert to boolean column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmost_memorable_characteristics <- most_memorable_characteristics %>% \n    mutate(most_memorable_characteristics = fct_lump_min(most_memorable_characteristics, min=100)) %>% \n    distinct() %>% \n    pivot_wider(names_from=most_memorable_characteristics, values_from=tmp) %>% \n    mutate_at(vars(-line_n), ~ifelse(is.na(.x),0,.x))\n```\n:::\n\n\n-   create chocolate_clean data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean <-\n    chocolate_raw %>% \n    mutate(line_n=row_number()) %>% \n    select(-ingredients, -most_memorable_characteristics) %>% \n    left_join(gredients) %>%\n    left_join(most_memorable_characteristics) %>%\n    I()\n```\n:::\n\n\n## Explore Data\n\nSeveral features are explored in terms of their association with rating.\n\n-   `country_of_bean_origin`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean %>% \n    mutate(country_of_bean_origin = fct_lump(country_of_bean_origin, n=10)) %>% \n    ggplot(aes(x=rating, y=country_of_bean_origin)) +\n    geom_boxplot(aes(fill=country_of_bean_origin)) +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nBlend and non-blend on `country_of_bean_origin` shows big difference, thus we convert `country_of_bean_origin` to `country_of_bean_origin_blend`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean <- chocolate_clean %>% \n    mutate(country_of_bean_origin_blend = ifelse(country_of_bean_origin==\"Blend\", country_of_bean_origin, \"Non-blend\"))\n```\n:::\n\n\n-   `company_manufacturer` and `company_location`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean %>% \n    mutate(company_manufacturer = fct_lump(company_manufacturer, prop=0.01)) %>% \n    ggplot(aes(x=rating, y=reorder(company_manufacturer, rating, median))) +\n    geom_boxplot(aes(fill=company_manufacturer)) +\n    theme_bw() +\n    labs(y=\"company_manufacturer\")\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean %>% \n    mutate(company_location = fct_lump(company_location, n=5)) %>% \n    ggplot(aes(x=rating, y=reorder(company_location, rating))) +\n    geom_boxplot(aes(fill=company_location)) +\n    theme_bw() +\n    labs(y=\"company_location\")\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n-   `cocoa_percent`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean %>% \n    ggplot(aes(x=cocoa_percent, y=rating)) +\n    geom_point(aes(color=as.factor(cocoa))) +\n    theme_bw() +\n    theme(legend.position = \"bottom\") +\n    labs(color=\"cocoa as most_memorable_characteristics\")\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n`rating` is not as continuous as what i originally imagined. Thus, I convert `rating` to nominal variable `rating_bl` using 3 as threshold\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean <- chocolate_clean %>% \n    mutate(rating_bl = ifelse(rating >= 3, \">=3\", \"< 3\"))\n\nchocolate_clean %>% \n    group_by(rating_bl) %>% \n    count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n# Groups:   rating_bl [2]\n  rating_bl     n\n  <chr>     <int>\n1 < 3         566\n2 >=3        1964\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean %>% \n    ggplot(aes(x=cocoa_percent, y=rating_bl)) +\n    geom_boxplot(aes(fill=as.factor(cocoa))) +\n    theme_bw() +\n    labs(y=\"rating\", fill=\"cocoa as most_memorable_characteristics\") +\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n-   most_memorable_characteristics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean <- chocolate_clean %>% \n    mutate_at(vars(Other:creamy), as.factor)\n```\n:::\n\n\nmost_memorable_characteristics like `cocoa` and `creamy` positive effect rating, while `fatty`, `earthy`, `sandy`, `sour` and `sweet` negatively effect rating.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean %>% \n    select(rating, fatty:creamy) %>% \n    pivot_longer(!rating, names_to=\"most_memorable_characteristics\", values_to=\"yes\") %>%\n    ggplot(aes(y=reorder(most_memorable_characteristics, rating, FUN=median), x=rating)) +\n    geom_boxplot(aes(fill=yes)) +\n    theme_bw() +\n    theme(\n        legend.position = \"bottom\"\n    ) +\n    scale_fill_discrete(labels = c(\"0\"=\"No\", \"1\"=\"Yes\")) +\n    labs(y=\"most_memorable_characteristics\", fill=\"is most_memorable_characteristics?\") +\n    NULL\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n-   ingredients\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean <- \n    chocolate_clean %>% \n    dplyr::rename(igrdt_beans=b, igrdt_sugar=s, igrdt_cocoa=c, igrdt_lecithin=l, igrdt_vanilla=v, igrdt_salt=sa, igrdt_sweeter=s_2) %>% \n    mutate_at(vars(contains(\"igrdt_\")), as.factor)\n```\n:::\n\n\ningredient number `ingredient_num` between 2-3 are associated with higher rating.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean %>% \n    mutate(ingredient_num=as.factor(ingredient_num)) %>% \n    filter(!is.na(ingredient_num)) %>% \n    ggplot(aes(x = ingredient_num, y=rating)) +\n    geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\ningrediants like `beans` and `sugar` positively effect rating, while `vanilla`, `sweeter` and `salt` negatively effect rating.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_clean %>% \n    select(rating, contains(\"igrdt_\")) %>% \n    pivot_longer(!rating, names_to=\"ingredients\", values_to=\"yes\") %>%\n    mutate(ingredients = gsub(\"igrdt_\",\"\",ingredients)) %>% \n    ggplot(aes(y=reorder(ingredients, rating, FUN=median), x=rating)) +\n    geom_boxplot(aes(fill=yes)) +\n    theme_bw() +\n    theme(\n        legend.position = \"bottom\"\n    ) +\n    scale_fill_discrete(labels = c(\"0\"=\"No\", \"1\"=\"Yes\")) +\n    labs(y=\"ingredients\", fill=\"contain the ingredient?\") +\n    NULL\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## ML\n\nBased on the exploratory analysis, to study the effect on overall rating of chocolates, the following features are selected for building ML models. Plus, using nominal feature `rating_bl` instead of numeric feature `rating` as outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_df <- chocolate_clean %>% \n    select(rating_bl, company_manufacturer, country_of_bean_origin_blend, cocoa_percent, ingredient_num, contains('igrdt_'), cocoa, creamy, fatty, earthy, sandy, sour, sweet) %>% \n    select(-igrdt_cocoa, -igrdt_lecithin) %>% \n    na.omit()\n```\n:::\n\n\n### split samples\n\n-   `initial_split`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nchocolate_split <- initial_split(chocolate_df, strata = rating_bl)\nchocolate_train <- training(chocolate_split)\nchocolate_testing <- testing(chocolate_split)\n```\n:::\n\n\n-   resample\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nfolds <- vfold_cv(chocolate_train, v = 10)\nfolds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   <list>             <chr> \n 1 <split [1647/184]> Fold01\n 2 <split [1648/183]> Fold02\n 3 <split [1648/183]> Fold03\n 4 <split [1648/183]> Fold04\n 5 <split [1648/183]> Fold05\n 6 <split [1648/183]> Fold06\n 7 <split [1648/183]> Fold07\n 8 <split [1648/183]> Fold08\n 9 <split [1648/183]> Fold09\n10 <split [1648/183]> Fold10\n```\n:::\n:::\n\n\n### recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_rec <- \n    recipe(rating_bl ~ ., data = chocolate_train) %>% \n    step_other(company_manufacturer, threshold=0.01, other=\"otherCompany\") %>% \n    # step_mutate_at(c(\"company_manufacturer\",\"country_of_bean_origin_blend\", \"rating_bl\"), fn = ~as.factor(.x)) %>% \n    step_dummy(all_nominal_predictors()) %>% \n    step_zv(all_predictors())\n\nchocolate_rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         16\n\nOperations:\n\nCollapsing factor levels for company_manufacturer\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()\n```\n:::\n:::\n\n\ncheck preprocessed data.frame\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchocolate_rec %>% \n    prep(new_data = NULL) %>% \n    juice()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,831 × 20\n   cocoa_percent ingre…¹ ratin…² compa…³ compa…⁴ compa…⁵ compa…⁶ compa…⁷ count…⁸\n           <dbl>   <dbl> <fct>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1            70       4 < 3           0       0       0       0       0       1\n 2            70       4 < 3           0       0       0       0       0       1\n 3            60       3 < 3           0       0       0       0       1       1\n 4            70       2 < 3           0       0       0       0       1       1\n 5            70       2 < 3           0       0       0       0       1       1\n 6            75       4 < 3           0       0       0       0       1       1\n 7            75       4 < 3           0       0       0       0       1       1\n 8            75       5 < 3           0       0       0       0       1       1\n 9            75       5 < 3           0       0       0       0       1       1\n10            65       6 < 3           0       0       0       0       1       1\n# … with 1,821 more rows, 11 more variables: igrdt_sugar_X1 <dbl>,\n#   igrdt_vanilla_X1 <dbl>, igrdt_salt_X1 <dbl>, igrdt_sweeter_X1 <dbl>,\n#   cocoa_X1 <dbl>, creamy_X1 <dbl>, fatty_X1 <dbl>, earthy_X1 <dbl>,\n#   sandy_X1 <dbl>, sour_X1 <dbl>, sweet_X1 <dbl>, and abbreviated variable\n#   names ¹​ingredient_num, ²​rating_bl, ³​company_manufacturer_Arete,\n#   ⁴​company_manufacturer_Bonnat, ⁵​company_manufacturer_Fresco,\n#   ⁶​company_manufacturer_Soma, ⁷​company_manufacturer_otherCompany, …\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n```\n:::\n:::\n\n\n### grid tune xgboost\n\n-   create model `boost_tree`\n\nDetails about `boost_tree` can be found https://parsnip.tidymodels.org/reference/details_boost_tree_xgboost.html\n\n*require library `xgboost` installed.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxg_spec <- \n    boost_tree(\n        mtry=tune(), # the number (or proportion) of predictors that will be randomly sampled\n        min_n=tune() # minimum number of data points in a node\n    ) %>% \n    set_engine(\"xgboost\") %>% # importance=\"permutation\"\n    set_mode('classification')\n```\n:::\n\n\n-   define grid\n\n`grid_max_entropy`, `grid_regular`, `grid_random` can be used for quickly specify levels for tuned hyperparameters.\n\nbe aware that `mtry` usually requires `range` parameters, it usually contains the `sqrt(predictor_num)`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxg_grid <- grid_regular(\n    mtry(range = c(3, 10)),\n    min_n(),\n    levels = 5 # each tune how many levels\n)\n\nxg_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 25 × 2\n    mtry min_n\n   <int> <int>\n 1     3     2\n 2     4     2\n 3     6     2\n 4     8     2\n 5    10     2\n 6     3    11\n 7     4    11\n 8     6    11\n 9     8    11\n10    10    11\n# … with 15 more rows\n# ℹ Use `print(n = ...)` to see more rows\n```\n:::\n:::\n\n\n-   create workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxg_wf <- workflow() %>% \n    add_model(xg_spec) %>% \n    add_recipe(chocolate_rec)\n\nxg_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_other()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: xgboost \n```\n:::\n:::\n\n\n-   tune model to get result\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(\n    xg_res <- \n        xg_wf %>% \n        tune_grid(\n            resamples = folds,\n            grid = xg_grid\n            )\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 30.965   0.215  31.417 \n```\n:::\n:::\n\n\n-   evaluate models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxg_res %>% \n    collect_metrics() %>% \n    ggplot(aes(x = min_n, y=mean, color=as.factor(mtry))) +\n    facet_wrap(~.metric, scales=\"free\") +\n    geom_point() +\n    geom_line(aes(group=as.factor(mtry))) +\n    theme_bw() +\n    labs(y=\"metrics estimate\", x='minimum number of data points in a node (min_n)', color='the number of predictors that will be randomly sampled (mtry)') +\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n-   select hyperparameters and finalize wf\n\n`show_best(metric = )` allows to see the top 5 from `xg_res %>% collect_metrics()`\n\n`select_best`, `select_by_pct_loss`, `select_by_one_std_err` select hyperparameters and corresponding `.config` to a tibble.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxg_tune_hy <- xg_res %>% \n    select_best(metric = \"accuracy\")\n\nxg_tune_hy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1    10    11 Preprocessor1_Model10\n```\n:::\n:::\n\n\nfinalize model using selected hyperparameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- \n  xg_wf %>% \n  finalize_workflow(xg_tune_hy)\n\nfinal_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_other()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 10\n  min_n = 11\n\nComputational engine: xgboost \n```\n:::\n:::\n\n\n### `last_fit` model\n\n-   use `last_fit(split)`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- final_wf %>% \n    last_fit(chocolate_split)\n\nfinal_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  <list>             <chr>            <list>   <list>   <list>       <list>    \n1 <split [1831/612]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n:::\n\n\n-   `collect_metrics` for overall data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>% \n    collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.786 Preprocessor1_Model1\n2 roc_auc  binary         0.668 Preprocessor1_Model1\n```\n:::\n:::\n\n\nmetrics are comparable to training data, so not overfiting.\n\n-   `collect_predictions` for test data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>% \n    collect_predictions()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 612 × 7\n   id               `.pred_< 3` `.pred_>=3`  .row .pred_class rating_bl .config \n   <chr>                  <dbl>       <dbl> <int> <fct>       <fct>     <chr>   \n 1 train/test split      0.141        0.859     3 >=3         >=3       Preproc…\n 2 train/test split      0.125        0.875    10 >=3         < 3       Preproc…\n 3 train/test split      0.0668       0.933    11 >=3         < 3       Preproc…\n 4 train/test split      0.156        0.844    17 >=3         >=3       Preproc…\n 5 train/test split      0.0668       0.933    24 >=3         >=3       Preproc…\n 6 train/test split      0.0668       0.933    25 >=3         >=3       Preproc…\n 7 train/test split      0.0711       0.929    32 >=3         >=3       Preproc…\n 8 train/test split      0.236        0.764    42 >=3         < 3       Preproc…\n 9 train/test split      0.491        0.509    46 >=3         < 3       Preproc…\n10 train/test split      0.385        0.615    55 >=3         < 3       Preproc…\n# … with 602 more rows\n# ℹ Use `print(n = ...)` to see more rows\n```\n:::\n:::\n\n\n-   `roc_auc` and `roc_curve` on test data\n\ncalculate `roc_auc` manually on test data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>%\n  collect_predictions() %>% \n  roc_auc(truth=rating_bl, `.pred_< 3`)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.668\n```\n:::\n:::\n\n\nplot `roc_curve`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>%\n  collect_predictions() %>% \n  roc_curve(truth=rating_bl, `.pred_< 3`) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n-   `extract_workflow()` to save `final_trained_wf`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_trained_wf <- final_fit %>% \n    extract_workflow()\n\nfinal_trained_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_other()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 21.7 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 0.526315789473684, \n    min_child_weight = 11L, subsample = 1, objective = \"binary:logistic\"), \n    data = x$data, nrounds = 15, watchlist = x$watchlist, verbose = 0, \n    nthread = 1)\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"0.526315789473684\", min_child_weight = \"11\", subsample = \"1\", objective = \"binary:logistic\", nthread = \"1\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 19 \nniter: 15\nnfeatures : 19 \nevaluation_log:\n    iter training_logloss\n       1        0.6020652\n       2        0.5525599\n---                      \n      14        0.4693209\n      15        0.4688216\n```\n:::\n:::\n\n\n-   `extract_*` information from `final_trained_wf`\n\n    -   `extract_fit_engine()` is engine-specific model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_trained_wf %>%\n  extract_fit_engine()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n##### xgb.Booster\nraw: 21.7 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 0.526315789473684, \n    min_child_weight = 11L, subsample = 1, objective = \"binary:logistic\"), \n    data = x$data, nrounds = 15, watchlist = x$watchlist, verbose = 0, \n    nthread = 1)\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"0.526315789473684\", min_child_weight = \"11\", subsample = \"1\", objective = \"binary:logistic\", nthread = \"1\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 19 \nniter: 15\nnfeatures : 19 \nevaluation_log:\n    iter training_logloss\n       1        0.6020652\n       2        0.5525599\n---                      \n      14        0.4693209\n      15        0.4688216\n```\n:::\n:::\n\n\n-   `extract_fit_parsnip()` is parsnip model object\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_trained_wf %>%\n  extract_fit_parsnip()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n##### xgb.Booster\nraw: 21.7 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 0.526315789473684, \n    min_child_weight = 11L, subsample = 1, objective = \"binary:logistic\"), \n    data = x$data, nrounds = 15, watchlist = x$watchlist, verbose = 0, \n    nthread = 1)\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"0.526315789473684\", min_child_weight = \"11\", subsample = \"1\", objective = \"binary:logistic\", nthread = \"1\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 19 \nniter: 15\nnfeatures : 19 \nevaluation_log:\n    iter training_logloss\n       1        0.6020652\n       2        0.5525599\n---                      \n      14        0.4693209\n      15        0.4688216\n```\n:::\n:::\n\n\n-   `extract_recipe` or `extract_preprocessing` to get recipe/preprocessing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_trained_wf %>% extract_preprocessor()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         16\n\nOperations:\n\nCollapsing factor levels for company_manufacturer\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()\n```\n:::\n:::\n\n\n### feature importance\n\n-   `vip()` plot top 10\n-   `vi_model()` return tibble\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_trained_wf %>%\n  extract_fit_parsnip() %>% \n  vip()\n```\n\n::: {.cell-output-display}\n![](2022-01-18-chocolate_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n## Final notes\n\n-   I convert numeric `rating` to categorical rating using threshold because, based on the exploratory analysis, the `rating` values are not continuous.\n-   The `boost_tree` did not produce good estimate for the data.\n    -   Other models, like `rand_forest()`, `logistic_reg` and `svm_linear` are worth to try.\n    -   Tuning other hyperparameters `tree_depth`, `learning_rate` and `trees` are worth to try. *I don't know which tune-able hyperparameter corresponds to regularization `gamma`*.\n-   Julia Silge posted a [screencast](https://www.youtube.com/watch?v=w-lF65hKtrQ) and [blog](https://juliasilge.com/blog/chocolate-ratings/) of using `rand_forest()` and `svm_linear` training rating as linear model on the same dataset.\n",
    "supporting": [
      "2022-01-18-chocolate_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
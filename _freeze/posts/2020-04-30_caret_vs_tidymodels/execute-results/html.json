{
  "hash": "e0b2ee83e4fa903c187ccd1e0c766987",
  "result": {
    "markdown": "---\ntitle: 'ML pipeline with tidymodels vs. caret'\ndate: \"2020-04-30\"\ncategories: [\"R\", \"ML\", \"tidymodels\"]\n---\n\n\n\n\nAs a DS beginner, I first came across ML in R by studying the book [Hands-On Machine Learning with R](https://bradleyboehmke.github.io/HOML/). The book mainly focuses on the package `caret` with general introductions to packages like `recipe` and `h2o`. Most examples use the workflow in which feature engineering is performed by `recipe` and the modeling/learning part is done using `caret`.\n\nIt was a great pleasure to take the [tidymodels](https://www.tidymodels.org/) workshop hosted by [Dr. Alison Hill](https://twitter.com/apreshill) last week. `tidymodels` was recently launched as a collection of packages for ML using tidyverse principles. It is built on `recipes` for feature engineering and `parsnip` as the major modeling package, and links ML steps together with `workflow`.\n\nIn this post, I am going to present the general ML frameworks using `caret` and `tidymodels`, independently. The data used as an example is \"Watson churn data\" from [modeldata](https://modeldata.tidymodels.org/reference/index.html)\n\n## 0. required libraries and data\n\nSince tidymodels is a collection of packages like `tidyverse`, we can just use `library(tidymodels)` to load all the required libraries for the tidymodels pipeline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n:::\n\n\nFor the `caret` pipeline, additional helper pacakges, like `recipes` and `rsample`, were needed to process the data. Most of those packages are already collected in the `tidymodels` pipeline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(vip)\n```\n:::\n\n\nFor the data, I used \"Watson churn data\" from modeldata which is also a part of `tidymodels`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(modeldata)\ndata(wa_churn)\n# quick view and summarize data\nglimpse(wa_churn)\n# visdat::vis_miss(wa_churn)\nHmisc::describe(wa_churn)\n\n# relevel factors\nwa_churn = wa_churn %>% \n        mutate(churn=relevel(churn, ref=\"No\")) %>% \n        mutate(multiple_lines=relevel(multiple_lines, ref=\"No phone service\")) %>% \n        mutate(internet_service=relevel(internet_service, ref=\"No\")) %>% \n        mutate(online_security=relevel(online_security, ref=\"No internet service\")) %>% \n        mutate(online_backup=relevel(online_backup, ref=\"No internet service\")) %>% \n        mutate(device_protection=relevel(device_protection, ref=\"No internet service\")) %>% \n        mutate(tech_support=relevel(tech_support, ref=\"No internet service\")) %>% \n        mutate(streaming_tv=relevel(streaming_tv, ref=\"No internet service\")) %>% \n        mutate(streaming_movies=relevel(streaming_movies, ref=\"No internet service\")) %>% \n        mutate(contract=relevel(contract, ref=\"Month-to-month\"))\n\n# to simplify the case here, we are going to remove missing variable\nwa_churn = wa_churn %>% \n        na.omit      \n```\n:::\n\n\n## 1. data split\n\nBoth frameworks use `rsample::initial_split` to split the data into training and testing data. Here, we choose the standard 7:3 split between training and testing, with stratification on the target variable \"churn\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split\nset.seed(123)\ndata_splits = initial_split(wa_churn, strata=\"churn\", prob=0.7)\ndata_train=training(data_splits)\ndata_test=testing(data_splits)\n```\n:::\n\n\nTo stratify on the numeric variables, we can add the `breaks` parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_split(wa_churn, strata=\"tenure\", prob=0.7, breaks=4)\n```\n:::\n\n\n## 2. feature engineer\n\nGeneral [feature engineering](http://www.feat.engineering/) steps include\n\n-   removing variables with zero variance or near zero variance: `step_zv`, `step_nzv`\n-   lumping nominal variables: `step_other`\n-   normalizing (scale + center) numeric variables (specific for regression-based models): `step_scale`, `step_center`, `step_normalize`\n-   encoding nominal variables to dummy features: `step_novel` + `step_dummy`, `step_integer`\n-   value transformation to fit normal distribution: `step_log`, `step_YeoJohnson`, `step_BoxCox`\n-   feature dimension reduction: `step_pca`\n-   dealing with missing values with imputation: `step_medianimpute`, `step_knnimpute`, `step_bagimpute`\n\nFeature engineering is done by `recipes` in both `tidymodels` and `caret`. The functions in `recipes` starts with `step_*` and create a blueprint for feature engineering. The complete list of step is at https://tidymodels.github.io/recipes/reference/index.html\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- recipe(churn ~ ., data = wa_churn) %>%\n        step_zv(all_predictors()) %>% \n        step_nzv(all_predictors())  %>%\n        step_novel(all_nominal(), -all_outcomes()) %>% \n        step_dummy(all_nominal(), -all_outcomes())\n```\n:::\n\n\nThe blueprint will not change the data until it is `fit` in the modeling step. We can use `prep` and `bake` to see \"transformed\" data in data.frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec %>% \n        prep(wa_churn) %>% \n        bake(wa_churn)\n```\n:::\n\n\nOne reason to use `recipe` is to avoid [data leakage](https://www.quora.com/Whats-data-leakage-in-data-science). Data leakage is when information from outside the training data set is used to create the model.\n\n## 3. resample\n\nResampling methods split the training data into additional sets. It will generate train set and validation set. Typical resampling method include cross-validation (cv), repeated cross-validation (repeated cv), leave-one-out and bootstrapping (with replacement).\n\nWe can use `rsample::vfold_cv` for both caret and tidymodels pipeline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 10 fold cross validation stratified on target variable churn\ncv_folds = rsample::vfold_cv(data=data_train, v=10, strata=churn)\n```\n:::\n\n\nHowever to make above `cv_folds` compatible with `caret`, we need to used `rsample2caret` to convert a `trainControl` list\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_folds_cr = rsample2caret(cv_folds)\ncv_folds_trCtrl = trainControl(\n        method = \"cv\",\n        verboseIter = FALSE,\n        classProbs = TRUE,\n        summaryFunction = twoClassSummary,\n        returnResamp = \"final\",\n        savePredictions = \"final\",\n        index = cv_folds_cr$index,\n        indexOut = cv_folds_cr$indexOut\n  )\n```\n:::\n\n\nOr we can simply use caret function `trainControl` function to generate split. However, no stratify option is available here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_folds_trCtrl = trainControl(method = \"cv\", number=10)\n```\n:::\n\n\n## 4. hyperparameters grid\n\nA hyperparameter is a parameter whose value is set before the learning process begins. It is distinguished from other parameters by the fact that it is not used for fitting the machine to the training set. For different models, there are a different number of hyperparameters you can tune. Here I choose to use random forest to model the data. The hyperparameters for random forest from `ranger` include\n\n1.  the number of trees -- `num.trees` or `trees`\n2.  depth of tree -- `max.depth`\n3.  number of features to consider at every split -- `mtry`\n4.  minimum number of samples required to split a node -- `min.node.size` or `min_n`\n5.  whether using boostrapping to select samples for training -- `replace`.\n6.  fraction of observation to sample -- `sample.fraction`. Specifying `sample.fraction` requires `replace` being set as TRUE\n\nA rule of thumb to start is\n\n-   `num.trees` start with 10x p (p means number of features).\n-   `max.depth`\n-   `mtry`: sqrt(p) for classification and p/3 for regression\n-   `min.node.size` default values of 1 for classification and 5 for regression\n-   `replace` and `sample.fraction`: Default is 1 for sampling with replacement and 0.632 for sampling without replacement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhyp_grid = expand.grid(\n        trees = c(500,1000),\n        mtry=c(5,15),\n        min_n=c(10,20)\n)\n```\n:::\n\n\nThe hyperparameters can be checked by function `args(rand_forest)`\n\n`rf` method (from RandomForest) for `caret` has only one hyperparameter (mtry) by default.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhyp_grid_cr = expand.grid(\n        mtry=5:10\n)\n```\n:::\n\n\n## 5. fit model\n\nHere is the step where tidymodel and caret start to diverge in syntax. Typically, tidymodel builds a model using `workflow` pipe which specifies formular/recipe and model, while caret uses `train` to fit model.\n\n**tidymodel**\n\ndefault version of model fit `fit_resamples`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# without grid_tune -> fit_resamples() at train\nrf_tm <- rand_forest() %>% \n        set_engine(\"ranger\", importance=\"permutation\") %>% \n        set_mode(\"classification\")\n\nrf_tm_wf <- workflow() %>% \n        add_model(rf_tm) %>% \n        add_recipe(rec)\n\nset.seed(123)\ndefault_tm_fit= rf_tm_wf %>% \n        fit_resamples(\n                resamples = cv_folds,\n                control = control_resamples(save_pred = TRUE)\n                )\n```\n:::\n\n\ngrid version of model fit `grid_tune`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# with grid_tune -> set tune() at model, use tune_grid() at train\nrf_tm <- rand_forest(\n                mtry=tune(), \n                trees=tune(), \n                min_n=tune()\n        ) %>% \n        set_engine(\"ranger\", importance=\"impurity\") %>% \n        set_mode(\"classification\")\n\nrf_tm_wf <- workflow() %>% \n        add_model(rf_tm) %>% \n        add_recipe(rec)\n\nset.seed(123)\ngrid_tm_fit = rf_tm_wf %>% \n        tune_grid(resamples = cv_folds,\n            grid = hyp_grid,\n                control = control_grid(save_pred = TRUE)\n            )\n```\n:::\n\n\nNotes: 1. `control` specification will be `control_grid()` in `grid_tune()` 2. `grid` parameter here can also be a integer which test for top N parameters.\n\nFollow the thread https://github.com/tidymodels/parsnip/issues/235 to find how to print out default hyperparameters.\n\n**caret**\n\ndefault version of model fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# without hyp grid\nset.seed(123)\ndefault_cr_fit=train(\n        rec,\n        data = data_train,\n        method = \"rf\",\n        trControl = cv_folds_trCtrl,\n        metric = \"ROC\"\n)\n```\n:::\n\n\ngrid version of model fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# with grid --- tuneGrid\nset.seed(123)\ngrid_cr_fit=train(\n        rec,\n        data = data_train,\n        method = \"rf\",\n        trControl = cv_folds_trCtrl,\n        tuneGrid = hyp_grid_cr,\n        metric = \"ROC\"\n)\n```\n:::\n\n\n## 6. collect metrics\n\nMetrics are used to determine how good the model fit. For classification problem, accuracy and ROC/AUC are commonly used. For regression problem, RSEM is the most commonly used approach.\n\nWe used `collect_metrics` in `tidymodels`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for default model\ndefault_tm_fit %>% collect_metrics()\n\n# for grid tune model\ngrid_tm_fit %>% collect_metrics()\n```\n:::\n\n\nlist `results` stores metrics for `caret`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_cr_fit$results\n```\n:::\n\n\nFrom the results, we can tell that `train` fit 3 hyperparameters by default.\n\n## 7. collect prediction for training data\n\nBesides model metrics, we also care about what predicted value of target variable is in training data.\n\n**tidymodels**\n\nTo see predicted target value for data_train, we can use `collect_predictions`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_tm_fit %>% \n        collect_predictions()\n\n# plot auc\nautoplot(\n        roc_curve(\n                default_tm_fit %>% collect_predictions(), churn, .pred_Yes\n        )\n)\n```\n:::\n\n\n*Notes: collect_predictions() only works when specifying `save_pred = TRUE` in control.*\n\n**caret**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_cr_fit$pred %>% tbl_df\n\n# plot auc\nautoplot(\n        roc_curve(\n                default_cr_fit$pred %>% tbl_df, \n                obs, Yes\n        )\n)\n```\n:::\n\n\n*For both caret and tidymodels, it is possible that each row of the original data point might be represented multiple times per tuning paramete if boostrap or repeated cv is used*\n\n## 8. collect prediction for testing data\n\nFor default fit, only one set of hyperparameters is specified, thus we can just apply the fitted model to `data_test`. However, for grid fit, we end up with multiple sets of hyperparameters. Thus, before fitting the model, we need to pick the best set of hyperparameters based on metrics on training data (which is summarized using specified rsample method), then apply the best model to test_data\n\n**tidymodels**\n\n`last_fit` is a function that is applied to workflow and fits to test data. By default, it generates predictions that can be reported by `collect_prediction` (no need to specify `control` in the fit). We can also use `collect_metrics` to check the metrics in testing data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# default\ndefault_last_fit = rf_tm_wf %>% \n        last_fit(split = data_splits)\n\ndefault_last_fit %>% \n        collect_metrics()\n\ndefault_last_fit %>% \n        collect_predictions()\n```\n:::\n\n\nTo select best set of hyperparameters from `grid_tune`, we use `select_best` by specifying which metrics to use. Then we apply this set of hyperparameters to original workflow by `finalize_workflow`. Finally, like default, apply `last_fit` to the best workflow and get predictions and metrics for the testing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# grid tune\nbest_hyp <- grid_tm_fit %>% \n        select_best(metric = \"roc_auc\")\n\nbest_wf <- rf_tm_wf %>%\n        finalize_workflow(best_hyp)\n\ngrid_last_fit <- best_wf %>% \n  last_fit(split = data_splits)\n\ngrid_last_fit %>% \n        collect_metrics()\n\ngrid_last_fit %>% \n        collect_predictions()\n```\n:::\n\n\n**caret**\n\nThe `predict` function can be directly applied to fitted model to test data. For grid fit, it will automatically detect the best hyperparameters (here mtry=5) and apply it to the testing data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# default\ntest_prediction <- predict(\n    default_cr_fit,\n    newdata = data_test,\n    type = \"prob\") %>%\n  as_tibble() %>%\n        transmute(estimate=Yes) %>%  # for binary result we can randomly pick one, it will be same roc_auc\n  add_column(churn = data_test$churn) \n\n## auc\nroc_auc(test_prediction, churn, estimate)$.estimate\n## accuracy\ntest_prediction %>% \n        mutate(.pred=ifelse(estimate > 0.5, \"Yes\",\"No\")) %>% \n        summarise(accuracy=mean(.pred==churn)) %>% \n        pull(accuracy)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# grid\n# best hyp\ngrid_cr_fit$results %>% \n        slice(which.max(ROC))\n\ntest_prediction2 <- predict(\n    grid_cr_fit,\n    newdata = data_test,\n    type = \"prob\") %>%\n  as_tibble() %>%\n        transmute(estimate=Yes) %>%  # for binary result we can randomly pick one, it will be same roc_auc\n  add_column(churn = data_test$churn) \n\n## auc\nroc_auc(test_prediction2, churn, estimate)$.estimate\n## accuracy\ntest_prediction2 %>% \n        mutate(.pred=ifelse(estimate > 0.5, \"Yes\",\"No\")) %>% \n        summarise(accuracy=mean(.pred==churn)) %>% \n        pull(accuracy)\n```\n:::\n\n\n## 9. importance of variables\n\nLastly, we can use fit result to find most important variables by `vip` package or caret function `varImp`. Be aware that, for `tidymodels` different `importance` specified in the model will result in different ranks\n\n**tidymodels**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_fit <- pull_workflow_fit(default_last_fit$.workflow[[1]])\nvip::vip(rf_fit)$data\nvip::vip(rf_fit, geom = \"point\")\n\n# model-specific variable importance scores are currently not available for objects of class \"_rangermodel_fit\"\n```\n:::\n\n\n**caret**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarImp(default_cr_fit, scale = TRUE)$importance %>% tbl_df\n```\n:::\n\n\n## Summary\n\nThe following table summarizes the ML workflow using caret versus tidymodels:\n\n\n\n\n\n## Thank you\n\nSpecial thanks to [Amy Goodwin Davies](https://www.linkedin.com/in/amygoodwindavies/) who helped editing and proof-reading this post!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
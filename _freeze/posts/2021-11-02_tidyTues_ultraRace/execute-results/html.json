{
  "hash": "2957332ce558a683846de96f70fa2e7e",
  "result": {
    "markdown": "---\ntitle: 'TidyTuesday: predict ultra race time'\ndate: \"2021-11-02\"\ncategories: [\"R\", \"tidyTuesday\", \"tidymodels\"]\nparams:\n  data_date: '2021-10-26'\n---\n\n\n\n\nLoad required libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(lubridate)\n```\n:::\n\n\n## data skim\n\nData README is available at [here](%60r%20readme_link%60).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_rankings <- tuesdata$ultra_rankings\nrace <- tuesdata$race\n\nultra_join <-\n    ultra_rankings %>% \n    left_join(race, by=\"race_year_id\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(ultra_join)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |ultra_join |\n|Number of rows           |137803     |\n|Number of columns        |20         |\n|_______________________  |           |\n|Column type frequency:   |           |\n|character                |9          |\n|Date                     |1          |\n|difftime                 |1          |\n|numeric                  |9          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|runner        |         0|          1.00|   3|  52|     0|    73629|          0|\n|time          |     17791|          0.87|   8|  11|     0|    72840|          0|\n|gender        |        30|          1.00|   1|   1|     0|        2|          0|\n|nationality   |         0|          1.00|   3|   3|     0|      133|          0|\n|event         |         0|          1.00|   4|  57|     0|      435|          0|\n|race          |         0|          1.00|   3|  63|     0|      371|          0|\n|city          |     15599|          0.89|   2|  30|     0|      308|          0|\n|country       |        77|          1.00|   4|  17|     0|       60|          0|\n|participation |         0|          1.00|   4|   5|     0|        4|          0|\n\n\n**Variable type: Date**\n\n|skim_variable | n_missing| complete_rate|min        |max        |median     | n_unique|\n|:-------------|---------:|-------------:|:----------|:----------|:----------|--------:|\n|date          |         0|             1|2012-01-14 |2021-09-03 |2017-10-13 |      711|\n\n\n**Variable type: difftime**\n\n|skim_variable | n_missing| complete_rate|min    |max        |median   | n_unique|\n|:-------------|---------:|-------------:|:------|:----------|:--------|--------:|\n|start_time    |         0|             1|0 secs |82800 secs |05:00:00 |       39|\n\n\n**Variable type: numeric**\n\n|skim_variable   | n_missing| complete_rate|      mean|       sd|     p0|     p25|      p50|    p75|     p100|hist  |\n|:---------------|---------:|-------------:|---------:|--------:|------:|-------:|--------:|------:|--------:|:-----|\n|race_year_id    |         0|          1.00|  26678.70| 20156.18|   2320|  8670.0|  21795.0|  40621|  72496.0|▇▃▃▂▂ |\n|rank            |     17791|          0.87|    253.56|   390.80|      1|    31.0|     87.0|    235|   1962.0|▇▁▁▁▁ |\n|age             |         0|          1.00|     46.25|    10.11|      0|    40.0|     46.0|     53|    133.0|▁▇▂▁▁ |\n|time_in_seconds |     17791|          0.87| 122358.26| 37234.38|   3600| 96566.0| 114167.0| 148020| 296806.0|▁▇▆▁▁ |\n|distance        |         0|          1.00|    154.08|    39.22|      0|   160.9|    162.6|    168|    179.1|▁▁▁▁▇ |\n|elevation_gain  |         0|          1.00|   6473.94|  3293.50|      0|  3910.0|   6640.0|   9618|  14430.0|▅▆▆▇▁ |\n|elevation_loss  |         0|          1.00|  -6512.20|  3305.73| -14440| -9618.0|  -6810.0|  -3950|      0.0|▁▇▆▅▅ |\n|aid_stations    |         0|          1.00|      9.58|     7.56|      0|     0.0|     12.0|     16|     56.0|▇▇▁▁▁ |\n|participants    |         0|          1.00|    510.75|   881.25|      0|     0.0|     65.0|    400|   2900.0|▇▁▁▁▁ |\n:::\n:::\n\n\n## EDA\n\nWe want to estimate the time (time_in_seconds) for runner to finish based on the features.\n\n### the effect of gender and age\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    filter(!is.na(gender)) %>% \n    filter(age > 10, age < 100) %>% \n    mutate(age_decade = 5* (age %/% 5)) %>% \n    select(time_in_seconds, gender, age, age_decade) %>% \n    group_by(age_decade, gender) %>% \n    summarise(\n        time_in_seconds_sd = sd(time_in_seconds),\n         time_in_seconds = mean(time_in_seconds)\n    ) %>% \n    ggplot(aes(x = age_decade, color=gender, group=gender)) +\n    geom_point(aes(y=time_in_seconds)) +\n    geom_line(aes(y=time_in_seconds)) +\n    geom_errorbar(aes(ymin=time_in_seconds - time_in_seconds_sd, ymax=time_in_seconds + time_in_seconds_sd), width=0.2, alpha=0.7) +\n    scale_color_viridis_d() +\n    labs(x = \"age\", y = \"time (second)\") +\n    scale_y_continuous(labels = scales::label_comma())\n```\n\n::: {.cell-output-display}\n![](2021-11-02_tidyTues_ultraRace_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n### the effect of nationality, age, gender\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_join %>% \n    mutate(nationality = fct_lump(nationality, prop=0.05)) %>% \n    count(nationality, sort=TRUE) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 2\n  nationality     n\n  <fct>       <int>\n1 Other       50563\n2 USA         47259\n3 FRA         28905\n4 GBR         11076\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    filter(!is.na(gender)) %>% \n    filter(age > 10, age < 100) %>% \n    mutate(nationality = fct_lump(nationality, prop=0.05)) %>% \n    ggplot(aes(x = age, fill=nationality), group=nationality) +\n    facet_wrap(vars(gender)) +\n    geom_bar(stat=\"density\", alpha=0.5)\n```\n\n::: {.cell-output-display}\n![](2021-11-02_tidyTues_ultraRace_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nnationality\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    filter(!is.na(gender)) %>% \n    filter(age > 10, age < 100) %>% \n    mutate(nationality = fct_lump(nationality, prop=0.05)) %>% \n    ggplot(aes(x=fct_reorder(nationality, time_in_seconds), y=time_in_seconds, fill=nationality)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +\n    labs(x=\"runner's nationality\", fill=NULL, y=\"time (second)\") +\n    scale_y_continuous(labels = scales::label_comma())\n```\n\n::: {.cell-output-display}\n![](2021-11-02_tidyTues_ultraRace_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### effect of distance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    filter(distance >= 150) %>% \n    ggplot(aes(x=distance, y=time_in_seconds)) +\n    geom_point(alpha=0.1, size=1) +\n    geom_smooth() +\n    labs(y=\"time (second)\") +\n    scale_y_continuous(labels = scales::label_comma())\n```\n\n::: {.cell-output-display}\n![](2021-11-02_tidyTues_ultraRace_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n### effect of elevation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    filter(distance >= 150) %>% \n    mutate(elevation = ifelse(\n        elevation_gain > abs(elevation_loss), elevation_gain,  abs(elevation_loss)\n        )) %>% \n    ggplot(aes(x=elevation , y=time_in_seconds)) +\n    geom_point(alpha=0.1, size=1) +\n    geom_smooth() +\n    labs(y=\"time (second)\") +\n    scale_y_continuous(labels = scales::label_comma())\n```\n\n::: {.cell-output-display}\n![](2021-11-02_tidyTues_ultraRace_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n### effect of date\n\nThe year of the race\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    mutate(\n        race_year=lubridate::year(date), \n        race_month=lubridate::month(date)\n    ) %>% \n    group_by(race_year) %>% \n    summarise(\n        time_in_seconds_sd=mean(time_in_seconds),\n        time_in_seconds=mean(time_in_seconds)\n    ) %>% \n    ungroup() %>% \n    ggplot(aes(x=race_year, y=time_in_seconds)) +\n    geom_point() +\n    geom_line() +\n    geom_errorbar(aes(ymin=time_in_seconds - time_in_seconds_sd, ymax=time_in_seconds + time_in_seconds_sd), alpha=0.5)\n```\n\n::: {.cell-output-display}\n![](2021-11-02_tidyTues_ultraRace_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThe month of race can be the proxy to estimate the season when race was hosted. However, here I did not take the geographic information (hemisphere) into consideration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_join %>% \n    filter(!is.na(time_in_seconds)) %>% \n    mutate(\n        race_year=lubridate::year(date), \n        race_month=lubridate::month(date)\n    ) %>% \n    group_by(race_month) %>% \n    summarise(\n        time_in_seconds_sd=mean(time_in_seconds),\n        time_in_seconds=mean(time_in_seconds)\n    ) %>% \n    ungroup() %>% \n    ggplot(aes(x=race_month, y=time_in_seconds)) +\n    geom_point() +\n    geom_line() +\n    geom_errorbar(aes(ymin=time_in_seconds - time_in_seconds_sd, ymax=time_in_seconds + time_in_seconds_sd), alpha=0.5)\n```\n\n::: {.cell-output-display}\n![](2021-11-02_tidyTues_ultraRace_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## learning models\n\nHere I will perform two distinct models -- linear regression and random forest to predict the race time using runner's gender, age, nationality, elevation and distance of race.\n\n### data budget\n\ninistal split to train and test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_df <- ultra_join %>% \n  filter(!is.na(time_in_seconds)) %>% \n  filter(!is.na(gender)) %>% \n  filter(age > 10, age < 100) %>% \n  filter(distance >= 150) %>% \n  mutate(elevation = ifelse(\n        elevation_gain > abs(elevation_loss), \n        elevation_gain,\n        abs(elevation_loss)\n        )\n  ) %>% \n  select(time_in_seconds, age, gender, nationality, distance, elevation)\n\nset.seed(2021)\nultra_split <- initial_split(ultra_df, strata = time_in_seconds)\nultra_train <- training(ultra_split)\nultra_test <- testing(ultra_split)\n```\n:::\n\n\ncreate resamples for cross validation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(124)\nultra_folds <- vfold_cv(ultra_train, v=10)\n```\n:::\n\n\n### recipes for feature engineer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nultra_rec <- recipe(time_in_seconds ~., data = ultra_train) %>% \n  step_other(nationality) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_string2factor(all_nominal_predictors()) %>% \n  # step_dummy(all_nominal_predictors()) %>% \n  I()\n\n# want to test whether dummy variables affect the model behave\nind_rec <- ultra_rec %>% \n  step_dummy(all_nominal_predictors())\n```\n:::\n\n\n### fit linear model\n\nspecify models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_spec <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n```\n:::\n\n\nDoes linear model need dummy variable? Using `workflow_set` to test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_wf <- workflow_set(\n  preproc = list(\"nodummy\"=ultra_rec, \"dummy\"=ind_rec),\n  models = list(lm_spec)\n)\n\nlm_rs <- workflow_map(\n  lm_wf, 'fit_resamples', resamples=ultra_folds\n  )\n\nlm_rs %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 9\n  wflow_id           .config preproc model .metric .esti…¹    mean     n std_err\n  <chr>              <chr>   <chr>   <chr> <chr>   <chr>     <dbl> <int>   <dbl>\n1 nodummy_linear_reg Prepro… AsIs    line… rmse    standa… 2.39e+4    10 6.94e+1\n2 nodummy_linear_reg Prepro… AsIs    line… rsq     standa… 5.66e-1    10 2.73e-3\n3 dummy_linear_reg   Prepro… AsIs    line… rmse    standa… 2.39e+4    10 6.94e+1\n4 dummy_linear_reg   Prepro… AsIs    line… rsq     standa… 5.66e-1    10 2.73e-3\n# … with abbreviated variable name ¹​.estimator\n```\n:::\n:::\n\n\nBased on the r-square value, the linear model with age, distance, elevation, gender and nationality explained \\~57% variance of time_in_seconds.\n\nUsing dummy variable or not does not change the metrics. In fact, the number of coefficients will be exactly same no matter whether using dummy or not. Below shows coefficients of linear regression by fitting the \"nodummy_linear_reg\" workflow to the training data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_coef <- lm_rs %>% \n  extract_workflow('nodummy_linear_reg') %>% \n  fit(ultra_train) %>% \n  tidy()\n\nlm_coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)       142711.     217.      658.  0        \n2 age                 4220.      83.3      50.6 0        \n3 genderW             6315.     236.       26.8 1.82e-157\n4 nationalityGBR    -25432.     389.      -65.3 0        \n5 nationalityJPN    -20211.     406.      -49.8 0        \n6 nationalityUSA    -30025.     302.      -99.6 0        \n7 nationalityother  -19682.     254.      -77.6 0        \n8 distance            2630.      99.2      26.5 2.65e-154\n9 elevation          17421.     117.      149.  0        \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_coef %>% \n  filter(term!=\"(Intercept)\") %>% \n  ggplot(aes(x = estimate, y = fct_reorder(term, estimate))) +\n  geom_col(aes(fill=(estimate < 0)), alpha = 0.5) +\n  geom_errorbar(aes(xmin=estimate - std.error, xmax = estimate + std.error), width=0.5) +\n  theme(legend.position = 'none') +\n  labs(fill=NULL, y = NULL)\n```\n\n::: {.cell-output-display}\n![](2021-11-02_tidyTues_ultraRace_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nElevation, being a women (compare to being a men), age and distance positively affect race time, while racers from JPN/GBR/USA/other (compare to racers from FRA) finish the race in shorter time.\n\n### fit random forest model using workflow\n\nUsing random forest as model to get Resampling results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest() %>% \n  set_engine('ranger') %>% \n  set_mode('regression')\n\nrf_wf <- workflow() %>% \n  add_model(rf_spec) %>% \n  add_recipe(ultra_rec)\n\n# resample evaluate \nrf_rs  <- rf_wf %>% \n  fit_resamples(\n    resamples = ultra_folds\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_rs  %>% \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator      mean     n  std_err .config             \n  <chr>   <chr>          <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   18535.       10 57.2     Preprocessor1_Model1\n2 rsq     standard       0.738    10  0.00219 Preprocessor1_Model1\n```\n:::\n:::\n\n\nCompared to linear model shown above, random forest with same predictors can explain more variance of Y (74% vs. 56%) and show smaller rmse (1.8e4 vs. 2.4e4).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_rows(\n  rf_rs  %>% \n    collect_metrics() %>% \n    select(.metric, mean, std_err) %>% \n    mutate(model = \"random forest\"),\n  lm_rs  %>% \n    collect_metrics() %>% \n    filter(wflow_id == 'nodummy_linear_reg') %>% \n    select(.metric, mean, std_err) %>% \n    mutate(model = \"linear reg\")\n) %>% \n  ggplot(aes(x = model, y = mean)) +\n  facet_wrap(vars(.metric), scales = 'free') +\n  geom_point() +\n  geom_errorbar(aes(ymin=mean - std_err, ymax=mean + std_err), width=0)\n```\n\n::: {.cell-output-display}\n![](2021-11-02_tidyTues_ultraRace_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nNotes: above plot can also be done by autoplot if we perform the comparison between linear regression and random forest models using `workflow_set`.\n\n### `last_fit` test data using random forest result\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_final_rs <- rf_wf %>% \n  last_fit(ultra_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_final_rs %>% \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard   18558.    Preprocessor1_Model1\n2 rsq     standard       0.737 Preprocessor1_Model1\n```\n:::\n:::\n\n\nDifferent from `fit_resample` results, these metrics are calculated on the test data. The value is very close to the values done on training data (resample data), thus the model is not over-fitted.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- rf_final_rs %>% \n  extract_workflow()\n\nfinal_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_other()\n• step_normalize()\n• step_string2factor()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      83042 \nNumber of independent variables:  5 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       342750489 \nR squared (OOB):                  0.7386915 \n```\n:::\n:::\n\n\nThe above **trained** workflow from `last_fit` can be saved in `.rda` for future prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# using final_wf for prediction\nfinal_wf %>% \n  predict(new_data = ultra_train %>% dplyr::slice(1)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n    .pred\n    <dbl>\n1 108741.\n```\n:::\n:::\n\n\n## what techniques i learned\n\n-   deal with high-levels nominal features (`fct_lump` and `step_other`) in EDA and modeling\n-   `workflow_set` and `map_workflow` to create multiple workflows for model and/or recipes comparison.\n-   `fit_resample` for cross-validation. The metrics collected from cross-validation results are used for workflow comparison.\n-   `last_fit` model and save **trained** workflow for future use\n",
    "supporting": [
      "2021-11-02_tidyTues_ultraRace_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}